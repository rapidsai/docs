{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "cloud/aws/sagemaker",
     "workflow/hpo",
     "library/xgboost",
     "library/cuml",
     "library/cupy",
     "library/cudf",
     "dataset/airline",
     "library/dask"
    ]
   },
   "source": [
    "# Deep Dive into running Hyper Parameter Optimization on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hyper Parameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) (HPO) improves model quality by searching over hyperparameters, parameters not typically learned during the training process but rather values that control the learning process itself (e.g., model size/capacity). This search can significantly boost model quality relative to default settings and non-expert tuning; however, HPO can take a very long time on a non-accelerated platform. In this notebook, we containerize a RAPIDS workflow and run Bring-Your-Own-Container SageMaker HPO to show how we can overcome the computational complexity of model search. \n",
    "\n",
    "We accelerate HPO in two key ways: \n",
    "* by *scaling within a node* (e.g., multi-GPU where each GPU brings a magnitude higher core count relative to CPUs), and \n",
    "* by *scaling across nodes* and running parallel trials on cloud instances.\n",
    "\n",
    "By combining these two powers HPO experiments that feel unapproachable and may take multiple days on CPU instances can complete in just hours. For example, we find a <span style=\"color:#8735fb; font-size:14pt\"> **12x** </span> speedup in wall clock time (6 hours vs 3+ days) and a <span style=\"color:#8735fb; font-size:14pt\"> **4.5x** </span> reduction in cost when comparing between GPU and CPU [EC2 Spot instances](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) on 100 XGBoost HPO trials using 10 parallel workers on 10 years of the Airline Dataset (~63M flights) hosted in a S3 bucket. For additional details refer to the <a href='#experiments'>end of the notebook</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these powerful tools at our disposal, every data scientist should feel empowered to up-level their model before serving it to the world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../_static/images/examples/rapids-sagemaker-hpo/hpo.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things rolling let's make sure we can query our AWS SageMaker execution role and session as well as our account ID and AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY   TAG       IMAGE ID   CREATED   SIZE\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.54)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.29.54)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.54->boto3) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.54->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.54->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "account = !(aws sts get-caller-identity --query Account --output text)\n",
    "region = !(aws configure get region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['561241433344'], ['us-west-2'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account, region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and choose the configuration options for our HPO run.\n",
    "\n",
    "Below are two reference configurations showing a small and a large scale HPO (sized in terms of total experiments/compute). \n",
    "\n",
    "The default values in the notebook are set for the small HPO configuration, however you are welcome to scale them up.\n",
    "\n",
    "> **small HPO**: 1_year, XGBoost, 3 CV folds, singleGPU, max_jobs = 10, max_parallel_jobs = 2\n",
    "\n",
    "> **large HPO**: 10_year, XGBoost, 10 CV folds, multiGPU, max_jobs = 100, max_parallel_jobs = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer free hosting for several demo datasets that you can try running HPO with, or alternatively you can bring your own dataset (BYOD). \n",
    "\n",
    "By default we leverage the `Airline` dataset, which is a large public tracker of US domestic flight logs which we offer in various sizes (1 year, 3 year, and 10 year) and in <a href='https://parquet.apache.org/'>Parquet</a> (compressed column storage) format. The machine learning objective with this dataset is to predict whether flights will be more than 15 minutes late arriving to their destination ([dataset link](https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&DB_URL=), additional details in <a href='#dataset'>Section 1.1</a>). \n",
    "\n",
    "As an alternative we also offer the `NYC Taxi` dataset which captures yellow cab trip details in Ney York in January 2020, stored in <a href='https://en.wikipedia.org/wiki/Comma-separated_values'>CSV </a> format without any compression. The machine learning objective with this dataset is to predict whether a trip had an above average tip (>$2.20).\n",
    "\n",
    "We host the demo datasets in public S3 demo buckets in both the **us-east-1** (N. Virginia) or **us-west-2** (Oregon) regions (i.e., `sagemaker-rapids-hpo-us-east-1`, and `sagemaker-rapids-hpo-us-west-2`). You should run the SageMaker HPO workflow in either of these two regions if you wish to leverage the demo datasets since SageMaker requires that the S3 dataset and the compute you'll be renting are co-located. \n",
    "\n",
    "Lastly, if you plan to use your own dataset refer to the <a href='#byod'>BYOD checklist in the Appendix</a> to help integrate into the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| dataset | data_bucket | dataset_directory | # samples | storage type | time span |\n",
    "|---|---|---|---|---|---|\n",
    "| Airline Stats Small    | demo    | 1_year   | 6.3M   | Parquet     | 2019         |\n",
    "| Airline Stats Medium   | demo    | 3_year   | 18M    | Parquet     | 2019-2017    |\n",
    "| Airline Stats Large    | demo    | 10_year  | 63M    | Parquet     | 2019-2010    |\n",
    "| NYC Taxi               | demo    | NYC_taxi | 6.3M   | CSV         | 2020 January |\n",
    "| Bring Your Own Dataset | custom  | custom   | custom | Parquet/CSV | custom       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose dataset S3 bucket and directory\n",
    "data_bucket = \"sagemaker-rapids-hpo-\" + region[0]\n",
    "dataset_directory = \"10_year\"  # '1_year', '3_year', '10_year', 'NYC_taxi'\n",
    "\n",
    "# please choose output bucket for trained model(s)\n",
    "model_output_bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_input = f\"s3://{data_bucket}/{dataset_directory}\"\n",
    "s3_model_output = f\"s3://{model_output_bucket}/trained-models\"\n",
    "\n",
    "best_hpo_model_local_save_directory = os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a ML/algorithm perspective, we offer [XGBoost](https://xgboost.readthedocs.io/en/latest/#), [RandomForest](https://docs.rapids.ai/api/cuml/stable/cuml_blogs.html#tree-and-forest-models) and [KMeans](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=kmeans#cuml.KMeans). You are free to switch between these algorithm choices and everything in the example will continue to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose learning algorithm\n",
    "algorithm_choice = \"XGBoost\"\n",
    "\n",
    "assert algorithm_choice in [\"XGBoost\", \"RandomForest\", \"KMeans\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optionally increase robustness via reshuffles of the train-test split (i.e., [cross-validation folds](https://scikit-learn.org/stable/modules/cross_validation.html)). Typical values here are between 3 and 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose cross-validation folds\n",
    "cv_folds = 10\n",
    "\n",
    "assert cv_folds >= 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Workflow Compute Choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable the option of running different code variations that unlock increasing amounts of parallelism in the compute workflow. \n",
    "\n",
    "* `singleCPU`** = [pandas](https://pandas.pydata.org/) + [sklearn](https://scikit-learn.org/stable/)\n",
    "* `multiCPU`   = [dask](https://dask.org/) + [pandas](https://pandas.pydata.org/) + [sklearn](https://scikit-learn.org/stable/)\n",
    "\n",
    "* <span style=\"color:#8735fb; font-size:14pt\"> RAPIDS </span> `singleGPU` = [cudf](https://github.com/rapidsai/cudf) + [cuml](https://github.com/rapidsai/cuml)\n",
    "* <span style=\"color:#8735fb; font-size:14pt\"> RAPIDS </span> `multiGPU`  = [dask](https://dask.org/) + [cudf](https://github.com/rapidsai/cudf) + [cuml](https://github.com/rapidsai/cuml) \n",
    "\n",
    "All of these code paths are available in the `/workflows` directory for your reference. \n",
    "\n",
    "> **Note that the single-CPU option will leverage multiple cores in the model training portion of the workflow; however, to unlock full parallelism in each stage of the workflow we use [Dask](https://dask.org/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose code variant\n",
    "ml_workflow_choice = \"multiGPU\"\n",
    "\n",
    "assert ml_workflow_choice in [\"singleCPU\", \"singleGPU\", \"multiCPU\", \"multiGPU\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Ranges and Strategy\n",
    "<a id='strategy-and-param-ranges'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important choices when running HPO is to choose the bounds of the hyperparameter search process. Below we've set the ranges of the hyperparameters to allow for interesting variation, you are of course welcome to revise these ranges based on domain knowledge especially if you plan to plug in your own dataset. \n",
    "\n",
    "> Note that we support additional algorithm specific parameters (refer to the `parse_hyper_parameter_inputs` function in `HPOConfig.py`), but for demo purposes have limited our choice to the three parameters that overlap between the XGBoost and RandomForest algorithms. For more details see the documentation for [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) and [RandomForest parameters](https://docs.rapids.ai/api/cuml/stable/api.html#random-forest). Since KMeans uses different parameters, we adjust accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose HPO search ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"max_depth\": sagemaker.parameter.IntegerParameter(5, 15),\n",
    "    \"n_estimators\": sagemaker.parameter.IntegerParameter(100, 500),\n",
    "    \"max_features\": sagemaker.parameter.ContinuousParameter(0.1, 1.0),\n",
    "}  # see note above for adding additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"XGBoost\" in algorithm_choice:\n",
    "    # number of trees parameter name difference b/w XGBoost and RandomForest\n",
    "    hyperparameter_ranges[\"num_boost_round\"] = hyperparameter_ranges.pop(\"n_estimators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"KMeans\" in algorithm_choice:\n",
    "    hyperparameter_ranges = {\n",
    "        \"n_clusters\": sagemaker.parameter.IntegerParameter(2, 20),\n",
    "        \"max_iter\": sagemaker.parameter.IntegerParameter(100, 500),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose between a Random and Bayesian search strategy for picking parameter combinations. \n",
    "\n",
    "**Random Search**: Choose a random combination of values from within the ranges for each training job it launches. The choice of hyperparameters doesn't depend on previous results so you can run the maximum number of concurrent workers without affecting the performance of the search. \n",
    "\n",
    "**Bayesian Search**: Make a guess about which hyperparameter combinations are likely to get the best results. After testing the first set of hyperparameter values, hyperparameter tuning uses regression to choose the next set of hyperparameter values to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose HPO search strategy\n",
    "search_strategy = \"Random\"\n",
    "\n",
    "assert search_strategy in [\"Random\", \"Bayesian\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to decide how may total experiments to run, and how many should run in parallel. Below we have a very conservative number of maximum jobs to run so that you don't accidently spawn large computations when starting out, however for meaningful HPO searches this number should be much higher (e.g., in our experiments we often run 100 max_jobs). Note that you may need to request a [quota limit increase](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html) for additional  `max_parallel_jobs` parallel workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose total number of HPO experiments[ we have set this number very low to allow for automated CI testing ]\n",
    "max_jobs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose number of experiments that can run in parallel\n",
    "max_parallel_jobs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set the max duration for an individual job to 24 hours so we don't have run-away compute jobs taking too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration_of_experiment_seconds = 60 * 60 * 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset size and compute choice we will try to recommend an instance choice*, you are of course welcome to select alternate configurations. \n",
    "> e.g., For the 10_year dataset option, we suggest ml.p3.8xlarge instances (4 GPUs) and ml.m5.24xlarge CPU instances ( we will need upwards of 200GB CPU RAM during model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended instance type : ml.p3.8xlarge \n",
      "instance details          : 4x GPUs [ V100 ], 64GB GPU memory,  244GB CPU memory\n"
     ]
    }
   ],
   "source": [
    "# we will recommend a compute instance type, feel free to modify\n",
    "instance_type = recommend_instance_type(ml_workflow_choice, dataset_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to choosing our instance type, we can also enable significant savings by leveraging [AWS EC2 Spot Instances](https://aws.amazon.com/ec2/spot/).\n",
    "\n",
    "We **highly recommend** that you set this flag to `True` as it typically leads to 60-70% cost savings. Note, however that you may need to request a [quota limit increase](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html) to enable Spot instances in SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose whether spot instances should be used\n",
    "use_spot_instances_flag = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 data input    =\ts3://sagemaker-rapids-hpo-us-west-2/10_year\n",
      "s3 model output  =\ts3://sagemaker-us-west-2-561241433344/trained-models\n",
      "compute          =\tmultiGPU\n",
      "algorithm        =\tXGBoost, 10 cv-fold\n",
      "instance         =\tml.p3.8xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t100\n",
      "max_parallel     =\t10\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices(\n",
    "    s3_data_input,\n",
    "    s3_model_output,\n",
    "    ml_workflow_choice,\n",
    "    algorithm_choice,\n",
    "    cv_folds,\n",
    "    instance_type,\n",
    "    use_spot_instances_flag,\n",
    "    search_strategy,\n",
    "    max_jobs,\n",
    "    max_parallel_jobs,\n",
    "    max_duration_of_experiment_seconds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **1. ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../_static/images/examples/rapids-sagemaker-hpo/ml_workflow.png' width='800'> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "<a id ='dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default settings for this demo are built to utilize the Airline dataset (Carrier On-Time Performance 1987-2020, available from the [Bureau of Transportation Statistics](https://transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)). Below are some additional details about this dataset, we plan to offer a companion notebook that does a deep dive on the data science behind this dataset. Note that if you are using an alternate dataset (e.g., NYC Taxi or BYOData) these details are not relevant.\n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* Locations and distance  ( `Origin`, `Dest`, `Distance` )\n",
    "* Airline / carrier ( `Reporting_Airline` )\n",
    "* Scheduled departure and arrival times ( `CRSDepTime` and `CRSArrTime` )\n",
    "* Actual departure and arrival times ( `DpTime` and `ArrTime` )\n",
    "* Difference between scheduled & actual times ( `ArrDelay` and `DepDelay` )\n",
    "* Binary encoded version of late, aka our target variable ( `ArrDelay15` )\n",
    "\n",
    "Using these features we will build a classifier model to predict whether a flight is going to be more than 15 minutes late on arrival as it prepares to depart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python ML Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAPIDS enabled SageMaker HPO we first need to build a [SageMaker Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html). An Estimator is a container image that captures all the software needed to run an HPO experiment. The container is augmented with entrypoint code that will be trggered at runtime by each worker. The entrypoint code enables us to write custom models and hook them up to data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with SageMaker HPO, the entrypoint logic should parse hyperparameters (supplied by AWS SageMaker), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyperparameter setting. We've already built multiple variations of this code.\n",
    "\n",
    "If you would like to make changes by adding your custom model logic feel free to modify the **train.py** and/or the specific workflow files in the `workflows` directory. You are also welcome to uncomment the cells below to load the read/review the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's switch our working directory to the location of the Estimator entrypoint and library code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "rapids-sagemaker-hpo\n",
    "├── notebook.ipynb\n",
    "├── Dockerfile\n",
    "├── entrypoint.sh\n",
    "├── HPOConfig.py\n",
    "├── HPODatasets.py\n",
    "├── MLWorkflow.py\n",
    "├── serve.py\n",
    "├── train.py\n",
    "└── workflows\n",
    "    ├── MLWorkflowMultiCPU.py\n",
    "    ├── MLWorkflowMultiGPU.py\n",
    "    ├── MLWorkflowSingleCPU.py\n",
    "    └── MLWorkflowSingleGPU.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load workflows/MLWorkflowSingleGPU.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../_static/images/examples/rapids-sagemaker-hpo/estimator.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already mentioned, the SageMaker Estimator represents the containerized software stack that AWS SageMaker will replicate to each worker node.\n",
    "\n",
    "The first step to building our Estimator, is to augment a RAPIDS container with our ML Workflow code from above, and push this image to Amazon Elastic Cloud Registry so it is available to SageMaker.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerize and Push to ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to building our container so that it can integrate with the AWS SageMaker HPO API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container can either be built on top of the latest RAPIDS [ nightly ] image as a starting layer or the RAPIDS stable image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_base_container = (\n",
    "    \"rapidsai/rapidsai-core:22.12-cuda11.5-runtime-ubuntu18.04-py3.9\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also decide on the full name of our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = \"rapids-sagemaker-mnmg-100\"\n",
    "image_tag = rapids_base_container.split(\":\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_fullname = (\n",
    "    f\"{account[0]}.dkr.ecr.{region[0]}.amazonaws.com/{image_base}:{image_tag}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-west-2.amazonaws.com/rapids-sagemaker-mnmg-100:22.12-cuda11.5-runtime-ubuntu18.04-py3.9'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write out the Dockerfile to disk, and in a few cells execute the docker build command. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write our selected RAPDIS image layer as the first FROM statement in the the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dockerfile\", \"w\") as dockerfile:\n",
    "    dockerfile.writelines(\n",
    "        f\"FROM {rapids_base_container} \\n\\n\"\n",
    "        f'ENV AWS_DATASET_DIRECTORY=\"{dataset_directory}\"\\n'\n",
    "        f'ENV AWS_ALGORITHM_CHOICE=\"{algorithm_choice}\"\\n'\n",
    "        f'ENV AWS_ML_WORKFLOW_CHOICE=\"{ml_workflow_choice}\"\\n'\n",
    "        f'ENV AWS_CV_FOLDS=\"{cv_folds}\"\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's append write the remaining pieces of the Dockerfile, namely adding the sagemaker-training-toolkit, flask, dask-ml, and copying our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Dockerfile\n",
    "\n",
    "# ensure printed output/log-messages retain correct order\n",
    "ENV PYTHONUNBUFFERED=True\n",
    "    \n",
    "# add sagemaker-training-toolkit [ requires build tools ], flask [ serving ], and dask-ml\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential \\ \n",
    "    && source activate rapids \\\n",
    "    && pip3 install sagemaker-training cupy-cuda115 flask dask-ml \\\n",
    "    && pip3 install --upgrade protobuf\n",
    "\n",
    "# path where SageMaker looks for code when container runs in the cloud\n",
    "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "# copy our latest [local] code into the container \n",
    "COPY . $CLOUD_PATH\n",
    "\n",
    "# make the entrypoint script executable\n",
    "RUN chmod +x $CLOUD_PATH/entrypoint.sh\n",
    "\n",
    "WORKDIR $CLOUD_PATH\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's ensure that our Dockerfile correctly captured our base image selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dockerfile(rapids_base_container)\n",
    "!cat Dockerfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build step will be dominated by the download of the RAPIDS image (base layer). If it's already been downloaded the build will take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.12-cuda11.5-runtime-ubuntu18.04-py3.9: Pulling from rapidsai/rapidsai-core\n",
      "\n",
      "\u001b[1Be5416296: Pulling fs layer \n",
      "\u001b[1B2d3ed59c: Pulling fs layer \n",
      "\u001b[1B1b38369f: Pulling fs layer \n",
      "\u001b[1B4c8e4d7e: Pulling fs layer \n",
      "\u001b[1Ba06239d6: Pulling fs layer \n",
      "\u001b[1Bcb87b249: Pulling fs layer \n",
      "\u001b[1B61c55367: Pulling fs layer \n",
      "\u001b[1Bfb9847e6: Pulling fs layer \n",
      "\u001b[1B0cc4d9ef: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:959a2e80642e881ef99705473d95165cda8383543cff4ae5ca554da782021e47K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KExtracting   2.96GB/3.932GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for rapidsai/rapidsai-core:22.12-cuda11.5-runtime-ubuntu18.04-py3.9\n",
      "docker.io/rapidsai/rapidsai-core:22.12-cuda11.5-runtime-ubuntu18.04-py3.9\n"
     ]
    }
   ],
   "source": [
    "!docker pull $rapids_base_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY               TAG                                        IMAGE ID       CREATED       SIZE\n",
      "rapidsai/rapidsai-core   22.12-cuda11.5-runtime-ubuntu18.04-py3.9   9de590bd08c5   5 weeks ago   13.1GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  90.62kB\n",
      "Step 1/12 : FROM rapidsai/rapidsai-core:22.12-cuda11.5-runtime-ubuntu18.04-py3.9\n",
      " ---> 9de590bd08c5\n",
      "Step 2/12 : ENV AWS_DATASET_DIRECTORY=\"10_year\"\n",
      " ---> Running in 4b7c02990c9d\n",
      "Removing intermediate container 4b7c02990c9d\n",
      " ---> 967f657cfc72\n",
      "Step 3/12 : ENV AWS_ALGORITHM_CHOICE=\"XGBoost\"\n",
      " ---> Running in d6dbf287ed0f\n",
      "Removing intermediate container d6dbf287ed0f\n",
      " ---> 114a69f1a302\n",
      "Step 4/12 : ENV AWS_ML_WORKFLOW_CHOICE=\"multiGPU\"\n",
      " ---> Running in bfe707fe38f9\n",
      "Removing intermediate container bfe707fe38f9\n",
      " ---> 00c743a5a456\n",
      "Step 5/12 : ENV AWS_CV_FOLDS=\"10\"\n",
      " ---> Running in bb2a2f2a1a7e\n",
      "Removing intermediate container bb2a2f2a1a7e\n",
      " ---> ce9a5ab0fb1e\n",
      "Step 6/12 : ENV PYTHONUNBUFFERED=True\n",
      " ---> Running in d396568a5cab\n",
      "Removing intermediate container d396568a5cab\n",
      " ---> 1c75cb5d8dee\n",
      "Step 7/12 : RUN apt-get update && apt-get install -y --no-install-recommends build-essential     && source activate rapids     && pip3 install sagemaker-training cupy-cuda115 flask dask-ml     && pip3 install --upgrade protobuf\n",
      " ---> Running in 26458c19e436\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [1082 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.9 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3155 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1576 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1387 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1427 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2352 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [30.8 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3577 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [20.5 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [64.0 kB]\n",
      "Fetched 28.1 MB in 5s (5558 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu cpp cpp-7 dpkg-dev g++\n",
      "  g++-7 gcc gcc-7 gcc-7-base libasan4 libatomic1 libbinutils libcc1-0\n",
      "  libcilkrts5 libdpkg-perl libgcc-7-dev libgomp1 libisl19 libitm1 liblsan0\n",
      "  libmpc3 libmpfr6 libmpx2 libquadmath0 libstdc++-7-dev libtsan0 libubsan0\n",
      "  make xz-utils\n",
      "Suggested packages:\n",
      "  binutils-doc cpp-doc gcc-7-locales debian-keyring g++-multilib\n",
      "  g++-7-multilib gcc-7-doc libstdc++6-7-dbg gcc-multilib manpages-dev libtool\n",
      "  flex bison gdb gcc-doc gcc-7-multilib libgcc1-dbg libgomp1-dbg libitm1-dbg\n",
      "  libatomic1-dbg libasan4-dbg liblsan0-dbg libtsan0-dbg libubsan0-dbg\n",
      "  libcilkrts5-dbg libmpx2-dbg libquadmath0-dbg bzr libstdc++-7-doc make-doc\n",
      "Recommended packages:\n",
      "  fakeroot libalgorithm-merge-perl libfile-fcntllock-perl\n",
      "  liblocale-gettext-perl\n",
      "The following NEW packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential cpp cpp-7\n",
      "  dpkg-dev g++ g++-7 gcc gcc-7 gcc-7-base libasan4 libatomic1 libbinutils\n",
      "  libcc1-0 libcilkrts5 libdpkg-perl libgcc-7-dev libgomp1 libisl19 libitm1\n",
      "  liblsan0 libmpc3 libmpfr6 libmpx2 libquadmath0 libstdc++-7-dev libtsan0\n",
      "  libubsan0 make xz-utils\n",
      "0 upgraded, 32 newly installed, 0 to remove and 17 not upgraded.\n",
      "Need to get 37.2 MB of archives.\n",
      "After this operation, 137 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xz-utils amd64 5.2.2-1.3ubuntu0.1 [83.8 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.8 [197 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.8 [488 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.8 [1839 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.8 [3388 B]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-7-base amd64 7.5.0-3ubuntu1~18.04 [18.3 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libisl19 amd64 0.19-1 [551 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmpfr6 amd64 4.0.1-1 [243 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmpc3 amd64 1.1.0-1 [40.8 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpp-7 amd64 7.5.0-3ubuntu1~18.04 [8591 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpp amd64 4:7.4.0-1ubuntu2.3 [27.7 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcc1-0 amd64 8.4.0-1ubuntu1~18.04 [39.4 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgomp1 amd64 8.4.0-1ubuntu1~18.04 [76.5 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libitm1 amd64 8.4.0-1ubuntu1~18.04 [27.9 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libatomic1 amd64 8.4.0-1ubuntu1~18.04 [9192 B]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasan4 amd64 7.5.0-3ubuntu1~18.04 [358 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 liblsan0 amd64 8.4.0-1ubuntu1~18.04 [133 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtsan0 amd64 8.4.0-1ubuntu1~18.04 [288 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libubsan0 amd64 7.5.0-3ubuntu1~18.04 [126 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcilkrts5 amd64 7.5.0-3ubuntu1~18.04 [42.5 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmpx2 amd64 8.4.0-1ubuntu1~18.04 [11.6 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libquadmath0 amd64 8.4.0-1ubuntu1~18.04 [134 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgcc-7-dev amd64 7.5.0-3ubuntu1~18.04 [2378 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-7 amd64 7.5.0-3ubuntu1~18.04 [9381 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc amd64 4:7.4.0-1ubuntu2.3 [5184 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libstdc++-7-dev amd64 7.5.0-3ubuntu1~18.04 [1471 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 g++-7 amd64 7.5.0-3ubuntu1~18.04 [9697 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 g++ amd64 4:7.4.0-1ubuntu2.3 [1568 B]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 make amd64 4.1-9.1ubuntu1 [154 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdpkg-perl all 1.19.0.5ubuntu2.4 [212 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 dpkg-dev all 1.19.0.5ubuntu2.4 [607 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 build-essential amd64 12.4ubuntu1 [4758 B]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 37.2 MB in 2s (16.6 MB/s)\n",
      "Selecting previously unselected package xz-utils.\n",
      "(Reading database ... 13756 files and directories currently installed.)\n",
      "Preparing to unpack .../00-xz-utils_5.2.2-1.3ubuntu0.1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.2-1.3ubuntu0.1) ...\n",
      "Selecting previously unselected package binutils-common:amd64.\n",
      "Preparing to unpack .../01-binutils-common_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
      "Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.8) ...\n",
      "Selecting previously unselected package libbinutils:amd64.\n",
      "Preparing to unpack .../02-libbinutils_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
      "Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.8) ...\n",
      "Selecting previously unselected package binutils-x86-64-linux-gnu.\n",
      "Preparing to unpack .../03-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
      "Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.8) ...\n",
      "Selecting previously unselected package binutils.\n",
      "Preparing to unpack .../04-binutils_2.30-21ubuntu1~18.04.8_amd64.deb ...\n",
      "Unpacking binutils (2.30-21ubuntu1~18.04.8) ...\n",
      "Selecting previously unselected package gcc-7-base:amd64.\n",
      "Preparing to unpack .../05-gcc-7-base_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking gcc-7-base:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libisl19:amd64.\n",
      "Preparing to unpack .../06-libisl19_0.19-1_amd64.deb ...\n",
      "Unpacking libisl19:amd64 (0.19-1) ...\n",
      "Selecting previously unselected package libmpfr6:amd64.\n",
      "Preparing to unpack .../07-libmpfr6_4.0.1-1_amd64.deb ...\n",
      "Unpacking libmpfr6:amd64 (4.0.1-1) ...\n",
      "Selecting previously unselected package libmpc3:amd64.\n",
      "Preparing to unpack .../08-libmpc3_1.1.0-1_amd64.deb ...\n",
      "Unpacking libmpc3:amd64 (1.1.0-1) ...\n",
      "Selecting previously unselected package cpp-7.\n",
      "Preparing to unpack .../09-cpp-7_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking cpp-7 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package cpp.\n",
      "Preparing to unpack .../10-cpp_4%3a7.4.0-1ubuntu2.3_amd64.deb ...\n",
      "Unpacking cpp (4:7.4.0-1ubuntu2.3) ...\n",
      "Selecting previously unselected package libcc1-0:amd64.\n",
      "Preparing to unpack .../11-libcc1-0_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libcc1-0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libgomp1:amd64.\n",
      "Preparing to unpack .../12-libgomp1_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libgomp1:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libitm1:amd64.\n",
      "Preparing to unpack .../13-libitm1_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libitm1:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libatomic1:amd64.\n",
      "Preparing to unpack .../14-libatomic1_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libatomic1:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libasan4:amd64.\n",
      "Preparing to unpack .../15-libasan4_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libasan4:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package liblsan0:amd64.\n",
      "Preparing to unpack .../16-liblsan0_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking liblsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libtsan0:amd64.\n",
      "Preparing to unpack .../17-libtsan0_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libtsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libubsan0:amd64.\n",
      "Preparing to unpack .../18-libubsan0_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libubsan0:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libcilkrts5:amd64.\n",
      "Preparing to unpack .../19-libcilkrts5_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libcilkrts5:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libmpx2:amd64.\n",
      "Preparing to unpack .../20-libmpx2_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libmpx2:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libquadmath0:amd64.\n",
      "Preparing to unpack .../21-libquadmath0_8.4.0-1ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libquadmath0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Selecting previously unselected package libgcc-7-dev:amd64.\n",
      "Preparing to unpack .../22-libgcc-7-dev_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libgcc-7-dev:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package gcc-7.\n",
      "Preparing to unpack .../23-gcc-7_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking gcc-7 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package gcc.\n",
      "Preparing to unpack .../24-gcc_4%3a7.4.0-1ubuntu2.3_amd64.deb ...\n",
      "Unpacking gcc (4:7.4.0-1ubuntu2.3) ...\n",
      "Selecting previously unselected package libstdc++-7-dev:amd64.\n",
      "Preparing to unpack .../25-libstdc++-7-dev_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking libstdc++-7-dev:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package g++-7.\n",
      "Preparing to unpack .../26-g++-7_7.5.0-3ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking g++-7 (7.5.0-3ubuntu1~18.04) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../27-g++_4%3a7.4.0-1ubuntu2.3_amd64.deb ...\n",
      "Unpacking g++ (4:7.4.0-1ubuntu2.3) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../28-make_4.1-9.1ubuntu1_amd64.deb ...\n",
      "Unpacking make (4.1-9.1ubuntu1) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../29-libdpkg-perl_1.19.0.5ubuntu2.4_all.deb ...\n",
      "Unpacking libdpkg-perl (1.19.0.5ubuntu2.4) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../30-dpkg-dev_1.19.0.5ubuntu2.4_all.deb ...\n",
      "Unpacking dpkg-dev (1.19.0.5ubuntu2.4) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../31-build-essential_12.4ubuntu1_amd64.deb ...\n",
      "Unpacking build-essential (12.4ubuntu1) ...\n",
      "Setting up libquadmath0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up libgomp1:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up libatomic1:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up libcc1-0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up make (4.1-9.1ubuntu1) ...\n",
      "Setting up libtsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up libmpfr6:amd64 (4.0.1-1) ...\n",
      "Setting up libdpkg-perl (1.19.0.5ubuntu2.4) ...\n",
      "Setting up liblsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up gcc-7-base:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.8) ...\n",
      "Setting up libmpx2:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up xz-utils (5.2.2-1.3ubuntu0.1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up libmpc3:amd64 (1.1.0-1) ...\n",
      "Setting up libitm1:amd64 (8.4.0-1ubuntu1~18.04) ...\n",
      "Setting up libisl19:amd64 (0.19-1) ...\n",
      "Setting up libasan4:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.8) ...\n",
      "Setting up libcilkrts5:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up libubsan0:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up libgcc-7-dev:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up cpp-7 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up libstdc++-7-dev:amd64 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.8) ...\n",
      "Setting up cpp (4:7.4.0-1ubuntu2.3) ...\n",
      "Setting up binutils (2.30-21ubuntu1~18.04.8) ...\n",
      "Setting up gcc-7 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up g++-7 (7.5.0-3ubuntu1~18.04) ...\n",
      "Setting up gcc (4:7.4.0-1ubuntu2.3) ...\n",
      "Setting up dpkg-dev (1.19.0.5ubuntu2.4) ...\n",
      "Setting up g++ (4:7.4.0-1ubuntu2.3) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/c++.1.gz because associated file /usr/share/man/man1/g++.1.gz (of link group c++) doesn't exist\n",
      "Setting up build-essential (12.4ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
      "Collecting sagemaker-training\n",
      "  Downloading sagemaker_training-4.4.4.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.5/58.5 kB 4.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cupy-cuda115\n",
      "  Downloading cupy_cuda115-10.6.0-cp39-cp39-manylinux1_x86_64.whl (81.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.5/81.5 MB 23.7 MB/s eta 0:00:00\n",
      "Collecting flask\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 25.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dask-ml in /opt/conda/envs/rapids/lib/python3.9/site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sagemaker-training) (1.23.5)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.54-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.7/132.7 kB 33.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sagemaker-training) (1.16.0)\n",
      "Requirement already satisfied: pip in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sagemaker-training) (22.3.1)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting gevent\n",
      "  Downloading gevent-22.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/6.4 MB 99.6 MB/s eta 0:00:00\n",
      "Collecting inotify_simple==1.2.1\n",
      "  Downloading inotify_simple-1.2.1.tar.gz (7.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting werkzeug>=0.15.5\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 kB 8.5 MB/s eta 0:00:00\n",
      "Collecting paramiko>=2.4.2\n",
      "  Downloading paramiko-3.0.0-py3-none-any.whl (210 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.8/210.8 kB 47.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil>=5.6.7 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sagemaker-training) (5.9.4)\n",
      "Requirement already satisfied: protobuf<=3.20.2,>=3.9.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sagemaker-training) (3.20.2)\n",
      "Requirement already satisfied: scipy>=1.2.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sagemaker-training) (1.6.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cupy-cuda115) (0.8)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from flask) (5.1.0)\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from flask) (8.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (2022.11.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (1.5.2)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (2022.11.1+17.g23cbe9a9a)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (0.56.4)\n",
      "Requirement already satisfied: dask-glm>=0.2.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (0.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (0.24.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-ml) (22.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask-glm>=0.2.0->dask-ml) (2.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2022.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.3.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (1.26.13)\n",
      "Requirement already satisfied: locket>=1.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (1.0.4)\n",
      "Requirement already satisfied: tornado<6.2,>=6.0.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (6.1)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from distributed>=2.4.0->dask-ml) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from Jinja2>=3.0->flask) (2.1.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.51.0->dask-ml) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.51.0->dask-ml) (65.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas>=0.24.2->dask-ml) (2022.6)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from paramiko>=2.4.2->sagemaker-training) (38.0.4)\n",
      "Collecting bcrypt>=3.2\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 593.2/593.2 kB 80.9 MB/s eta 0:00:00\n",
      "Collecting pynacl>=1.5\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 856.7/856.7 kB 74.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn>=0.23->dask-ml) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn>=0.23->dask-ml) (3.1.0)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 kB 20.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from boto3->sagemaker-training) (1.0.1)\n",
      "Collecting botocore<1.30.0,>=1.29.54\n",
      "  Downloading botocore-1.29.54-py3-none-any.whl (10.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 71.3 MB/s eta 0:00:00\n",
      "Collecting greenlet>=2.0.0\n",
      "  Downloading greenlet-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 535.9/535.9 kB 18.4 MB/s eta 0:00:00\n",
      "Collecting zope.event\n",
      "  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.5.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (257 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 257.9/257.9 kB 54.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4.2->sagemaker-training) (1.15.1)\n",
      "Requirement already satisfied: heapdict in /opt/conda/envs/rapids/lib/python3.9/site-packages (from zict>=0.1.3->distributed>=2.4.0->dask-ml) (1.0.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4.2->sagemaker-training) (2.21)\n",
      "Building wheels for collected packages: sagemaker-training, inotify_simple\n",
      "  Building wheel for sagemaker-training (setup.py): started\n",
      "  Building wheel for sagemaker-training (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-training: filename=sagemaker_training-4.4.4-cp39-cp39-linux_x86_64.whl size=77792 sha256=d55794478490aade03aa771de3169132e0495b46201c93474f6a01b0dd52920f\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/d1/92/a280728ea435ca0905d0adc46e7e1294539cd20c20061e0847\n",
      "  Building wheel for inotify_simple (setup.py): started\n",
      "  Building wheel for inotify_simple (setup.py): finished with status 'done'\n",
      "  Created wheel for inotify_simple: filename=inotify_simple-1.2.1-py3-none-any.whl size=8201 sha256=3827b905d216809ebc6a6938f21634f4878cceab38fa928ac07b89ea7420e0d9\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/c2/6a/6f6c65836d2fad9ae7008373d82e38b519187113fac6b720c8\n",
      "Successfully built sagemaker-training inotify_simple\n",
      "Installing collected packages: inotify_simple, zope.interface, zope.event, werkzeug, retrying, itsdangerous, greenlet, cupy-cuda115, bcrypt, pynacl, gevent, flask, botocore, s3transfer, paramiko, boto3, sagemaker-training\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.59\n",
      "    Uninstalling botocore-1.27.59:\n",
      "      Successfully uninstalled botocore-1.27.59\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.54 which is incompatible.\n",
      "\u001b[0mSuccessfully installed bcrypt-4.0.1 boto3-1.26.54 botocore-1.29.54 cupy-cuda115-10.6.0 flask-2.2.2 gevent-22.10.2 greenlet-2.0.1 inotify_simple-1.2.1 itsdangerous-2.1.2 paramiko-3.0.0 pynacl-1.5.0 retrying-1.3.4 s3transfer-0.6.0 sagemaker-training-4.4.4 werkzeug-2.2.2 zope.event-4.6 zope.interface-5.5.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRequirement already satisfied: protobuf in /opt/conda/envs/rapids/lib/python3.9/site-packages (3.20.2)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.8/409.8 kB 11.6 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.2\n",
      "    Uninstalling protobuf-3.20.2:\n",
      "      Successfully uninstalled protobuf-3.20.2\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 22.12.1 requires cupy-cuda11x, which is not installed.\n",
      "sagemaker-training 4.4.4 requires protobuf<=3.20.2,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\n",
      "cudf 22.12.1 requires protobuf<3.21.0a0,>=3.20.1, but you have protobuf 4.21.12 which is incompatible.\n",
      "\u001b[0mSuccessfully installed protobuf-4.21.12\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 26458c19e436\n",
      " ---> c079363fd950\n",
      "Step 8/12 : ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      " ---> Running in 2a6107db1872\n",
      "Removing intermediate container 2a6107db1872\n",
      " ---> 544766cb6c2f\n",
      "Step 9/12 : COPY . $CLOUD_PATH\n",
      " ---> 5601b0026b3d\n",
      "Step 10/12 : RUN chmod +x $CLOUD_PATH/entrypoint.sh\n",
      " ---> Running in f1540779d7ac\n",
      "Removing intermediate container f1540779d7ac\n",
      " ---> b0e41515bf61\n",
      "Step 11/12 : WORKDIR $CLOUD_PATH\n",
      " ---> Running in 1630b5f89848\n",
      "Removing intermediate container 1630b5f89848\n",
      " ---> ad01ce435ad3\n",
      "Step 12/12 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Running in 3e2a828b1833\n",
      "Removing intermediate container 3e2a828b1833\n",
      " ---> 045f00216761\n",
      "Successfully built 045f00216761\n",
      "Successfully tagged 561241433344.dkr.ecr.us-west-2.amazonaws.com/rapids-sagemaker-mnmg-100:22.12-cuda11.5-runtime-ubuntu18.04-py3.9\n",
      "CPU times: user 687 ms, sys: 99.4 ms, total: 787 ms\n",
      "Wall time: 45.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!docker build . -t $ecr_fullname -f Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                               TAG                                        IMAGE ID       CREATED                  SIZE\n",
      "561241433344.dkr.ecr.us-west-2.amazonaws.com/rapids-sagemaker-mnmg-100   22.12-cuda11.5-runtime-ubuntu18.04-py3.9   045f00216761   Less than a second ago   13.7GB\n",
      "rapidsai/rapidsai-core                                                   22.12-cuda11.5-runtime-ubuntu18.04-py3.9   9de590bd08c5   5 weeks ago              13.1GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publish to Elastic Cloud Registry (ECR)\n",
    "\n",
    "Now that we've built and tagged our container its time to push it to Amazon's container registry (ECR). Once in ECR, AWS SageMaker will be able to leverage our image to build Estimators and run experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker Login to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_login_str = !(aws ecr get-login --region {region[0]} --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!{docker_login_str[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository [ if it doesn't already exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_query = !(aws ecr describe-repositories --repository-names $image_base)\n",
    "if repository_query[0] == \"\":\n",
    "    !(aws ecr create-repository --repository-name $image_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now actually push the container to ECR\n",
    "> Note the first push to ECR may take some time (hopefully less than 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [561241433344.dkr.ecr.us-west-2.amazonaws.com/rapids-sagemaker-mnmg-100]\n",
      "\n",
      "\u001b[1B36e5943b: Preparing \n",
      "\u001b[1B61737e1c: Preparing \n",
      "\u001b[1Bb2094ffc: Preparing \n",
      "\u001b[1B601675bf: Preparing \n",
      "\u001b[1Ba211643c: Preparing \n",
      "\u001b[1B51d8b000: Preparing \n",
      "\u001b[1Bf7b7f229: Preparing \n",
      "\u001b[1B48598b79: Preparing \n",
      "\u001b[1B2b6403fc: Preparing \n",
      "\u001b[1Bca9f5267: Preparing \n",
      "\u001b[1Be36e26b2: Preparing \n",
      "\u001b[1B2c4843ad: Preparing \n",
      "\u001b[11B2094ffc: Pushed   588.6MB/583.5MB2A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K22.12-cuda11.5-runtime-ubuntu18.04-py3.9: digest: sha256:1d0c31e38929d70278bc2170206ad74207dd692dc7b4ff125de1d6c924fe0afe size: 3061\n"
     ]
    }
   ],
   "source": [
    "!docker push $ecr_fullname"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our container [ +custom logic] and pushed it to ECR, we can finally compile all of efforts into an Estimator instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                               TAG                                        IMAGE ID       CREATED              SIZE\n",
      "561241433344.dkr.ecr.us-west-2.amazonaws.com/rapids-sagemaker-mnmg-100   22.12-cuda11.5-runtime-ubuntu18.04-py3.9   045f00216761   About a minute ago   13.7GB\n",
      "rapidsai/rapidsai-core                                                   22.12-cuda11.5-runtime-ubuntu18.04-py3.9   9de590bd08c5   5 weeks ago          13.1GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'volume_size' - EBS volume size in GB, default = 30\n",
    "estimator_params = {\n",
    "    \"image_uri\": ecr_fullname,\n",
    "    \"role\": execution_role,\n",
    "    \"instance_type\": instance_type,\n",
    "    \"instance_count\": 2,\n",
    "    \"input_mode\": \"File\",\n",
    "    \"output_path\": s3_model_output,\n",
    "    \"use_spot_instances\": use_spot_instances_flag,\n",
    "    \"max_run\": max_duration_of_experiment_seconds,  # 24 hours\n",
    "    \"sagemaker_session\": session,\n",
    "}\n",
    "\n",
    "if use_spot_instances_flag == True:\n",
    "    estimator_params.update({\"max_wait\": max_duration_of_experiment_seconds + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(**estimator_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test by asking SageMaker to run the BYOContainer logic inside our Estimator. This is a useful step if you've made changes to your custom logic and are interested in making sure everything works before launching a large HPO search. \n",
    "\n",
    "> Note: This verification step will use the default hyperparameter values declared in our custom train code, as SageMaker HPO will not be orchestrating a search for this single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 data input    =\ts3://sagemaker-rapids-hpo-us-west-2/10_year\n",
      "s3 model output  =\ts3://sagemaker-us-west-2-561241433344/trained-models\n",
      "compute          =\tmultiGPU\n",
      "algorithm        =\tXGBoost, 10 cv-fold\n",
      "instance         =\tml.p3.8xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t100\n",
      "max_parallel     =\t10\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices(\n",
    "    s3_data_input,\n",
    "    s3_model_output,\n",
    "    ml_workflow_choice,\n",
    "    algorithm_choice,\n",
    "    cv_folds,\n",
    "    instance_type,\n",
    "    use_spot_instances_flag,\n",
    "    search_strategy,\n",
    "    max_jobs,\n",
    "    max_parallel_jobs,\n",
    "    max_duration_of_experiment_seconds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated job name : air-mGPU-XGB-10cv-a96aa65a146b7a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_name = new_job_name_from_config(\n",
    "    dataset_directory,\n",
    "    region,\n",
    "    ml_workflow_choice,\n",
    "    algorithm_choice,\n",
    "    cv_folds,\n",
    "    instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: air-mgpu-xgb-10cv-a96aa65a146b7a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-23 18:33:19 Starting - Starting the training job......\n",
      "2023-01-23 18:34:13 Starting - Preparing the instances for training............\n",
      "2023-01-23 18:36:00 Downloading - Downloading input data...\n",
      "2023-01-23 18:36:31 Training - Downloading the training image.....................\n",
      "2023-01-23 18:39:52 Training - Training image download completed. Training in progress..\u001b[34m@ entrypoint -> launching training script \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:14,715     INFO hpo_log \u001b[0m\n",
      "\u001b[34mparsing configuration from environment settings...\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:14,715     INFO hpo_log   Dataset: Airline\n",
      "  Compute: multi-GPU\n",
      "  Algorithm: XGBoost\n",
      "  CV_folds: 10\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:14,715     INFO hpo_log parsing model hyperparameters from command line arguments...log\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:14,720     INFO hpo_log {    'gamma': 0.0,\n",
      "     'lambda': 1,\n",
      "     'learning_rate': 0.3,\n",
      "     'max_depth': 5,\n",
      "     'num_boost_round': 10,\n",
      "     'objective': 'binary:logistic',\n",
      "     'random_state': 0,\n",
      "     'seed': 0,\n",
      "     'tree_method': 'gpu_hist',\n",
      "     'verbosity': 0}\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:14,721     INFO hpo_log Parquet input files detected\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/part.20.parquet',\n",
      " '/opt/ml/input/data/training/part.120.parquet',\n",
      " '/opt/ml/input/data/training/part.38.parquet',\n",
      " '/opt/ml/input/data/training/part.83.parquet',\n",
      " '/opt/ml/input/data/training/part.5.parquet',\n",
      " '/opt/ml/input/data/training/part.99.parquet',\n",
      " '/opt/ml/input/data/training/part.10.parquet',\n",
      " '/opt/ml/input/data/training/part.44.parquet',\n",
      " '/opt/ml/input/data/training/part.98.parquet',\n",
      " '/opt/ml/input/data/training/part.12.parquet',\n",
      " '/opt/ml/input/data/training/part.103.parquet',\n",
      " '/opt/ml/input/data/training/part.102.parquet',\n",
      " '/opt/ml/input/data/training/part.41.parquet',\n",
      " '/opt/ml/input/data/training/part.59.parquet',\n",
      " '/opt/ml/input/data/training/part.40.parquet',\n",
      " '/opt/ml/input/data/training/part.42.parquet',\n",
      " '/opt/ml/input/data/training/part.86.parquet',\n",
      " '/opt/ml/input/data/training/part.107.parquet',\n",
      " '/opt/ml/input/data/training/part.6.parquet',\n",
      " '/opt/ml/input/data/training/part.96.parquet',\n",
      " '/opt/ml/input/data/training/part.108.parquet',\n",
      " '/opt/ml/input/data/training/part.119.parquet',\n",
      " '/opt/ml/input/data/training/part.74.parquet',\n",
      " '/opt/ml/input/data/training/part.92.parquet',\n",
      " '/opt/ml/input/data/training/part.9.parquet',\n",
      " '/opt/ml/input/data/training/part.16.parquet',\n",
      " '/opt/ml/input/data/training/part.8.parquet',\n",
      " '/opt/ml/input/data/training/part.91.parquet',\n",
      " '/opt/ml/input/data/training/part.67.parquet',\n",
      " '/opt/ml/input/data/training/part.35.parquet',\n",
      " '/opt/ml/input/data/training/part.46.parquet',\n",
      " '/opt/ml/input/data/training/part.36.parquet',\n",
      " '/opt/ml/input/data/training/part.30.parquet',\n",
      " '/opt/ml/input/data/training/part.22.parquet',\n",
      " '/opt/ml/input/data/training/part.78.parquet',\n",
      " '/opt/ml/input/data/training/part.109.parquet',\n",
      " '/opt/ml/input/data/training/part.66.parquet',\n",
      " '/opt/ml/input/data/training/part.26.parquet',\n",
      " '/opt/ml/input/data/training/part.75.parquet',\n",
      " '/opt/ml/input/data/training/part.89.parquet',\n",
      " '/opt/ml/input/data/training/part.58.parquet',\n",
      " '/opt/ml/input/data/training/part.104.parquet',\n",
      " '/opt/ml/input/data/training/part.13.parquet',\n",
      " '/opt/ml/input/data/training/part.37.parquet',\n",
      " '/opt/ml/input/data/training/part.72.parquet',\n",
      " '/opt/ml/input/data/training/part.116.parquet',\n",
      " '/opt/ml/input/data/training/part.23.parquet',\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:14,722     INFO hpo_log detected 121 files as input\n",
      " '/opt/ml/input/data/training/part.68.parquet',\n",
      " '/opt/ml/input/data/training/part.47.parquet',\n",
      " '/opt/ml/input/data/training/part.57.parquet',\n",
      " '/opt/ml/input/data/training/part.97.parquet',\n",
      " '/opt/ml/input/data/training/part.4.parquet',\n",
      " '/opt/ml/input/data/training/part.85.parquet',\n",
      " '/opt/ml/input/data/training/part.110.parquet',\n",
      " '/opt/ml/input/data/training/part.73.parquet',\n",
      " '/opt/ml/input/data/training/part.53.parquet',\n",
      " '/opt/ml/input/data/training/part.63.parquet',\n",
      " '/opt/ml/input/data/training/part.25.parquet',\n",
      " '/opt/ml/input/data/training/part.15.parquet',\n",
      " '/opt/ml/input/data/training/part.19.parquet',\n",
      " '/opt/ml/input/data/training/part.87.parquet',\n",
      " '/opt/ml/input/data/training/part.21.parquet',\n",
      " '/opt/ml/input/data/training/part.32.parquet',\n",
      " '/opt/ml/input/data/training/part.29.parquet',\n",
      " '/opt/ml/input/data/training/part.76.parquet',\n",
      " '/opt/ml/input/data/training/part.84.parquet',\n",
      " '/opt/ml/input/data/training/part.115.parquet',\n",
      " '/opt/ml/input/data/training/part.55.parquet',\n",
      " '/opt/ml/input/data/training/part.31.parquet',\n",
      " '/opt/ml/input/data/training/part.11.parquet',\n",
      " '/opt/ml/input/data/training/part.39.parquet',\n",
      " '/opt/ml/input/data/training/part.100.parquet',\n",
      " '/opt/ml/input/data/training/part.64.parquet',\n",
      " '/opt/ml/input/data/training/part.94.parquet',\n",
      " '/opt/ml/input/data/training/part.60.parquet',\n",
      " '/opt/ml/input/data/training/part.93.parquet',\n",
      " '/opt/ml/input/data/training/part.28.parquet',\n",
      " '/opt/ml/input/data/training/part.14.parquet',\n",
      " '/opt/ml/input/data/training/part.80.parquet',\n",
      " '/opt/ml/input/data/training/part.34.parquet',\n",
      " '/opt/ml/input/data/training/part.82.parquet',\n",
      " '/opt/ml/input/data/training/part.48.parquet',\n",
      " '/opt/ml/input/data/training/part.70.parquet',\n",
      " '/opt/ml/input/data/training/part.81.parquet',\n",
      " '/opt/ml/input/data/training/part.65.parquet',\n",
      " '/opt/ml/input/data/training/part.27.parquet',\n",
      " '/opt/ml/input/data/training/part.52.parquet',\n",
      " '/opt/ml/input/data/training/part.0.parquet',\n",
      " '/opt/ml/input/data/training/part.50.parquet',\n",
      " '/opt/ml/input/data/training/part.71.parquet',\n",
      " '/opt/ml/input/data/training/part.118.parquet',\n",
      " '/opt/ml/input/data/training/part.111.parquet',\n",
      " '/opt/ml/input/data/training/part.54.parquet',\n",
      " '/opt/ml/input/data/training/part.77.parquet',\n",
      " '/opt/ml/input/data/training/part.79.parquet',\n",
      " '/opt/ml/input/data/training/part.17.parquet',\n",
      " '/opt/ml/input/data/training/part.101.parquet',\n",
      " '/opt/ml/input/data/training/part.3.parquet',\n",
      " '/opt/ml/input/data/training/part.95.parquet',\n",
      " '/opt/ml/input/data/training/part.56.parquet',\n",
      " '/opt/ml/input/data/training/part.43.parquet',\n",
      " '/opt/ml/input/data/training/part.90.parquet',\n",
      " '/opt/ml/input/data/training/part.33.parquet',\n",
      " '/opt/ml/input/data/training/part.105.parquet',\n",
      " '/opt/ml/input/data/training/part.51.parquet',\n",
      " '/opt/ml/input/data/training/part.45.parquet',\n",
      " '/opt/ml/input/data/training/part.117.parquet',\n",
      " '/opt/ml/input/data/training/part.18.parquet',\n",
      " '/opt/ml/input/data/training/part.62.parquet',\n",
      " '/opt/ml/input/data/training/part.49.parquet',\n",
      " '/opt/ml/input/data/training/part.106.parquet',\n",
      " '/opt/ml/input/data/training/part.2.parquet',\n",
      " '/opt/ml/input/data/training/part.24.parquet',\n",
      " '/opt/ml/input/data/training/part.7.parquet',\n",
      " '/opt/ml/input/data/training/part.88.parquet',\n",
      " '/opt/ml/input/data/training/part.69.parquet',\n",
      " '/opt/ml/input/data/training/part.1.parquet',\n",
      " '/opt/ml/input/data/training/part.113.parquet',\n",
      " '/opt/ml/input/data/training/part.114.parquet',\n",
      " '/opt/ml/input/data/training/part.61.parquet',\n",
      " '/opt/ml/input/data/training/part.112.parquet']\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:17,980     INFO hpo_log Multi-GPU Workflow\u001b[0m\n",
      "\u001b[35m@ entrypoint -> launching training script \u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:15,669     INFO hpo_log \u001b[0m\n",
      "\u001b[35mparsing configuration from environment settings...\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:15,670     INFO hpo_log   Dataset: Airline\n",
      "  Compute: multi-GPU\n",
      "  Algorithm: XGBoost\n",
      "  CV_folds: 10\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:15,670     INFO hpo_log parsing model hyperparameters from command line arguments...log\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:15,675     INFO hpo_log {    'gamma': 0.0,\n",
      "     'lambda': 1,\n",
      "     'learning_rate': 0.3,\n",
      "     'max_depth': 5,\n",
      "     'num_boost_round': 10,\n",
      "     'objective': 'binary:logistic',\n",
      "     'random_state': 0,\n",
      "     'seed': 0,\n",
      "     'tree_method': 'gpu_hist',\n",
      "     'verbosity': 0}\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:15,676     INFO hpo_log Parquet input files detected\u001b[0m\n",
      "\u001b[35m['/opt/ml/input/data/training/part.95.parquet',\n",
      " '/opt/ml/input/data/training/part.40.parquet',\n",
      " '/opt/ml/input/data/training/part.41.parquet',\n",
      " '/opt/ml/input/data/training/part.20.parquet',\n",
      " '/opt/ml/input/data/training/part.29.parquet',\n",
      " '/opt/ml/input/data/training/part.23.parquet',\n",
      " '/opt/ml/input/data/training/part.17.parquet',\n",
      " '/opt/ml/input/data/training/part.83.parquet',\n",
      " '/opt/ml/input/data/training/part.78.parquet',\n",
      " '/opt/ml/input/data/training/part.103.parquet',\n",
      " '/opt/ml/input/data/training/part.70.parquet',\n",
      " '/opt/ml/input/data/training/part.53.parquet',\n",
      " '/opt/ml/input/data/training/part.51.parquet',\n",
      " '/opt/ml/input/data/training/part.66.parquet',\n",
      " '/opt/ml/input/data/training/part.72.parquet',\n",
      " '/opt/ml/input/data/training/part.76.parquet',\n",
      " '/opt/ml/input/data/training/part.31.parquet',\n",
      " '/opt/ml/input/data/training/part.44.parquet',\n",
      " '/opt/ml/input/data/training/part.77.parquet',\n",
      " '/opt/ml/input/data/training/part.47.parquet',\n",
      " '/opt/ml/input/data/training/part.113.parquet',\n",
      " '/opt/ml/input/data/training/part.45.parquet',\n",
      " '/opt/ml/input/data/training/part.3.parquet',\n",
      " '/opt/ml/input/data/training/part.11.parquet',\n",
      " '/opt/ml/input/data/training/part.13.parquet',\n",
      " '/opt/ml/input/data/training/part.111.parquet',\n",
      " '/opt/ml/input/data/training/part.69.parquet',\n",
      " '/opt/ml/input/data/training/part.21.parquet',\n",
      " '/opt/ml/input/data/training/part.114.parquet',\n",
      " '/opt/ml/input/data/training/part.28.parquet',\n",
      " '/opt/ml/input/data/training/part.94.parquet',\n",
      " '/opt/ml/input/data/training/part.73.parquet',\n",
      " '/opt/ml/input/data/training/part.48.parquet',\n",
      " '/opt/ml/input/data/training/part.35.parquet',\n",
      " '/opt/ml/input/data/training/part.16.parquet',\n",
      " '/opt/ml/input/data/training/part.67.parquet',\n",
      " '/opt/ml/input/data/training/part.79.parquet',\n",
      " '/opt/ml/input/data/training/part.25.parquet',\n",
      " '/opt/ml/input/data/training/part.119.parquet',\n",
      " '/opt/ml/input/data/training/part.37.parquet',\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:15,677     INFO hpo_log detected 121 files as input\n",
      " '/opt/ml/input/data/training/part.4.parquet',\n",
      " '/opt/ml/input/data/training/part.74.parquet',\n",
      " '/opt/ml/input/data/training/part.68.parquet',\n",
      " '/opt/ml/input/data/training/part.9.parquet',\n",
      " '/opt/ml/input/data/training/part.54.parquet',\n",
      " '/opt/ml/input/data/training/part.39.parquet',\n",
      " '/opt/ml/input/data/training/part.96.parquet',\n",
      " '/opt/ml/input/data/training/part.71.parquet',\n",
      " '/opt/ml/input/data/training/part.7.parquet',\n",
      " '/opt/ml/input/data/training/part.50.parquet',\n",
      " '/opt/ml/input/data/training/part.15.parquet',\n",
      " '/opt/ml/input/data/training/part.104.parquet',\n",
      " '/opt/ml/input/data/training/part.88.parquet',\n",
      " '/opt/ml/input/data/training/part.80.parquet',\n",
      " '/opt/ml/input/data/training/part.12.parquet',\n",
      " '/opt/ml/input/data/training/part.84.parquet',\n",
      " '/opt/ml/input/data/training/part.116.parquet',\n",
      " '/opt/ml/input/data/training/part.19.parquet',\n",
      " '/opt/ml/input/data/training/part.46.parquet',\n",
      " '/opt/ml/input/data/training/part.0.parquet',\n",
      " '/opt/ml/input/data/training/part.61.parquet',\n",
      " '/opt/ml/input/data/training/part.36.parquet',\n",
      " '/opt/ml/input/data/training/part.64.parquet',\n",
      " '/opt/ml/input/data/training/part.102.parquet',\n",
      " '/opt/ml/input/data/training/part.100.parquet',\n",
      " '/opt/ml/input/data/training/part.86.parquet',\n",
      " '/opt/ml/input/data/training/part.89.parquet',\n",
      " '/opt/ml/input/data/training/part.6.parquet',\n",
      " '/opt/ml/input/data/training/part.60.parquet',\n",
      " '/opt/ml/input/data/training/part.120.parquet',\n",
      " '/opt/ml/input/data/training/part.56.parquet',\n",
      " '/opt/ml/input/data/training/part.26.parquet',\n",
      " '/opt/ml/input/data/training/part.99.parquet',\n",
      " '/opt/ml/input/data/training/part.14.parquet',\n",
      " '/opt/ml/input/data/training/part.105.parquet',\n",
      " '/opt/ml/input/data/training/part.106.parquet',\n",
      " '/opt/ml/input/data/training/part.90.parquet',\n",
      " '/opt/ml/input/data/training/part.33.parquet',\n",
      " '/opt/ml/input/data/training/part.43.parquet',\n",
      " '/opt/ml/input/data/training/part.118.parquet',\n",
      " '/opt/ml/input/data/training/part.27.parquet',\n",
      " '/opt/ml/input/data/training/part.65.parquet',\n",
      " '/opt/ml/input/data/training/part.101.parquet',\n",
      " '/opt/ml/input/data/training/part.87.parquet',\n",
      " '/opt/ml/input/data/training/part.107.parquet',\n",
      " '/opt/ml/input/data/training/part.8.parquet',\n",
      " '/opt/ml/input/data/training/part.110.parquet',\n",
      " '/opt/ml/input/data/training/part.49.parquet',\n",
      " '/opt/ml/input/data/training/part.112.parquet',\n",
      " '/opt/ml/input/data/training/part.24.parquet',\n",
      " '/opt/ml/input/data/training/part.115.parquet',\n",
      " '/opt/ml/input/data/training/part.55.parquet',\n",
      " '/opt/ml/input/data/training/part.57.parquet',\n",
      " '/opt/ml/input/data/training/part.98.parquet',\n",
      " '/opt/ml/input/data/training/part.85.parquet',\n",
      " '/opt/ml/input/data/training/part.117.parquet',\n",
      " '/opt/ml/input/data/training/part.52.parquet',\n",
      " '/opt/ml/input/data/training/part.93.parquet',\n",
      " '/opt/ml/input/data/training/part.30.parquet',\n",
      " '/opt/ml/input/data/training/part.82.parquet',\n",
      " '/opt/ml/input/data/training/part.75.parquet',\n",
      " '/opt/ml/input/data/training/part.81.parquet',\n",
      " '/opt/ml/input/data/training/part.97.parquet',\n",
      " '/opt/ml/input/data/training/part.62.parquet',\n",
      " '/opt/ml/input/data/training/part.18.parquet',\n",
      " '/opt/ml/input/data/training/part.22.parquet',\n",
      " '/opt/ml/input/data/training/part.91.parquet',\n",
      " '/opt/ml/input/data/training/part.32.parquet',\n",
      " '/opt/ml/input/data/training/part.92.parquet',\n",
      " '/opt/ml/input/data/training/part.38.parquet',\n",
      " '/opt/ml/input/data/training/part.63.parquet',\n",
      " '/opt/ml/input/data/training/part.2.parquet',\n",
      " '/opt/ml/input/data/training/part.59.parquet',\n",
      " '/opt/ml/input/data/training/part.42.parquet',\n",
      " '/opt/ml/input/data/training/part.34.parquet',\n",
      " '/opt/ml/input/data/training/part.108.parquet',\n",
      " '/opt/ml/input/data/training/part.109.parquet',\n",
      " '/opt/ml/input/data/training/part.1.parquet',\n",
      " '/opt/ml/input/data/training/part.5.parquet',\n",
      " '/opt/ml/input/data/training/part.58.parquet',\n",
      " '/opt/ml/input/data/training/part.10.parquet']\u001b[0m\n",
      "\u001b[35m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:18,987     INFO hpo_log Multi-GPU Workflow\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[35m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[35m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[35m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[35m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "    cupy, cupy-cuda115\n",
      "  Follow these steps to resolve this issue:\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "         $ pip uninstall <package_name>\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "         $ conda uninstall cupy\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "         https://docs.cupy.dev/en/stable/install.html\u001b[0m\n",
      "\u001b[35m--------------------------------------------------------------------------------\n",
      "  warnings.warn(f'''\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,970 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:19,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:20,828     INFO hpo_log dask multi-GPU cluster with 4 workers \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:20,828     INFO hpo_log  --- cluster_initialize completed in 2.84786 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:20,828     INFO hpo_log > parquet data ingestion\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:20,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:20,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:20,999 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,849     INFO hpo_log dask multi-GPU cluster with 4 workers \u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,849     INFO hpo_log  --- cluster_initialize completed in 2.86224 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:21,850     INFO hpo_log > parquet data ingestion\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:29,306     INFO hpo_log #011 dataset len: 64451250\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:30,140     INFO hpo_log #011 dataset len: 64451250\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:29,925     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:30,744     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:33,156     INFO hpo_log  --- split_dataset completed in 3.23046 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:33,156     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:33,936     INFO hpo_log  --- split_dataset completed in 3.19171 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:33,936     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:40:34] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:40:34] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:40:34] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:40:34] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:40:35] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:40:35] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:40:35] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:40:35] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:36,484     INFO hpo_log  --- fit completed in 3.32772 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:36,484     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:37,244     INFO hpo_log  --- fit completed in 3.30818 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:37,244     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,562     INFO hpo_log  --- predict completed in 1.07803 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,562     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,334     INFO hpo_log  --- predict completed in 1.08976 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,334     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,989     INFO hpo_log #011 score = 0.9204908609390259\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,989     INFO hpo_log  --- score completed in 0.42674 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,989     INFO hpo_log > saving high-scoring model\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,990     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,990     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:37,997     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,765     INFO hpo_log #011 score = 0.9204908609390259\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,766     INFO hpo_log  --- score completed in 0.43147 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,766     INFO hpo_log > saving high-scoring model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,766     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,766     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:38,773     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:41,207     INFO hpo_log  --- split_dataset completed in 3.20958 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:41,251     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:41,981     INFO hpo_log  --- split_dataset completed in 3.20839 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:42,016     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:40:42] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:40:42] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:40:42] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:40:42] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:40:42] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:40:42] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:40:42] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:40:43] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:43,508     INFO hpo_log  --- fit completed in 2.25698 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:43,508     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:44,258     INFO hpo_log  --- fit completed in 2.24222 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:44,259     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:44,789     INFO hpo_log  --- predict completed in 1.28069 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:44,789     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:45,453     INFO hpo_log  --- predict completed in 1.19442 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:45,454     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:45,393     INFO hpo_log #011 score = 0.919971764087677\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:45,394     INFO hpo_log  --- score completed in 0.60415 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:45,394     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:45,394     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:45,400     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:46,064     INFO hpo_log #011 score = 0.919971764087677\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:46,064     INFO hpo_log  --- score completed in 0.61057 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:46,064     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:46,064     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:46,071     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:48,400     INFO hpo_log  --- split_dataset completed in 2.99933 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:48,446     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:49,216     INFO hpo_log  --- split_dataset completed in 3.14545 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:49,269     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:40:49] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:40:49] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:40:49] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:40:49] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:40:50] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:40:50] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:40:50] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:40:50] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:50,681     INFO hpo_log  --- fit completed in 2.23430 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:50,681     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:51,517     INFO hpo_log  --- fit completed in 2.24880 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:51,518     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:51,776     INFO hpo_log  --- predict completed in 1.09539 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:51,777     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,575     INFO hpo_log  --- predict completed in 1.05687 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,575     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:52,180     INFO hpo_log #011 score = 0.9202624559402466\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:52,181     INFO hpo_log  --- score completed in 0.40424 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:52,181     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:52,181     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:52,188     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,981     INFO hpo_log #011 score = 0.9202624559402466\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,981     INFO hpo_log  --- score completed in 0.40601 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,981     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,981     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:52,988     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:55,538     INFO hpo_log  --- split_dataset completed in 3.35015 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:55,589     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:56,388     INFO hpo_log  --- split_dataset completed in 3.39989 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:56,437     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:40:56] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:40:56] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:40:56] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:40:56] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:40:57] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:40:57] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:40:57] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:40:57] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:57,559     INFO hpo_log  --- fit completed in 1.97020 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:57,559     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:58,604     INFO hpo_log  --- predict completed in 1.04410 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:58,604     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:58,721     INFO hpo_log  --- fit completed in 2.28395 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:58,722     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:59,014     INFO hpo_log #011 score = 0.920303463935852\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:59,014     INFO hpo_log  --- score completed in 0.41051 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:59,014     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:59,014     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:40:59,021     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:59,789     INFO hpo_log  --- predict completed in 1.06737 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:40:59,790     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:00,199     INFO hpo_log #011 score = 0.920303463935852\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:00,199     INFO hpo_log  --- score completed in 0.40916 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:00,199     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:00,199     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:00,206     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:02,481     INFO hpo_log  --- split_dataset completed in 3.46011 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:02,532     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:41:03] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:41:03] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:41:03] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:41:03] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:03,729     INFO hpo_log  --- split_dataset completed in 3.52238 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:03,786     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m[18:41:04] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:04,817     INFO hpo_log  --- fit completed in 2.28582 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:04,818     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m[18:41:04] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:41:04] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:41:04] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:06,031     INFO hpo_log  --- fit completed in 2.24465 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:06,031     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:05,908     INFO hpo_log  --- predict completed in 1.08988 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:05,908     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:06,313     INFO hpo_log #011 score = 0.9203875064849854\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:06,314     INFO hpo_log  --- score completed in 0.40549 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:06,314     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:06,314     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:06,321     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,149     INFO hpo_log  --- predict completed in 1.11832 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,150     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,579     INFO hpo_log #011 score = 0.9203870296478271\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,579     INFO hpo_log  --- score completed in 0.42956 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,579     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,579     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:07,586     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:09,514     INFO hpo_log  --- split_dataset completed in 3.19311 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:09,564     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:41:10] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:41:10] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:41:10] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:41:10] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:10,942     INFO hpo_log  --- split_dataset completed in 3.35631 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:10,984     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:11,462     INFO hpo_log  --- fit completed in 1.89739 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:11,462     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m[18:41:11] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:41:11] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:41:11] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:41:12] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,511     INFO hpo_log  --- predict completed in 1.04895 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,511     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:13,189     INFO hpo_log  --- fit completed in 2.20526 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:13,189     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,918     INFO hpo_log #011 score = 0.9205151200294495\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,918     INFO hpo_log  --- score completed in 0.40648 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,918     INFO hpo_log > saving high-scoring model\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,918     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,918     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:12,926     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,264     INFO hpo_log  --- predict completed in 1.07464 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,264     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,681     INFO hpo_log #011 score = 0.9205151200294495\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,681     INFO hpo_log  --- score completed in 0.41679 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,681     INFO hpo_log > saving high-scoring model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,682     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,682     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:14,688     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:16,754     INFO hpo_log  --- split_dataset completed in 3.82814 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:16,781     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:18,182     INFO hpo_log  --- split_dataset completed in 3.49332 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:18,224     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:41:17] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:41:18] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:41:18] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:41:18] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:41:19] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:41:19] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:41:19] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:19,408     INFO hpo_log  --- fit completed in 2.62662 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:19,408     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m[18:41:19] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,504     INFO hpo_log  --- predict completed in 1.09576 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,504     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:20,813     INFO hpo_log  --- fit completed in 2.58889 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:20,814     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,917     INFO hpo_log #011 score = 0.9204996228218079\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,917     INFO hpo_log  --- score completed in 0.41257 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,917     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,917     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:20,924     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:21,901     INFO hpo_log  --- predict completed in 1.08747 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:21,901     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:22,308     INFO hpo_log #011 score = 0.9204998016357422\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:22,308     INFO hpo_log  --- score completed in 0.40620 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:22,308     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:22,308     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:22,314     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:24,066     INFO hpo_log  --- split_dataset completed in 3.14142 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:24,117     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:25,349     INFO hpo_log  --- split_dataset completed in 3.03426 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:25,389     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:41:24] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:41:25] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:41:25] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:41:25] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:41:25] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:41:25] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:41:25] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:41:26] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:27,246     INFO hpo_log  --- fit completed in 1.85756 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:27,247     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:28,755     INFO hpo_log  --- predict completed in 1.50824 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:28,755     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:29,157     INFO hpo_log #011 score = 0.9205771088600159\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:29,157     INFO hpo_log  --- score completed in 0.40233 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:29,158     INFO hpo_log > saving high-scoring model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:29,158     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:29,158     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:29,165     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:26,389     INFO hpo_log  --- fit completed in 2.27204 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:26,389     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,531     INFO hpo_log  --- predict completed in 1.14184 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,531     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,943     INFO hpo_log #011 score = 0.9205771088600159\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,944     INFO hpo_log  --- score completed in 0.41231 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,944     INFO hpo_log > saving high-scoring model\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,944     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,944     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:27,951     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:31,246     INFO hpo_log  --- split_dataset completed in 3.29588 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:31,279     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:32,337     INFO hpo_log  --- split_dataset completed in 3.17197 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:32,381     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:41:32] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:41:32] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:41:32] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:41:32] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m[18:41:32] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:41:32] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:41:32] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:41:33] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:33,563     INFO hpo_log  --- fit completed in 2.28411 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:33,564     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:34,268     INFO hpo_log  --- fit completed in 1.88717 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:34,268     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:34,660     INFO hpo_log  --- predict completed in 1.09677 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:34,661     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:35,604     INFO hpo_log  --- predict completed in 1.33573 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:35,604     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:35,079     INFO hpo_log #011 score = 0.9202908873558044\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:35,079     INFO hpo_log  --- score completed in 0.41860 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:35,080     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:35,080     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:35,087     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:36,014     INFO hpo_log #011 score = 0.9202908873558044\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:36,014     INFO hpo_log  --- score completed in 0.40990 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:36,014     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:36,014     INFO hpo_log > skipping ingestion, using cache\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:36,022     INFO hpo_log > train-test split\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:38,950     INFO hpo_log  --- split_dataset completed in 3.86303 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:39,003     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[34m[18:41:39] task [xgboost.dask-0]:tcp://127.0.0.1:45687 got new rank 0\u001b[0m\n",
      "\u001b[34m[18:41:39] task [xgboost.dask-1]:tcp://127.0.0.1:34471 got new rank 1\u001b[0m\n",
      "\u001b[34m[18:41:39] task [xgboost.dask-2]:tcp://127.0.0.1:33085 got new rank 2\u001b[0m\n",
      "\u001b[34m[18:41:39] task [xgboost.dask-3]:tcp://127.0.0.1:43469 got new rank 3\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:39,638     INFO hpo_log  --- split_dataset completed in 3.61579 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:39,691     INFO hpo_log > fit xgboost model\u001b[0m\n",
      "\u001b[35m[18:41:40] task [xgboost.dask-0]:tcp://127.0.0.1:37271 got new rank 0\u001b[0m\n",
      "\u001b[35m[18:41:40] task [xgboost.dask-1]:tcp://127.0.0.1:46615 got new rank 1\u001b[0m\n",
      "\u001b[35m[18:41:40] task [xgboost.dask-2]:tcp://127.0.0.1:35569 got new rank 2\u001b[0m\n",
      "\u001b[35m[18:41:40] task [xgboost.dask-3]:tcp://127.0.0.1:42903 got new rank 3\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:40,845     INFO hpo_log  --- fit completed in 1.84259 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:40,846     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:41,516     INFO hpo_log  --- fit completed in 1.82582 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:41,517     INFO hpo_log > predict with trained model \u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,574     INFO hpo_log  --- predict completed in 1.05731 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,574     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,470     INFO hpo_log  --- predict completed in 1.62403 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,471     INFO hpo_log > score predictions\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,873     INFO hpo_log #011 score = 0.9202508926391602\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,873     INFO hpo_log  --- score completed in 0.40286 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,874     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,874     INFO hpo_log total_time = 84.89323 s \u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,874     INFO hpo_log fold scores : [0.9204908609390259, 0.919971764087677, 0.9202624559402466, 0.920303463935852, 0.9203875064849854, 0.9205151200294495, 0.9204996228218079, 0.9205771088600159, 0.9202908873558044, 0.9202508926391602]\u001b[0m\n",
      "\u001b[34m2023-01-23 18:41:42,874     INFO hpo_log final-score: 0.9203549683094024;\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,976     INFO hpo_log #011 score = 0.9202508926391602\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,976     INFO hpo_log  --- score completed in 0.40173 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,976     INFO hpo_log  --- cleanup completed in 0.00000 s\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,976     INFO hpo_log total_time = 83.98951 s \u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,976     INFO hpo_log fold scores : [0.9204908609390259, 0.919971764087677, 0.9202624559402466, 0.920303463935852, 0.9203870296478271, 0.9205151200294495, 0.9204998016357422, 0.9205771088600159, 0.9202908873558044, 0.9202508926391602]\u001b[0m\n",
      "\u001b[35m2023-01-23 18:41:42,976     INFO hpo_log final-score: 0.9203549385070801;\u001b[0m\n",
      "\n",
      "2023-01-23 18:41:59 Uploading - Uploading generated training model\n",
      "2023-01-23 18:41:59 Completed - Training job completed\n",
      "Training seconds: 718\n",
      "Billable seconds: 216\n",
      "Managed Spot Training savings: 69.9%\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=s3_data_input, job_name=job_name.lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a working SageMaker Estimator in hand, the hardest part is behind us. In the key choices section we <a href='#strategy-and-param-ranges'>already defined our search strategy and hyperparameter ranges</a>, so all that remains is to choose a metric to evaluate performance on. For more documentation check out the AWS SageMaker [Hyperparameter Tuner documentation](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../_static/images/examples/rapids-sagemaker-hpo/run_hpo.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only focus on a single metric, which we call 'final-score', that captures the accuracy of our model on the test data unseen during training. You are of course welcome to add aditional metrics, see [AWS SageMaker documentation on Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html). When defining a metric we provide a regular expression (i.e., string parsing rule) to extract the key metric from the output of each Estimator/worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"final-score\", \"Regex\": \"final-score: (.*);\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"final-score\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put all of the elements we've been building up together into a HyperparameterTuner declaration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    metric_definitions=metric_definitions,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    objective_type=\"Maximize\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    strategy=search_strategy,\n",
    "    max_jobs=max_jobs,\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 data input    =\ts3://sagemaker-rapids-hpo-us-west-2/10_year\n",
      "s3 model output  =\ts3://sagemaker-us-west-2-561241433344/trained-models\n",
      "compute          =\tmultiGPU\n",
      "algorithm        =\tXGBoost, 10 cv-fold\n",
      "instance         =\tml.p3.8xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t100\n",
      "max_parallel     =\t10\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices(\n",
    "    s3_data_input,\n",
    "    s3_model_output,\n",
    "    ml_workflow_choice,\n",
    "    algorithm_choice,\n",
    "    cv_folds,\n",
    "    instance_type,\n",
    "    use_spot_instances_flag,\n",
    "    search_strategy,\n",
    "    max_jobs,\n",
    "    max_parallel_jobs,\n",
    "    max_duration_of_experiment_seconds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure we take a moment to confirm before launching all of our HPO experiments. Depending on your configuration options running this cell can kick off a massive amount of computation!\n",
    "> Once this process begins, we recommend that you use the SageMaker UI to keep track of the <a href='../../_static/images/examples/rapids-sagemaker-hpo/gpu_hpo_100x10.png'>health of the HPO process and the individual workers</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning_job_name = new_job_name_from_config(dataset_directory, region, ml_workflow_choice,\n",
    "#                                            algorithm_choice, cv_folds,\n",
    "# #                                            instance_type)\n",
    "# hpo.fit( inputs=s3_data_input,\n",
    "#          job_name=tuning_job_name,\n",
    "#          wait=True,\n",
    "#          logs='All')\n",
    "\n",
    "# hpo.wait()  # block until the .fit call above is completed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your job is complete there are multiple ways to analyze the results.\n",
    "Below we display the performance of the best job, as well printing each HPO trial/job as a row of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = \"air-mGPU-XGB-10cv-527fd372fa4d8d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.9203665256500244\n",
      "best params: {'max_depth': '7', 'max_features': '0.29751893065195945', 'num_boost_round': '346'}\n",
      "best job-name: air-mGPU-XGB-10cv-527fd372fa4d8d-042-ed1ff13b\n"
     ]
    }
   ],
   "source": [
    "hpo_results = summarize_hpo_results(tuning_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>num_boost_round</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.715196</td>\n",
       "      <td>116.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-100-c04c691b</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920362</td>\n",
       "      <td>2023-01-23 21:01:38+00:00</td>\n",
       "      <td>2023-01-23 21:06:21+00:00</td>\n",
       "      <td>283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.855974</td>\n",
       "      <td>243.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-099-97d44628</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920355</td>\n",
       "      <td>2023-01-23 21:17:56+00:00</td>\n",
       "      <td>2023-01-23 21:22:34+00:00</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>395.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-098-e74f483f</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920356</td>\n",
       "      <td>2023-01-23 20:56:06+00:00</td>\n",
       "      <td>2023-01-23 21:00:44+00:00</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.882803</td>\n",
       "      <td>179.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-097-50755cd6</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920356</td>\n",
       "      <td>2023-01-23 20:54:35+00:00</td>\n",
       "      <td>2023-01-23 20:59:13+00:00</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.416939</td>\n",
       "      <td>267.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-096-5c95eb2f</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920355</td>\n",
       "      <td>2023-01-23 20:51:24+00:00</td>\n",
       "      <td>2023-01-23 20:56:02+00:00</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.426204</td>\n",
       "      <td>330.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-005-7b755a81</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920355</td>\n",
       "      <td>2023-01-23 18:48:35+00:00</td>\n",
       "      <td>2023-01-23 18:53:48+00:00</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.283752</td>\n",
       "      <td>256.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-004-e4d086fb</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920356</td>\n",
       "      <td>2023-01-23 18:48:34+00:00</td>\n",
       "      <td>2023-01-23 18:53:47+00:00</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.137874</td>\n",
       "      <td>377.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-003-89cd8506</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920355</td>\n",
       "      <td>2023-01-23 18:48:31+00:00</td>\n",
       "      <td>2023-01-23 18:53:44+00:00</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.934718</td>\n",
       "      <td>365.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-002-caf8f6c3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920360</td>\n",
       "      <td>2023-01-23 18:48:25+00:00</td>\n",
       "      <td>2023-01-23 18:53:48+00:00</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.356588</td>\n",
       "      <td>460.0</td>\n",
       "      <td>air-mGPU-XGB-10cv-527fd372fa4d8d-001-e8a6d247</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.920356</td>\n",
       "      <td>2023-01-23 18:48:29+00:00</td>\n",
       "      <td>2023-01-23 18:53:47+00:00</td>\n",
       "      <td>318.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  max_features  num_boost_round  \\\n",
       "0         5.0      0.715196            116.0   \n",
       "1        12.0      0.855974            243.0   \n",
       "2        11.0      0.549247            395.0   \n",
       "3         7.0      0.882803            179.0   \n",
       "4         8.0      0.416939            267.0   \n",
       "..        ...           ...              ...   \n",
       "95        5.0      0.426204            330.0   \n",
       "96        5.0      0.283752            256.0   \n",
       "97        5.0      0.137874            377.0   \n",
       "98       15.0      0.934718            365.0   \n",
       "99       15.0      0.356588            460.0   \n",
       "\n",
       "                                  TrainingJobName TrainingJobStatus  \\\n",
       "0   air-mGPU-XGB-10cv-527fd372fa4d8d-100-c04c691b         Completed   \n",
       "1   air-mGPU-XGB-10cv-527fd372fa4d8d-099-97d44628         Completed   \n",
       "2   air-mGPU-XGB-10cv-527fd372fa4d8d-098-e74f483f         Completed   \n",
       "3   air-mGPU-XGB-10cv-527fd372fa4d8d-097-50755cd6         Completed   \n",
       "4   air-mGPU-XGB-10cv-527fd372fa4d8d-096-5c95eb2f         Completed   \n",
       "..                                            ...               ...   \n",
       "95  air-mGPU-XGB-10cv-527fd372fa4d8d-005-7b755a81         Completed   \n",
       "96  air-mGPU-XGB-10cv-527fd372fa4d8d-004-e4d086fb         Completed   \n",
       "97  air-mGPU-XGB-10cv-527fd372fa4d8d-003-89cd8506         Completed   \n",
       "98  air-mGPU-XGB-10cv-527fd372fa4d8d-002-caf8f6c3         Completed   \n",
       "99  air-mGPU-XGB-10cv-527fd372fa4d8d-001-e8a6d247         Completed   \n",
       "\n",
       "    FinalObjectiveValue         TrainingStartTime           TrainingEndTime  \\\n",
       "0              0.920362 2023-01-23 21:01:38+00:00 2023-01-23 21:06:21+00:00   \n",
       "1              0.920355 2023-01-23 21:17:56+00:00 2023-01-23 21:22:34+00:00   \n",
       "2              0.920356 2023-01-23 20:56:06+00:00 2023-01-23 21:00:44+00:00   \n",
       "3              0.920356 2023-01-23 20:54:35+00:00 2023-01-23 20:59:13+00:00   \n",
       "4              0.920355 2023-01-23 20:51:24+00:00 2023-01-23 20:56:02+00:00   \n",
       "..                  ...                       ...                       ...   \n",
       "95             0.920355 2023-01-23 18:48:35+00:00 2023-01-23 18:53:48+00:00   \n",
       "96             0.920356 2023-01-23 18:48:34+00:00 2023-01-23 18:53:47+00:00   \n",
       "97             0.920355 2023-01-23 18:48:31+00:00 2023-01-23 18:53:44+00:00   \n",
       "98             0.920360 2023-01-23 18:48:25+00:00 2023-01-23 18:53:48+00:00   \n",
       "99             0.920356 2023-01-23 18:48:29+00:00 2023-01-23 18:53:47+00:00   \n",
       "\n",
       "    TrainingElapsedTimeSeconds  \n",
       "0                        283.0  \n",
       "1                        278.0  \n",
       "2                        278.0  \n",
       "3                        278.0  \n",
       "4                        278.0  \n",
       "..                         ...  \n",
       "95                       313.0  \n",
       "96                       313.0  \n",
       "97                       313.0  \n",
       "98                       323.0  \n",
       "99                       318.0  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For a more in depth look at the HPO process we invite you to check out the <a href='https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/analyze_results'>HPO_Analyze_TuningJob_Results.ipynb</a> notebook which shows how we can explore interesting things like the <a href='../../_static/images/examples/rapids-sagemaker-hpo/results_analysis.png'>impact of each individual hyperparameter on the performance metric</a>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's download the best trained model from our HPO runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded best model\n",
      "> filename: /home/ec2-user/SageMaker/cloud-ml-examples/aws/best_model.tar.gz\n",
      "> local directory : /home/ec2-user/SageMaker/cloud-ml-examples/aws\n",
      "\n",
      "full S3 path : s3://sagemaker-us-west-2-561241433344/trained-models/air-mGPU-XGB-10cv-527fd372fa4d8d-042-ed1ff13b/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "local_filename, s3_path_to_best_model = download_best_model(\n",
    "    model_output_bucket,\n",
    "    s3_model_output,\n",
    "    hpo_results,\n",
    "    best_hpo_model_local_save_directory,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serving\n",
    "\n",
    "With your best model in hand, you can now move on to [serving this model on SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html). \n",
    "\n",
    "In the example below we show you how to build a [RealTimePredictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) using the best model found during the HPO search. We will add a lightweight Flask server to our RAPIDS Estimator (a.k.a., container) which will handle the incoming requests and pass them along to the trained model for inference. If you are curious about how this works under the hood check out the [Use Your Own Inference Server](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html) documentation and reference the code in `serve.py`.\n",
    "\n",
    "If you are interested in additional serving options (e.g., large batch with batch-transform), we plan to add a companion notebook that will provide additional details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_model = sagemaker.model.Model(\n",
    "    image_uri=ecr_fullname, role=execution_role, model_data=s3_path_to_best_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-west-2.amazonaws.com/rapids-sagemaker-mnmg-100:22.12-cuda11.5-runtime-ubuntu18.04-py3.9'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kick off an instance for prediction [ recommend 'ml.g4dn.2xlarge' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: rapids-sagemaker-mnmg-100-2023-01-23-22-24-22-008\n",
      "INFO:sagemaker:Creating endpoint-config with name rapids-sagemaker-mnmg-100-2023-01-23-22-24-22-498\n",
      "INFO:sagemaker:Creating endpoint with name rapids-sagemaker-mnmg-100-2023-01-23-22-24-22-498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "DEMO_SERVING_FLAG = True\n",
    "\n",
    "if DEMO_SERVING_FLAG:\n",
    "    endpoint_model.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.g4dn.2xlarge\"\n",
    "    )  #'ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the prediction and return the result(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we've compiled examples to sanity test the trained model performance on the Airline dataset.\n",
    "> The first example is from a 2019 flight that departed nine minutes early, \n",
    "\n",
    "> The second example is from a 2018 flight that was more than two hours late to depart.\n",
    "\n",
    "When we run these samples we expect to see **b'[0.0, 1.0]** as the printed result.\n",
    "\n",
    "We encourage you to modify the queries below especially if you plug in your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[0.0, 1.0]'\n"
     ]
    }
   ],
   "source": [
    "if DEMO_SERVING_FLAG:\n",
    "    predictor = sagemaker.predictor.Predictor(\n",
    "        endpoint_name=str(endpoint_model.endpoint_name), sagemaker_session=session\n",
    "    )\n",
    "\n",
    "    if dataset_directory in [\"1_year\", \"3_year\", \"10_year\"]:\n",
    "        on_time_example = [\n",
    "            2019.0,\n",
    "            4.0,\n",
    "            12.0,\n",
    "            2.0,\n",
    "            3647.0,\n",
    "            20452.0,\n",
    "            30977.0,\n",
    "            33244.0,\n",
    "            1943.0,\n",
    "            -9.0,\n",
    "            0.0,\n",
    "            75.0,\n",
    "            491.0,\n",
    "        ]  # 9 minutes early departure\n",
    "        late_example = [\n",
    "            2018.0,\n",
    "            3.0,\n",
    "            9.0,\n",
    "            5.0,\n",
    "            2279.0,\n",
    "            20409.0,\n",
    "            30721.0,\n",
    "            31703.0,\n",
    "            733.0,\n",
    "            123.0,\n",
    "            1.0,\n",
    "            61.0,\n",
    "            200.0,\n",
    "        ]\n",
    "        example_payload = str(list([on_time_example, late_example]))\n",
    "    else:\n",
    "        example_payload = \"\"  # fill in a sample payload\n",
    "\n",
    "    result = predictor.predict(example_payload)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are finished with the serving example, we should be sure to clean up and delete the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEMO_SERVING_FLAG:\n",
    "\n",
    "#     predictor.delete_endpoint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now successfully built a RAPIDS ML workflow, containerized it (as a SageMaker Estimator), and launched a set of HPO experiments to find the best hyperparamters for our model.\n",
    "\n",
    "If you are curious to go further, we invite you to plug in your own dataset and tweak the configuration settings to find your champion model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HPO Experiment Details**\n",
    "<a id='experiments'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction we find a <span style=\"color:#8735fb; font-size:14pt\"> **12X** </span> speedup in wall clock time and a <span style=\"color:#8735fb; font-size:14pt\"> **4.5x** </span> reduction in cost when comparing between GPU and CPU instances on 100 HPO trials using 10 parallel workers on 10 years of the Airline Dataset (~63M flights). In these experiments we used the XGBoost algorithm with the multi-GPU vs multi-CPU Dask cluster and 10 cross validaiton folds. Below we offer a table with additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../_static/images/examples/rapids-sagemaker-hpo/results.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the CPU runs, 12 jobs were stopped since they exceeded the 24 hour limit we set. <a href='../../_static/images/examples/rapids-sagemaker-hpo/cpu_hpo_100x10.png'>CPU Job Summary Image</a>\n",
    "\n",
    "In the case of the GPU runs, no jobs were stopped. <a href='../../_static/images/examples/rapids-sagemaker-hpo/gpu_hpo_100x10.png'>GPU Job Summary Image</a>\n",
    "\n",
    "Note that in both cases 1 job failed because a spot instance was terminated. But 1 failed job out of 100 is a minimal tradeoff for the significant cost savings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Bring Your Own Dataset Checklist\n",
    "<a id ='byod'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use your own dataset (BYOD) here is a checklist to help you integrate into the workflow:\n",
    "\n",
    "> - [ ] Dataset should be in either CSV or Parquet format.\n",
    "> - [ ] Dataset is already pre-processed (and all feature-engineering is done).\n",
    "> - [ ] Dataset is uploaded to S3 and `data_bucket` and `dataset_directory` have been set to the location of your data.\n",
    "> - [ ] Dataset feature and target columns have been enumerated in `/HPODataset.py`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapids References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [More Cloud Deployment Workflow Examples](https://docs.rapids.ai/deployment/stable/examples/)\n",
    "\n",
    "> [RAPIDS HPO](https://rapids.ai/hpo)\n",
    "\n",
    "> [cuML Documentation](https://docs.rapids.ai/api/cuml/stable/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit)\n",
    "\n",
    "> [Estimator Parameters](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n",
    "\n",
    "> Spot Instances [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html), and [blog]()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "deployment-docs-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7a54d993f849a0f97fda357a1a3bac7e25a43aff77e618e8d69a4ad36661dba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
