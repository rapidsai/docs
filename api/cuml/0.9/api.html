

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>cuML API Reference &mdash; cuml 0.9.0a documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/params.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to cuML’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> cuml
          

          
          </a>

          
            
            
              <div class="version">
                0.9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">cuML API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#preprocessing">Preprocessing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-selection-and-data-splitting">Model Selection and Data Splitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#label-encoding">Label Encoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset-generation">Dataset Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression-and-classification">Regression and Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasso-regression">Lasso Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#elasticnet-regression">ElasticNet Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#forest-inferencing">Forest Inferencing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quasi-newton">Quasi-Newton</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#clustering">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means-clustering">K-Means Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dbscan">DBSCAN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dimensionality-reduction-and-manifold-learning">Dimensionality Reduction and Manifold Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principal-component-analysis">Principal Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#truncated-svd">Truncated SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#umap">UMAP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-projections">Random Projections</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tsne">TSNE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neighbors">Neighbors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nearest-neighbors">Nearest Neighbors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#time-series">Time Series</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#holtwinters">HoltWinters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kalman-filter">Kalman Filter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multi-node-multi-gpu-algorithms">Multi-Node, Multi-GPU Algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id9">K-Means Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">Random Forest</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">cuml</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>cuML API Reference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="cuml-api-reference">
<h1>cuML API Reference<a class="headerlink" href="#cuml-api-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="preprocessing">
<h2>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="model-selection-and-data-splitting">
<h3>Model Selection and Data Splitting<a class="headerlink" href="#model-selection-and-data-splitting" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><span class="target" id="module-cuml.preprocessing.model_selection"></span><dl class="function">
<dt id="cuml.preprocessing.model_selection.train_test_split">
<code class="sig-prename descclassname">cuml.preprocessing.model_selection.</code><code class="sig-name descname">train_test_split</code><span class="sig-paren">(</span><em class="sig-param">X: cudf.dataframe.dataframe.DataFrame, y: Union[str, cudf.dataframe.series.Series], train_size: Union[float, int] = 0.8, shuffle: bool = True, seed: int = None</em><span class="sig-paren">)</span> &#x2192; Tuple[cudf.dataframe.dataframe.DataFrame, cudf.dataframe.dataframe.DataFrame, cudf.dataframe.dataframe.DataFrame, cudf.dataframe.dataframe.DataFrame]<a class="headerlink" href="#cuml.preprocessing.model_selection.train_test_split" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions the data into four collated dataframes, mimicing sklearn’s
<cite>train_test_split</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cudf.DataFrame</span></dt><dd><p>Data to split, has shape (n_samples, n_features)</p>
</dd>
<dt><strong>y</strong><span class="classifier">str or cudf.Series</span></dt><dd><p>Set of labels for the data, either a series of shape (n_samples) or
the string label of a column in X containing the labels</p>
</dd>
<dt><strong>train_size</strong><span class="classifier">float or int, optional</span></dt><dd><p>If float, represents the proportion [0, 1] of the data
to be assigned to the training set. If an int, represents the number
of instances to be assigned to the training set. Defaults to 0.8</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">bool, optional</span></dt><dd><p>Whether or not to shuffle inputs before splitting</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int, optional</span></dt><dd><p>If shuffle is true, seeds the generator. Unseeded by default</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_train, X_test, y_train, y_test</strong><span class="classifier">cudf.DataFrame</span></dt><dd><p>Partitioned dataframes. If <cite>y</cite> was provided as a column name, the
column was dropped from the <a href="#id1"><span class="problematic" id="id2">`</span></a>X`s</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div></blockquote>
</div>
<div class="section" id="label-encoding">
<h3>Label Encoding<a class="headerlink" href="#label-encoding" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="class">
<dt id="cuml.preprocessing.LabelEncoder">
<em class="property">class </em><code class="sig-prename descclassname">cuml.preprocessing.</code><code class="sig-name descname">LabelEncoder</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.preprocessing.LabelEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>An nvcategory based implementation of ordinal label encoding</p>
<p class="rubric">Examples</p>
<p>Converting a categorical implementation to a numerical one</p>
<p>Output:</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.fit" title="cuml.preprocessing.LabelEncoder.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.fit_transform" title="cuml.preprocessing.LabelEncoder.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.inverse_transform" title="cuml.preprocessing.LabelEncoder.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.transform" title="cuml.preprocessing.LabelEncoder.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cuml.preprocessing.LabelEncoder.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">y: cudf.dataframe.series.Series</em><span class="sig-paren">)</span> &#x2192; 'LabelEncoder'<a class="headerlink" href="#cuml.preprocessing.LabelEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a LabelEncoder (nvcategory) instance to a set of categories</p>
<dl class="simple">
<dt>y<span class="classifier">cudf.Series</span></dt><dd><p>Series containing the categories to be encoded. It’s elements
may or may not be unique</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>self</strong><span class="classifier">LabelEncoder</span></dt><dd><p>A fitted instance of itself to allow method chaining</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.preprocessing.LabelEncoder.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">y: cudf.dataframe.series.Series</em><span class="sig-paren">)</span> &#x2192; cudf.dataframe.series.Series<a class="headerlink" href="#cuml.preprocessing.LabelEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Simultaneously fit and transform an input</p>
<p>This is functionally equivalent to (but faster than)
<cite>LabelEncoder().fit(y).transform(y)</cite></p>
</dd></dl>

<dl class="method">
<dt id="cuml.preprocessing.LabelEncoder.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">y: cudf.dataframe.series.Series</em><span class="sig-paren">)</span> &#x2192; cudf.dataframe.series.Series<a class="headerlink" href="#cuml.preprocessing.LabelEncoder.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Revert ordinal label to original label</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cudf.Series, dtype=int32</span></dt><dd><p>Ordinal labels to be reverted</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>reverted</strong><span class="classifier">cudf.Series</span></dt><dd><p>Reverted labels</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.preprocessing.LabelEncoder.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">y: cudf.dataframe.series.Series</em><span class="sig-paren">)</span> &#x2192; cudf.dataframe.series.Series<a class="headerlink" href="#cuml.preprocessing.LabelEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform an input into its categorical keys.</p>
<p>This is intended for use with small inputs relative to the size of the
dataset. For fitting and transforming an entire dataset, prefer
<cite>fit_transform</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cudf.Series</span></dt><dd><p>Input keys to be transformed. Its values should match the
categories given to <cite>fit</cite></p>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>——</strong></dt><dd></dd>
<dt><strong>encoded</strong><span class="classifier">cudf.Series</span></dt><dd><p>The ordinally encoded input series</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><dl class="simple">
<dt>KeyError</dt><dd><p>if a category appears that was not seen in <cite>fit</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></blockquote>
</div>
<div class="section" id="dataset-generation">
<h3>Dataset Generation<a class="headerlink" href="#dataset-generation" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="method">
<dt id="cuml.datasets.make_blobs">
<code class="sig-prename descclassname">datasets.</code><code class="sig-name descname">make_blobs</code><span class="sig-paren">(</span><em class="sig-param">n_samples=100</em>, <em class="sig-param">n_features=2</em>, <em class="sig-param">centers=None</em>, <em class="sig-param">cluster_std=1.0</em>, <em class="sig-param">center_box=(-10.0</em>, <em class="sig-param">10.0)</em>, <em class="sig-param">shuffle=True</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">dtype=u'single'</em>, <em class="sig-param">handle=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.datasets.make_blobs" title="Permalink to this definition">¶</a></dt>
<dd><p>Generator of datasets composed of isotropic Gaussian distributed clusters
in GPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int (default = 100)</span></dt><dd><p>Total number of points equally divided among clusters. Alternatively,
it is the total number of rows of the dataset and labels.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of features for each sample. Alternatively, the number of
columns in the resulting dataset.</p>
</dd>
<dt><strong>centers</strong><span class="classifier">int or array-like (device or host) shape = (n_samples, n_features)  # noqa</span></dt><dd><p>The number of centers to generate, or the fixed center locations.
If centers is None, 3 centers are generated.</p>
</dd>
<dt><strong>cluster_std</strong><span class="classifier">float or array-like (device or host) (default = 1.0)</span></dt><dd><p>The standard deviation of the clusters.</p>
</dd>
<dt><strong>center_box</strong><span class="classifier">tuple of floats (min, max), optional (default = (-10.0, 10.0))</span></dt><dd><p>The bounding box for cluster centers when generated at random.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Whether to shuffle the samples.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None (default)</span></dt><dd><p>Seed for the random number generator for dataset creation</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="o">-</span><span class="mf">6.4611025</span>   <span class="mf">2.980582</span>  <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">1.8473494</span>   <span class="mf">6.4483595</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.48936838</span>  <span class="mf">5.255189</span>  <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">6.0078964</span>   <span class="mf">0.59910655</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">3.7753344</span>   <span class="mf">7.0041647</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.6350849</span>   <span class="mf">5.1219263</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">4.675709</span>    <span class="mf">3.0528255</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">5.933864</span>    <span class="mf">2.0036478</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.11404657</span>  <span class="mf">4.69242</span>   <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.23619342</span>  <span class="mf">4.699105</span>  <span class="p">]]</span>

<span class="p">[</span><span class="mi">0</span> <span class="mi">2</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">2</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</div></blockquote>
</div>
</div>
<div class="section" id="regression-and-classification">
<h2>Regression and Classification<a class="headerlink" href="#regression-and-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.LinearRegression">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">LinearRegression</code><a class="headerlink" href="#cuml.LinearRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>LinearRegression is a simple machine learning model where the response y is
modelled by a linear combination of the predictors in X.</p>
<p>cuML’s LinearRegression expects either a cuDF DataFrame or a NumPy matrix
and provides 2 algorithms SVD and Eig to fit a linear model. SVD is more
stable, but Eig (default) is much faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algorithm</strong><span class="classifier">‘eig’ or ‘svd’ (default = ‘eig’)</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much
faster.
SVD is slower, but guaranteed to be stable.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, LinearRegression tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s
L2 norm.
If False, no scaling will be done.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>LinearRegression suffers from multicollinearity (when columns are
correlated with each other), and variance explosions from outliers.
Consider using Ridge Regression to fix the multicollinearity problem, and
consider maybe first DBSCAN to remove the outliers, or statistical analysis
to filter possible outliers.</p>
<p><strong>Applications of LinearRegression</strong></p>
<blockquote>
<div><p>LinearRegression is used in regression tasks where one wants to predict
say sales or house prices. It is also used in extrapolation or time
series tasks, dynamic systems modelling and many other machine learning
tasks. This model should be first tried if the machine learning problem
is a regression task (predicting a continuous variable).</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">scikitlearn’s OLS</a>.</p>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/cuml/blob/master/python/notebooks/linear_regression_demo.ipynb">the OLS notebook</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
                      <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;eig&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predictions:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">1.0000001</span>
            <span class="mi">1</span> <span class="mf">1.9999998</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>

<span class="n">Predictions</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">15.999999</span>
            <span class="mi">1</span> <span class="mf">14.999999</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <a href="#id11"><span class="problematic" id="id12">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LinearRegression.fit" title="cuml.LinearRegression.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LinearRegression.get_params" title="cuml.LinearRegression.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LinearRegression.predict" title="cuml.LinearRegression.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LinearRegression.set_params" title="cuml.LinearRegression.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.LinearRegression.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LinearRegression.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LinearRegression.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
</dl>
<dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LinearRegression.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="logistic-regression">
<h3>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.LogisticRegression">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">LogisticRegression</code><a class="headerlink" href="#cuml.LogisticRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>LogisticRegression is a linear model that is used to model probability of
occurrence of certain events, for example probability of success or fail of
an event.</p>
<p>cuML’s LogisticRegression can take array-like objects, either in host as
NumPy arrays or in device (as Numba or __cuda_array_interface__ compliant).
It provides both single-class (using sigmoid loss) and multiple-class
(using softmax loss) variants, depending on the input variables.</p>
<p>Only one solver option is currently available: Quasi-Newton (QN)
algorithms. Even though it is presented as a single option, this solver
resolves to two different algorithms underneath:</p>
<ul class="simple">
<li><p>Orthant-Wise Limited Memory Quasi-Newton (OWL-QN) if there is l1</p></li>
</ul>
<p>regularization
- Limited Memory BFGS (L-BFGS) otherwise.</p>
<p>Note that, just like in Scikit-learn, the bias will not be regularized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>penalty: ‘none’, ‘l1’, ‘l2’, ‘elasticnet’ (default = ‘l2’)</strong></dt><dd><p>Used to specify the norm used in the penalization.
If ‘none’ or ‘l2’ are selected, then L-BFGS solver will be used.
If ‘l1’ is selected, solver OWL-QN will be used.
If ‘elasticnet’ is selected, OWL-QN will be used if l1_ratio &gt; 0,
otherwise L-BFGS will be used.</p>
</dd>
<dt><strong>tol: float (default = 1e-4)</strong></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>C: float (default = 1.0)</strong></dt><dd><p>Inverse of regularization strength; must be a positive float.</p>
</dd>
<dt><strong>fit_intercept: boolean (default = True)</strong></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>class_weight: None</strong></dt><dd><p>Custom class weighs are currently not supported.</p>
</dd>
<dt><strong>max_iter: int (default = 1000)</strong></dt><dd><p>Maximum number of iterations taken for the solvers to converge.</p>
</dd>
<dt><strong>verbose: bool (optional, default False)</strong></dt><dd><p>Controls verbosity of logging.</p>
</dd>
<dt><strong>l1_ratio: float or None, optional (default=None)</strong></dt><dd><p>The Elastic-Net mixing parameter, with <cite>0 &lt;= l1_ratio &lt;= 1</cite></p>
</dd>
<dt><strong>solver: ‘qn’, ‘lbfgs’, ‘owl’ (default=qn).</strong></dt><dd><p>Algorithm to use in the optimization problem. Currently only <cite>qn</cite> is
supported, which automatically selects either L-BFGS or OWL-QN
depending on the condictions of the l1 regularization described
above. Options ‘lbfgs’ and ‘owl’ are just convenience values that
end up using the same solver following the same rules.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>cuML’s LogisticRegression uses a different solver that the equivalent
Scikit-learn except when there is no penalty and <cite>solver=lbfgs</cite> is
chosen in Scikit-learn. This can cause (smaller) differences in the
coefficients and predictions of the model, similar to difference when
using different solvers in Scikit-learn.</p>
<p>For additional docs, see Scikit-learn’s LogistRegression
&lt;<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>&gt;`_.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># Both import methods supported</span>
<span class="c1"># from cuml import LogisticRegression</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predictions:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_: device array, shape (n_classes, n_features)</strong></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_: device array (n_classes, 1)</strong></dt><dd><p>The independent term. If <a href="#id13"><span class="problematic" id="id14">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LogisticRegression.fit" title="cuml.LogisticRegression.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LogisticRegression.predict" title="cuml.LogisticRegression.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.LogisticRegression.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LogisticRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LogisticRegression.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LogisticRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="ridge-regression">
<h3>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.Ridge">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">Ridge</code><a class="headerlink" href="#cuml.Ridge" title="Permalink to this definition">¶</a></dt>
<dd><p>Ridge extends LinearRegression by providing L2 regularization on the
coefficients when predicting response y with a linear combination of the
predictors in X. It can reduce the variance of the predictors, and improves
the conditioning of the problem.</p>
<p>cuML’s Ridge an array-like object or cuDF DataFrame, and provides 3
algorithms: SVD, Eig and CD to fit a linear model. SVD is more stable,
but Eig (default) is much faster. CD uses Coordinate Descent and can be
faster when data is large.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float or double</span></dt><dd><p>Regularization strength - must be a positive float. Larger values
specify stronger regularization. Array input will be supported later.</p>
</dd>
<dt><strong>solver</strong><span class="classifier">‘eig’ or ‘svd’ or ‘cd’ (default = ‘eig’)</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much
faster.
SVD is slower, but guaranteed to be stable.
CD or Coordinate Descent is very fast and is suitable for large
problems.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Ridge tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Ridge provides L2 regularization. This means that the coefficients can
shrink to become very small, but not zero. This can cause issues of
interpretabiliy on the coefficients.
Consider using Lasso, or thresholding small coefficients to zero.</p>
<p><strong>Applications of Ridge</strong></p>
<blockquote>
<div><p>Ridge Regression is used in the same way as LinearRegression, but is
used frequently as it does not suffer from multicollinearity issues.
Ridge is used in insurance premium prediction, stock market analysis
and much more.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/ridge_regression_demo.ipynb">scikitlearn’s Ridge</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-5</span><span class="p">])</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
              <span class="n">solver</span> <span class="o">=</span> <span class="s2">&quot;eig&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_ridge</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_ridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predictions:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">1.0000001</span>
            <span class="mi">1</span> <span class="mf">1.9999998</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">15.999999</span>
            <span class="mi">1</span> <span class="mf">14.999999</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <a href="#id15"><span class="problematic" id="id16">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Ridge.fit" title="cuml.Ridge.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Ridge.get_params" title="cuml.Ridge.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Ridge.predict" title="cuml.Ridge.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Ridge.set_params" title="cuml.Ridge.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.Ridge.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Ridge.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Ridge.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Ridge.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="lasso-regression">
<h3>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.Lasso">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">Lasso</code><a class="headerlink" href="#cuml.Lasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Lasso extends LinearRegression by providing L1 regularization on the
coefficients when predicting response y with a linear combination of the
predictors in X. It can zero some of the coefficients for feature
selection, and improves the conditioning of the problem.</p>
<p>cuML’s Lasso an array-like object or cuDF DataFrame, and
uses coordinate descent to fit a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float or double</span></dt><dd><p>Constant that multiplies the L1 term. Defaults to 1.0.
alpha = 0 is equivalent to an ordinary least square, solved by the
LinearRegression class.
For numerical reasons, using alpha = 0 with the Lasso class is not
advised.
Given this, you should use the LinearRegression class.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Lasso tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float, optional</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than
tol, the optimization code checks the dual gap for optimality and
continues until it is smaller than tol.</p>
</dd>
<dt><strong>selection</strong><span class="classifier">str, default ‘cyclic’</span></dt><dd><p>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default.
This (setting to ‘random’) often leads to significantly faster
convergence especially when tol is higher than 1e-4.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">ls</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_lasso</span> <span class="o">=</span> <span class="n">ls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_lasso</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">0.85</span>
            <span class="mi">1</span> <span class="mf">0.0</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">0.149999</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">2.7</span>
            <span class="mi">1</span> <span class="mf">1.85</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <a href="#id17"><span class="problematic" id="id18">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
<dt><strong>For additional docs, see `scikitlearn’s Lasso</strong></dt><dd></dd>
<dt><strong>&lt;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&gt;`_.</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Lasso.fit" title="cuml.Lasso.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Lasso.get_params" title="cuml.Lasso.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Lasso.predict" title="cuml.Lasso.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Lasso.set_params" title="cuml.Lasso.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.Lasso.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Lasso.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will, when necessary,
convert y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Lasso.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Lasso.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Lasso.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Lasso.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Lasso.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Lasso.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="elasticnet-regression">
<h3>ElasticNet Regression<a class="headerlink" href="#elasticnet-regression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.ElasticNet">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">ElasticNet</code><a class="headerlink" href="#cuml.ElasticNet" title="Permalink to this definition">¶</a></dt>
<dd><p>ElasticNet extends LinearRegression with combined L1 and L2 regularizations
on the coefficients when predicting response y with a linear combination of
the predictors in X. It can reduce the variance of the predictors, force
some coefficients to be smaell, and improves the conditioning of the
problem.</p>
<p>cuML’s ElasticNet an array-like object or cuDF DataFrame, uses coordinate
descent to fit a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float or double</span></dt><dd><p>Constant that multiplies the L1 term. Defaults to 1.0.
alpha = 0 is equivalent to an ordinary least square, solved by the
LinearRegression object.
For numerical reasons, using alpha = 0 with the Lasso object is not
advised.
Given this, you should use the LinearRegression object.</p>
</dd>
<dt><strong>l1_ratio: The ElasticNet mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</strong></dt><dd><p>For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is
an L1 penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Lasso tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float, optional</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than
tol, the optimization code checks the dual gap for optimality and
continues until it is smaller than tol.</p>
</dd>
<dt><strong>selection</strong><span class="classifier">str, default ‘cyclic’</span></dt><dd><p>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default.
This (setting to ‘random’) often leads to significantly faster
convergence especially when tol is higher than 1e-4.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">enet</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_enet</span> <span class="o">=</span> <span class="n">enet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_enet</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_enet</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_enet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">0.448408</span>
            <span class="mi">1</span> <span class="mf">0.443341</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">0.1082506</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">3.67018</span>
            <span class="mi">1</span> <span class="mf">3.22177</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <a href="#id19"><span class="problematic" id="id20">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
<dt><strong>For additional docs, see `scikitlearn’s ElasticNet</strong></dt><dd></dd>
<dt><strong>&lt;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html&gt;`_.</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ElasticNet.fit" title="cuml.ElasticNet.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ElasticNet.get_params" title="cuml.ElasticNet.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ElasticNet.predict" title="cuml.ElasticNet.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ElasticNet.set_params" title="cuml.ElasticNet.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.ElasticNet.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ElasticNet.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will, when necessary,
convert y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ElasticNet.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ElasticNet.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ElasticNet.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ElasticNet.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ElasticNet.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ElasticNet.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stochastic-gradient-descent">
<h3>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.SGD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">SGD</code><a class="headerlink" href="#cuml.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Stochastic Gradient Descent is a very common machine learning algorithm
where one optimizes some cost function via gradient steps. This makes SGD
very attractive for large problems when the exact solution is hard or even
impossible to find.</p>
<p>cuML’s SGD algorithm accepts a numpy matrix or a cuDF DataFrame as the
input dataset. The SGD algorithm currently works with linear regression,
ridge regression and SVM models.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>loss</strong><span class="classifier">‘hinge’, ‘log’, ‘squared_loss’ (default = ‘squared_loss’)</span></dt><dd><p>‘hinge’ uses linear SVM
‘log’ uses logistic regression
‘squared_loss’ uses linear regression</p>
</dd>
<dt><strong>penalty: ‘none’, ‘l1’, ‘l2’, ‘elasticnet’ (default = ‘none’)</strong></dt><dd><p>‘none’ does not perform any regularization
‘l1’ performs L1 norm (Lasso) which minimizes the sum of the abs value
of coefficients
‘l2’ performs L2 norm (Ridge) which minimizes the sum of the square of
the coefficients
‘elasticnet’ performs Elastic Net regularization which is a weighted
average of L1 and L2 norms</p>
</dd>
<dt><strong>alpha: float (default = 0.0001)</strong></dt><dd><p>The constant value which decides the degree of regularization</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The number of times the model should iterate through the entire dataset
during training (default = 1000)</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>True, shuffles the training data after each epoch
False, does not shuffle the training data after each epoch</p>
</dd>
<dt><strong>eta0</strong><span class="classifier">float (default = 0.0)</span></dt><dd><p>Initial learning rate</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The exponent used for calculating the invscaling learning rate</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">‘optimal’, ‘constant’, ‘invscaling’,</span></dt><dd><blockquote>
<div><p>‘adaptive’ (default = ‘constant’)</p>
</div></blockquote>
<p>optimal option supported in the next version
constant keeps the learning rate constant
adaptive changes the learning rate if the training loss or the
validation accuracy does not improve for n_iter_no_change epochs.
The old learning rate is generally divide by 5</p>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int (default = 5)</span></dt><dd><p>the number of epochs to train without any imporvement in the model</p>
</dd>
<dt><strong>Notes</strong></dt><dd></dd>
<dt><strong>——</strong></dt><dd></dd>
<dt><strong>For additional docs, see `scikitlearn’s OLS</strong></dt><dd></dd>
<dt><strong>&lt;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&gt;</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Output:
.. code-block:: python</p>
<blockquote>
<div><p>cuML intercept :  0.004561662673950195
cuML coef :  0      0.9834546</p>
<blockquote>
<div><blockquote>
<div><p>1    0.010128272</p>
</div></blockquote>
<p>dtype: float32</p>
</div></blockquote>
<p>cuML predictions :  [3.0055666 2.0221121]</p>
</div></blockquote>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.SGD.fit" title="cuml.SGD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.SGD.predict" title="cuml.SGD.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.SGD.predictClass" title="cuml.SGD.predictClass"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predictClass</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.SGD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.SGD.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
</dl>
<dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.SGD.predictClass">
<code class="sig-name descname">predictClass</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD.predictClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predictClass method will automatically
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
<dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.ensemble.RandomForestClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.ensemble.</code><code class="sig-name descname">RandomForestClassifier</code><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Random Forest classifier model which fits multiple decision
tree classifiers in an ensemble.</p>
<p>Note that the underlying algorithm for tree node splits differs from that
used in scikit-learn. By default, the cuML Random Forest uses a
histogram-based algorithms to determine splits, rather than an exact
count. You can tune the size of the histograms with the n_bins parameter.</p>
<p><strong>Known Limitations</strong>: This is an initial release of the cuML
Random Forest code. It contains a few known limitations:</p>
<blockquote>
<div><ul class="simple">
<li><p>Inference/prediction takes place on the CPU. A GPU-based inference
solution based on the forest inference library is planned for a
near-future release.</p></li>
<li><p>Instances of RandomForestClassifier cannot be pickled currently.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 10)</span></dt><dd><p>number of trees in the forest.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
<dt><strong>split_criterion: The criterion used to split nodes.</strong></dt><dd><p>0 for GINI, 1 for ENTROPY
2 and 3 not valid for classification
(default = 0)</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">0 for HIST and 1 for GLOBAL_QUANTILE</span></dt><dd><p>(default = 1)
the algorithm to determine how nodes are split in the tree.
HIST curently uses a slower tree-building algorithm
so GLOBAL_QUANTILE is recommended for most cases.</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If set, each tree in the forest is built
on a bootstrapped sample with replacement.
If false, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = 16)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure),
if -1. Unlimited depth is not supported with split_algo=1.
<em>Note that this default differs from scikit-learn’s
random forest, which defaults to unlimited depth.</em></p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited,
if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int or float (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed
to split a node.
If int then number of sample rows
If float the min_rows_per_sample*n_rows</p>
</dd>
<dt><strong>quantile_per_tree</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Whether quantile is computed for individal trees in RF.
Only relevant for GLOBAL_QUANTILE split_algo.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span> <span class="k">as</span> <span class="n">cuRFC</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="n">cuml_model</span> <span class="o">=</span> <span class="n">cuRFC</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                   <span class="n">n_bins</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                   <span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">cuml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">cuml_predict</span> <span class="o">=</span> <span class="n">cuml_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted labels : &quot;</span><span class="p">,</span> <span class="n">cuml_predict</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Predicted labels :  [0 1 0 1 0 1 0 1 0 1]
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.fit" title="cuml.ensemble.RandomForestClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Perform Random Forest Classification on the input data</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.get_params" title="cuml.ensemble.RandomForestClassifier.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Returns the value of all parameters required to configure this estimator as a dictionary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.predict" title="cuml.ensemble.RandomForestClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.print_detailed" title="cuml.ensemble.RandomForestClassifier.print_detailed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_detailed</span></code></a>(self)</p></td>
<td><p>prints the detailed information about the forest used to train and test the Random Forest model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.print_summary" title="cuml.ensemble.RandomForestClassifier.print_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_summary</span></code></a>(self)</p></td>
<td><p>prints the summary of the forest used to train and test the model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.score" title="cuml.ensemble.RandomForestClassifier.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X, y)</p></td>
<td><p>Calculates the accuracy metric score of the model for X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.set_params" title="cuml.ensemble.RandomForestClassifier.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sets the value of parameters required to configure this estimator, it functions similar to the sklearn set_params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Random Forest Classification on the input data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (int32) of shape (n_samples, 1).
Acceptable formats: NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy
These labels should be contiguous integers from 0 to n_classes.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of all parameters
required to configure this estimator as a dictionary.
Parameters
———–
deep : boolean (default = True)</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y: NumPy</dt><dd><p>Dense vector (int) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.print_detailed">
<code class="sig-name descname">print_detailed</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.print_detailed" title="Permalink to this definition">¶</a></dt>
<dd><p>prints the detailed information about the forest used to
train and test the Random Forest model</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.print_summary">
<code class="sig-name descname">print_summary</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.print_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>prints the summary of the forest used to train and test the model</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the accuracy metric score of the model for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y: NumPy</dt><dd><p>Dense vector (int) of shape (n_samples, 1)</p>
</dd>
</dl>
<p>accuracy of the model</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestClassifier.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of parameters required to
configure this estimator, it functions similar to
the sklearn set_params.
Parameters
———–
params : dict of new params</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="cuml.ensemble.RandomForestRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.ensemble.</code><code class="sig-name descname">RandomForestRegressor</code><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Random Forest regressor model which fits multiple decision
trees in an ensemble.
Note that the underlying algorithm for tree node splits differs from that
used in scikit-learn. By default, the cuML Random Forest uses a
histogram-based algorithms to determine splits, rather than an exact
count. You can tune the size of the histograms with the n_bins parameter.</p>
<p><strong>Known Limitations</strong>: This is an initial release of the cuML
Random Forest code. It contains a few known limitations:</p>
<blockquote>
<div><ul class="simple">
<li><p>Inference/prediction takes place on the CPU. A GPU-based inference
solution based on the forest inference library is planned for a
near-future release.</p></li>
<li><p>Instances of RandomForestRegressor cannot be pickled currently.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 10)</span></dt><dd><p>number of trees in the forest.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">int (default = 1)</span></dt><dd><p>0 for HIST, 1 for GLOBAL_QUANTILE
The type of algorithm to be used to create the trees.
HIST curently uses a slower tree-building algorithm
so GLOBAL_QUANTILE is recommended for most cases.</p>
</dd>
<dt><strong>split_criterion: int (default = 2)</strong></dt><dd><p>The criterion used to split nodes.
0 for GINI, 1 for ENTROPY,
2 for MSE, or 3 for MAE
0 and 1 not valid for regression</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If set, each tree in the forest is built
on a bootstrapped sample with replacement.
If false, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = 16)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure),
if -1. Unlimited depth is not supported with split_algo=1.
<em>Note that this default differs from scikit-learn’s
random forest, which defaults to unlimited depth.</em></p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited,
if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int or float or string or None (default = ‘auto’)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.
If int then max_features/n_features.
If float then max_features is a fraction.
If ‘auto’ then max_features=n_features which is 1.0.
If ‘sqrt’ then max_features=1/sqrt(n_features).
If ‘log2’ then max_features=log2(n_features)/n_features.
If None, then max_features=n_features which is 1.0.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int or float (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed
to split a node.
If int then number of sample rows
If float the min_rows_per_sample*n_rows</p>
</dd>
<dt><strong>accuracy_metric</strong><span class="classifier">string (default = ‘mse’)</span></dt><dd><p>Decides the metric used to evaluate the performance
of the model.
for median of abs error : ‘median_ae’
for mean of abs error : ‘mean_ae’
for mean square error’ : ‘mse’</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.test.utils</span> <span class="kn">import</span> <span class="n">get_handle</span>
<span class="kn">from</span> <span class="nn">cuml.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span> <span class="k">as</span> <span class="n">curfc</span>
<span class="kn">from</span> <span class="nn">cuml.test.utils</span> <span class="kn">import</span> <span class="n">get_handle</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">30</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">40</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">cuml_model</span> <span class="o">=</span> <span class="n">curfc</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                   <span class="n">split_algo</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">min_rows_per_node</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                   <span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">accuracy_metric</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
<span class="n">cuml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">cuml_score</span> <span class="o">=</span> <span class="n">cuml_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;MSE score of cuml : &quot;</span><span class="p">,</span> <span class="n">cuml_score</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MSE</span> <span class="n">score</span> <span class="n">of</span> <span class="n">cuml</span> <span class="p">:</span>  <span class="mf">0.1123437201231765</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.fit" title="cuml.ensemble.RandomForestRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Perform Random Forest Classification on the input data</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.get_params" title="cuml.ensemble.RandomForestRegressor.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Returns the value of all parameters required to configure this estimator as a dictionary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.predict" title="cuml.ensemble.RandomForestRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.print_detailed" title="cuml.ensemble.RandomForestRegressor.print_detailed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_detailed</span></code></a>(self)</p></td>
<td><p>prints the detailed information about the forest used to train and test the Random Forest model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.print_summary" title="cuml.ensemble.RandomForestRegressor.print_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_summary</span></code></a>(self)</p></td>
<td><p>prints the summary of the forest used to train and test the model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.score" title="cuml.ensemble.RandomForestRegressor.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X, y)</p></td>
<td><p>Calculates the accuracy metric score of the model for X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.set_params" title="cuml.ensemble.RandomForestRegressor.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sets the value of parameters required to configure this estimator, it functions similar to the sklearn set_params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Random Forest Classification on the input data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (int32) of shape (n_samples, 1).
Acceptable formats: NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy
These labels should be contiguous integers from 0 to n_classes.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of all parameters
required to configure this estimator as a dictionary.
Parameters
———–
deep : boolean (default = True)</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y: NumPy</dt><dd><p>Dense vector (int) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.print_detailed">
<code class="sig-name descname">print_detailed</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.print_detailed" title="Permalink to this definition">¶</a></dt>
<dd><p>prints the detailed information about the forest used to
train and test the Random Forest model</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.print_summary">
<code class="sig-name descname">print_summary</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.print_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>prints the summary of the forest used to train and test the model</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the accuracy metric score of the model for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y: NumPy</dt><dd><p>Dense vector (int) of shape (n_samples, 1)</p>
</dd>
</dl>
<p>mean_square_error : float or
median_abs_error : float or
mean_abs_error : float</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ensemble.RandomForestRegressor.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of parameters required to
configure this estimator, it functions similar to
the sklearn set_params.
Parameters
———–
params : dict of new params</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="forest-inferencing">
<h3>Forest Inferencing<a class="headerlink" href="#forest-inferencing" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.ForestInference">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">ForestInference</code><a class="headerlink" href="#cuml.ForestInference" title="Permalink to this definition">¶</a></dt>
<dd><p>ForestInference provides GPU-accelerated inference (prediction)
for random forest and boosted decision tree models.</p>
<p>This module does not support training models. Rather, users should
train a model in another package and save it in a
treelite-compatible format. (See <a class="reference external" href="https://github.com/dmlc/treelite">https://github.com/dmlc/treelite</a>)
Currently, LightGBM and XGBoost GBDT and random forest models are
supported.</p>
<p>Users typically create a ForestInference object by loading a
saved model file with ForestInference.load. The resulting object
provides a <cite>predict</cite> method for carrying out inference.</p>
<dl class="simple">
<dt><strong>Known limitations</strong>:</dt><dd><ul class="simple">
<li><p>Trees are represented as complete binary trees, so a tree of depth k
will be stored in (2**k) - 1 nodes. This will be less space-efficient
for sparse trees.</p></li>
<li><p>While treelite supports additional formats, only XGBoost and LightGBM
are tested in FIL currently.</p></li>
<li><p>LightGBM categorical features are not supported</p></li>
<li><p>Inference uses a dense matrix format, which is efficient for many
problems but will be suboptimal for sparse datasets.</p></li>
<li><p>Only binary classification and regression are supported.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>For additional usage examples, see the sample notebook at
<a class="reference external" href="https://github.com/rapidsai/notebooks/blob/branch-0.9/cuml/forest_inference_demo.ipynb">https://github.com/rapidsai/notebooks/blob/branch-0.9/cuml/forest_inference_demo.ipynb</a> # noqa</p>
<p>In the example below, synthetic data is copied to the host before
infererence. ForestInference can also accept a numpy array directly at the
cost of a slight performance overhead.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assume that the file &#39;xgb.model&#39; contains a classifier model that was</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># previously saved by XGBoost&#39;s save_model function.</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">sklearn</span><span class="o">,</span> <span class="nn">sklearn.datasets</span><span class="o">,</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numba</span> <span class="k">import</span> <span class="n">cuda</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml</span> <span class="k">import</span> <span class="n">ForestInference</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;xgb.model&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_gpu</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fm</span> <span class="o">=</span> <span class="n">ForestInference</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">output_class</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fil_preds_gpu</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_gpu</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                             <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fil_preds_gpu</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ForestInference.load" title="cuml.ForestInference.load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load</span></code></a>(filename[, output_class, threshold, …])</p></td>
<td><p>Returns a FIL instance containing the forest saved in ‘filename’ This uses Treelite to load the saved model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ForestInference.load_from_treelite_model" title="cuml.ForestInference.load_from_treelite_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_treelite_model</span></code></a>(self, model, …[, …])</p></td>
<td><p>Creates a FIL model using the treelite model passed to the function.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ForestInference.predict" title="cuml.ForestInference.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, preds])</p></td>
<td><p>Predicts the labels for X with the loaded forest model.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.ForestInference.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param">filename</em>, <em class="sig-param">output_class=False</em>, <em class="sig-param">threshold=0.50</em>, <em class="sig-param">algo=u'TREE_REORG'</em>, <em class="sig-param">model_type=u'xgboost'</em>, <em class="sig-param">handle=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ForestInference.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a FIL instance containing the forest saved in ‘filename’
This uses Treelite to load the saved model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>filename</strong><span class="classifier">str</span></dt><dd><p>Path to saved model file in a treelite-compatible format
(See <a class="reference external" href="https://treelite.readthedocs.io/en/latest/treelite-api.html">https://treelite.readthedocs.io/en/latest/treelite-api.html</a></p>
</dd>
<dt><strong>output_class</strong><span class="classifier">bool</span></dt><dd><p>If true, return a 1 or 0 depending on whether the raw prediction
exceeds the threshold. If False, just return the raw prediction.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float</span></dt><dd><p>Cutoff value above which a prediction is set to 1.0
Only used if the model is classification and output_class is True</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string</span></dt><dd><p>Which inference algorithm to use.
See documentation in FIL.load_from_treelite_model</p>
</dd>
<dt><strong>model_type</strong><span class="classifier">str</span></dt><dd><p>Format of saved treelite model to load.
Can be ‘xgboost’, ‘lightgbm’, or ‘protobuf’</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ForestInference.load_from_treelite_model">
<code class="sig-name descname">load_from_treelite_model</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">model</em>, <em class="sig-param">output_class</em>, <em class="sig-param">algo=u'TREE_REORG'</em>, <em class="sig-param">threshold=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ForestInference.load_from_treelite_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a FIL model using the treelite model
passed to the function.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>model</strong><span class="classifier">the trained model information in the treelite format</span></dt><dd><p>loaded from a saved model using the treelite API
<a class="reference external" href="https://treelite.readthedocs.io/en/latest/treelite-api.html">https://treelite.readthedocs.io/en/latest/treelite-api.html</a></p>
</dd>
<dt><strong>output_class: boolean</strong></dt><dd><p>If true, return a 1 or 0 depending on whether the raw prediction
exceeds the threshold. If False, just return the raw prediction.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string name of the algo from (from algo_t enum)</span></dt><dd><p>‘NAIVE’ - simple inference using shared memory
‘TREE_REORG’ - similar to naive but trees rearranged to be more</p>
<blockquote>
<div><p>coalescing-friendly</p>
</div></blockquote>
<dl class="simple">
<dt>‘BATCH_TREE_REORG’ - similar to TREE_REORG but predicting</dt><dd><p>multiple rows per thread block</p>
</dd>
</dl>
</dd>
<dt><strong>threshold</strong><span class="classifier">threshold is used to for classification</span></dt><dd><p>applied if output_class == True, else it is ignored</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ForestInference.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">preds=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ForestInference.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X with the loaded forest model.
By default, the result is the raw floating point output
from the model, unless output_class was set to True
during model loading.</p>
<p>See the documentation of ForestInference.load for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy
For optimal performance, pass a device array with C-style layout</p>
</dd>
<dt><strong>preds: gpuarray or cudf.Series, shape = (n_samples,)</strong></dt><dd><p>Optional ‘out’ location to store inference results</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>GPU array of length n_samples with inference results</dt><dd></dd>
<dt>(or ‘preds’ filled with inference results if preds was specified)</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quasi-newton">
<h3>Quasi-Newton<a class="headerlink" href="#quasi-newton" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.QN">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">QN</code><a class="headerlink" href="#cuml.QN" title="Permalink to this definition">¶</a></dt>
<dd><p>Quasi-Newton methods are used to either find zeroes or local maxima
and minima of functions, and used by this class to optimize a cost
function.</p>
<p>Two algorithms are implemented underneath cuML’s QN class, and which one
is executed depends on the following rule:</p>
<ul class="simple">
<li><p>Orthant-Wise Limited Memory Quasi-Newton (OWL-QN) if there is l1</p></li>
</ul>
<p>regularization
- Limited Memory BFGS (L-BFGS) otherwise.</p>
<p>cuML’s QN class can take array-like objects, either in host as
NumPy arrays or in device (as Numba or __cuda_array_interface__ compliant).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss: ‘sigmoid’, ‘softmax’, ‘squared_loss’ (default = ‘squared_loss’)</strong></dt><dd><p>‘sigmoid’ loss used for single class logistic regression
‘softmax’ loss used for multiclass logistic regression
‘normal’ used for normal/square loss</p>
</dd>
<dt><strong>fit_intercept: boolean (default = True)</strong></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>l1_strength: float (default = 0.0)</strong></dt><dd><p>l1 regularization strength (if non-zero, will run OWL-QN, else L-BFGS).
Note, that as in Scikit-learn, the bias will not be regularized.</p>
</dd>
<dt><strong>l2_strength: float (default = 0.0)</strong></dt><dd><p>l2 regularization strength. Note, that as in Scikit-learn, the bias
will not be regularized.</p>
</dd>
<dt><strong>max_iter: int (default = 1000)</strong></dt><dd><p>Maximum number of iterations taken for the solvers to converge.</p>
</dd>
<dt><strong>tol: float (default = 1e-3)</strong></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>linesearch_max_iter: int (default = 50)</strong></dt><dd><p>Max number of linesearch iterations per outer iteration of the
algorithm.</p>
</dd>
<dt><strong>lbfgs_memory: int (default = 5)</strong></dt><dd><p>Rank of the lbfgs inverse-Hessian approximation. Method will use
O(lbfgs_memory * D) memory.</p>
</dd>
<dt><strong>verbose: bool (optional, default False)</strong></dt><dd><p>Controls verbosity of logging.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This class contains implementations of two popular Quasi-Newton methods:
- Limited-memory Broyden Fletcher Goldfarb Shanno (L-BFGS) [Nocedal,
Wright - Numerical Optimization (1999)]
- Orthant-wise limited-memory quasi-newton (OWL-QN) [Andrew, Gao - ICML
2007]
&lt;<a class="reference external" href="https://www.microsoft.com/en-us/research/publication/scalable-training-of-l1-regularized-log-linear-models/">https://www.microsoft.com/en-us/research/publication/scalable-training-of-l1-regularized-log-linear-models/</a>&gt;</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># Both import methods supported</span>
<span class="c1"># from cuml import QN</span>
<span class="kn">from</span> <span class="nn">cuml.solvers</span> <span class="kn">import</span> <span class="n">QN</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">solver</span> <span class="o">=</span> <span class="n">QN</span><span class="p">()</span>
<span class="n">solver</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Note: for now, the coefficients also include the intercept in the</span>
<span class="c1"># last position if fit_intercept=True</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">intercept_</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predictions:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_classes, n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.
Note: shape is (n_classes, n_features + 1) if fit_intercept = True.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array (n_classes, 1)</span></dt><dd><p>The independent term. If <a href="#id21"><span class="problematic" id="id22">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.QN.fit" title="cuml.QN.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.QN.predict" title="cuml.QN.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.QN.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.QN.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.QN.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.QN.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
</dl>
<dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-means-clustering">
<h3>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.KMeans">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">KMeans</code><a class="headerlink" href="#cuml.KMeans" title="Permalink to this definition">¶</a></dt>
<dd><p>KMeans is a basic but powerful clustering method which is optimized via
Expectation Maximization. It randomnly selects K data points in X, and
computes which samples are close to these points.
For every cluster of points, a mean is computed (hence the name), and this
becomes the new centroid.</p>
<p>cuML’s KMeans expects an array-like object or cuDF DataFrame, and supports
the scalable KMeans++ intialization method. This method is more stable
than randomnly selecting K points.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
<dt><strong>n_clusters</strong><span class="classifier">int (default = 8)</span></dt><dd><p>The number of centroids or clusters you want.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 300)</span></dt><dd><p>The more iterations of EM, the more accurate, but slower.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-4)</span></dt><dd><p>Stopping criterion when centroid means do not change much.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">boolean (default = 0)</span></dt><dd><p>If True, prints diagnositc information.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = 1)</span></dt><dd><p>If you want results to be the same when you restart Python, select a
state.</p>
</dd>
<dt><strong>precompute_distances</strong><span class="classifier">boolean (default = ‘auto’)</span></dt><dd><p>Not supported yet.</p>
</dd>
<dt><strong>init</strong><span class="classifier">{‘scalable-kmeans++’, ‘k-means||’ , ‘random’ or an ndarray}</span></dt><dd><blockquote>
<div><p>(default = ‘scalable-k-means++’)</p>
</div></blockquote>
<p>‘scalable-k-means++’ or ‘k-means||’: Uses fast and stable scalable
kmeans++ intialization.
‘random’: Choose ‘n_cluster’ observations (rows) at random from data
for the initial centroids. If an ndarray is passed, it should be of
shape (n_clusters, n_features) and gives the initial centers.</p>
</dd>
<dt><strong>n_init</strong><span class="classifier">int (default = 1)</span></dt><dd><p>Number of times intialization is run. More is slower,
but can be better.</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">“auto”</span></dt><dd><p>Currently uses full EM, but will support others later.</p>
</dd>
<dt><strong>n_gpu</strong><span class="classifier">int (default = 1)</span></dt><dd><p>Number of GPUs to use. Currently uses single GPU, but will support
multiple GPUs later.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>KMeans requires n_clusters to be specified. This means one needs to
approximately guess or know how many clusters a dataset has. If one is not
sure, one can start with a small number of clusters, and visualize the
resulting clusters with PCA, UMAP or T-SNE, and verify that they look
appropriate.</p>
<p><strong>Applications of KMeans</strong></p>
<blockquote>
<div><p>The biggest advantage of KMeans is its speed and simplicity. That is
why KMeans is many practitioner’s first choice of a clustering
algorithm. KMeans has been extensively used when the number of clusters
is approximately known, such as in big data clustering tasks,
image segmentation and medical clustering.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">scikitlearn’s Kmeans</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">cuml.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">np2cudf</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># convert numpy array to cuDF dataframe</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;fea</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span><span class="p">:</span><span class="n">df</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])})</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">column</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
      <span class="n">pdf</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pdf</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np2cudf</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;input:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Calling fit&quot;</span><span class="p">)</span>
<span class="n">kmeans_float</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_gpu</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">kmeans_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;labels:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kmeans_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;cluster_centers:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kmeans_float</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span><span class="p">:</span>

     <span class="mi">0</span>    <span class="mi">1</span>
 <span class="mi">0</span>  <span class="mf">1.0</span>  <span class="mf">1.0</span>
 <span class="mi">1</span>  <span class="mf">1.0</span>  <span class="mf">2.0</span>
 <span class="mi">2</span>  <span class="mf">3.0</span>  <span class="mf">2.0</span>
 <span class="mi">3</span>  <span class="mf">4.0</span>  <span class="mf">3.0</span>

<span class="n">Calling</span> <span class="n">fit</span>

<span class="n">labels</span><span class="p">:</span>

   <span class="mi">0</span>    <span class="mi">0</span>
   <span class="mi">1</span>    <span class="mi">0</span>
   <span class="mi">2</span>    <span class="mi">1</span>
   <span class="mi">3</span>    <span class="mi">1</span>

<span class="n">cluster_centers</span><span class="p">:</span>

   <span class="mi">0</span>    <span class="mi">1</span>
<span class="mi">0</span>  <span class="mf">1.0</span>  <span class="mf">1.5</span>
<span class="mi">1</span>  <span class="mf">3.5</span>  <span class="mf">2.5</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>cluster_centers_</strong><span class="classifier">array</span></dt><dd><p>The coordinates of the final clusters. This represents of “mean” of
each data cluster.</p>
</dd>
<dt><strong>labels_</strong><span class="classifier">array</span></dt><dd><p>Which cluster each datapoint belongs to.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.fit" title="cuml.KMeans.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Compute k-means clustering with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.fit_predict" title="cuml.KMeans.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(self, X)</p></td>
<td><p>Compute cluster centers and predict cluster index for each sample.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.fit_transform" title="cuml.KMeans.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Compute clustering and transform X to cluster-distance space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.get_params" title="cuml.KMeans.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Scikit-learn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.predict" title="cuml.KMeans.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predict the closest cluster each sample in X belongs to.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.score" title="cuml.KMeans.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X)</p></td>
<td><p>Opposite of the value of X on the K-means objective.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.set_params" title="cuml.KMeans.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Scikit-learn style set parameter state to dictionary of params.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.transform" title="cuml.KMeans.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Transform X to a cluster-distance space.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.KMeans.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute k-means clustering with X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.fit_predict">
<code class="sig-name descname">fit_predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cluster centers and predict cluster index for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute clustering and transform X to cluster-distance space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit_transform method will automatically
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Scikit-learn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the closest cluster each sample in X belongs to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>labels</strong><span class="classifier">array</span></dt><dd></dd>
<dt>Which cluster each datapoint belongs to.</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Opposite of the value of X on the K-means objective.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>score: float</dt><dd><p>Opposite of the value of X on the K-means objective.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Scikit-learn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X to a cluster-distance space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dbscan">
<h3>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.DBSCAN">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">DBSCAN</code><a class="headerlink" href="#cuml.DBSCAN" title="Permalink to this definition">¶</a></dt>
<dd><p>DBSCAN is a very powerful yet fast clustering technique that finds clusters
where data is concentrated. This allows DBSCAN to generalize to many
problems if the datapoints tend to congregate in larger groups.</p>
<p>cuML’s DBSCAN expects an array-like object or cuDF DataFrame, and
constructs an adjacency graph to compute the distances between close
neighbours.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>eps</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The maximum distance between 2 points such they reside in the same
neighborhood.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
<dt><strong>min_samples</strong><span class="classifier">int (default = 5)</span></dt><dd><p>The number of samples in a neighborhood such that this group can be
considered as an important core point (including the point itself).</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool</span></dt><dd><p>Whether to print debug spews</p>
</dd>
<dt><strong>max_bytes_per_batch</strong><span class="classifier">(optional) int64</span></dt><dd><p>Calculate batch size using no more than this number of bytes for the
pairwise distance computation. This enables the trade-off between
runtime and memory usage for making the N^2 pairwise distance
computations more tractable for large numbers of samples.
If you are experiencing out of memory errors when running DBSCAN, you
can set this value based on the memory size of your device.
Note: this option does not set the maximum total memory used in the
DBSCAN computation and so this value will not
be able to be set to the total memory available on the device.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>DBSCAN is very sensitive to the distance metric it is used with, and a
large assumption is that datapoints need to be concentrated in groups for
clusters to be constructed.</p>
<p><strong>Applications of DBSCAN</strong></p>
<blockquote>
<div><p>DBSCAN’s main benefit is that the number of clusters is not a
hyperparameter, and that it can find non-linearly shaped clusters.
This also allows DBSCAN to be robust to noise.
DBSCAN has been applied to analyzing particle collisons in the
Large Hadron Collider, customer segmentation in marketing analyses,
and much more.</p>
</div></blockquote>
<p>For an additional example, see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/dbscan_demo.ipynb">the DBSCAN notebook</a>.
For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">scikitlearn’s DBSCAN</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">DBSCAN</span>
<span class="kn">from</span> <span class="nn">cuml.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">dbscan_float</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">min_samples</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>    <span class="mi">0</span>
<span class="mi">1</span>    <span class="mi">1</span>
<span class="mi">2</span>    <span class="mi">2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>labels_</strong><span class="classifier">array</span></dt><dd><p>Which cluster each datapoint belongs to. Noisy samples are labeled as
-1.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.DBSCAN.fit" title="cuml.DBSCAN.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Perform DBSCAN clustering from features.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.DBSCAN.fit_predict" title="cuml.DBSCAN.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(self, X)</p></td>
<td><p>Performs clustering on input_gdf and returns cluster labels.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.DBSCAN.get_param_names" title="cuml.DBSCAN.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.DBSCAN.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform DBSCAN clustering from features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.DBSCAN.fit_predict">
<code class="sig-name descname">fit_predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs clustering on input_gdf and returns cluster labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cuDF Series, shape (n_samples)</span></dt><dd><p>cluster labels</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.DBSCAN.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="dimensionality-reduction-and-manifold-learning">
<h2>Dimensionality Reduction and Manifold Learning<a class="headerlink" href="#dimensionality-reduction-and-manifold-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="principal-component-analysis">
<h3>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.PCA">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">PCA</code><a class="headerlink" href="#cuml.PCA" title="Permalink to this definition">¶</a></dt>
<dd><p>PCA (Principal Component Analysis) is a fundamental dimensionality
reduction technique used to combine features in X in linear combinations
such that each new component captures the most information or variance of
the data. N_components is usually small, say at 3, where it can be used for
data visualization, data compression and exploratory analysis.</p>
<p>cuML’s PCA expects an array-like object or cuDF DataFrame, and provides 2
algorithms Full and Jacobi. Full (default) uses a full eigendecomposition
then selects the top K eigenvectors. The Jacobi algorithm is much faster
as it iteratively tries to correct the top K eigenvectors, but might be
less accurate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>copy</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, then copies data then removes mean from data. False might
cause data to be overwritten with its mean centered version.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
<dt><strong>iterated_power</strong><span class="classifier">int (default = 15)</span></dt><dd><p>Used in Jacobi solver. The more iterations, the more accurate, but
slower.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want.
Must be &lt;= number(columns).</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int / None (default = None)</span></dt><dd><p>If you want results to be the same when you restart Python, select a
state.</p>
</dd>
<dt><strong>svd_solver</strong><span class="classifier">‘full’ or ‘jacobi’ or ‘auto’ (default = ‘full’)</span></dt><dd><p>Full uses a eigendecomposition of the covariance matrix then discards
components.
Jacobi is much faster as it iteratively corrects, but is less accurate.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-7)</span></dt><dd><p>Used if algorithm = “jacobi”. Smaller tolerance can increase accuracy,
but but will slow down the algorithm’s convergence.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool</span></dt><dd><p>Whether to print debug spews</p>
</dd>
<dt><strong>whiten</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, de-correlates the components. This is done by dividing them by
the corresponding singular values then multiplying by sqrt(n_samples).
Whitening allows each component to have unit variance and removes
multi-collinearity. It might be beneficial for downstream
tasks like LinearRegression where correlated features cause problems.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>PCA considers linear combinations of features, specifically those that
maximise global variance structure. This means PCA is fantastic for global
structure analyses, but weak for local relationships. Consider UMAP or
T-SNE for a locally important embedding.</p>
<p><strong>Applications of PCA</strong></p>
<blockquote>
<div><p>PCA is used extensively in practice for data visualization and data
compression. It has been used to visualize extremely large word
embeddings like Word2Vec and GloVe in 2 or 3 dimensions, large
datasets of everyday objects and images, and used to distinguish
between cancerous cells from healthy cells.</p>
</div></blockquote>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/pca_demo.ipynb">the PCA notebook</a>.
For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">scikitlearn’s PCA</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">cuml.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">pca_float</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pca_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;components: {pca_float.components_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance: {pca_float.explained_variance_}&#39;</span><span class="p">)</span>
<span class="n">exp_var</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance ratio: {exp_var}&#39;</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;singular values: {pca_float.singular_values_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;mean: {pca_float.mean_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;noise variance: {pca_float.noise_variance_}&#39;</span><span class="p">)</span>

<span class="n">trans_gdf_float</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Inverse: {trans_gdf_float}&#39;</span><span class="p">)</span>

<span class="n">input_gdf_float</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">trans_gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Input: {input_gdf_float}&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">components</span><span class="p">:</span>
            <span class="mi">0</span>           <span class="mi">1</span>           <span class="mi">2</span>
            <span class="mi">0</span>  <span class="mf">0.69225764</span>  <span class="o">-</span><span class="mf">0.5102837</span> <span class="o">-</span><span class="mf">0.51028395</span>
            <span class="mi">1</span> <span class="o">-</span><span class="mf">0.72165036</span> <span class="o">-</span><span class="mf">0.48949987</span>  <span class="o">-</span><span class="mf">0.4895003</span>

<span class="n">explained</span> <span class="n">variance</span><span class="p">:</span>

            <span class="mi">0</span>   <span class="mf">8.510402</span>
            <span class="mi">1</span> <span class="mf">0.48959687</span>

<span class="n">explained</span> <span class="n">variance</span> <span class="n">ratio</span><span class="p">:</span>

             <span class="mi">0</span>   <span class="mf">0.9456003</span>
             <span class="mi">1</span> <span class="mf">0.054399658</span>

<span class="n">singular</span> <span class="n">values</span><span class="p">:</span>

           <span class="mi">0</span> <span class="mf">4.1256275</span>
           <span class="mi">1</span> <span class="mf">0.9895422</span>

<span class="n">mean</span><span class="p">:</span>

          <span class="mi">0</span> <span class="mf">2.6666667</span>
          <span class="mi">1</span> <span class="mf">2.3333333</span>
          <span class="mi">2</span> <span class="mf">2.3333333</span>

<span class="n">noise</span> <span class="n">variance</span><span class="p">:</span>

      <span class="mi">0</span>  <span class="mf">0.0</span>

<span class="n">transformed</span> <span class="n">matrix</span><span class="p">:</span>
             <span class="mi">0</span>           <span class="mi">1</span>
             <span class="mi">0</span>   <span class="o">-</span><span class="mf">2.8547091</span> <span class="o">-</span><span class="mf">0.42891636</span>
             <span class="mi">1</span> <span class="o">-</span><span class="mf">0.121316016</span>  <span class="mf">0.80743366</span>
             <span class="mi">2</span>    <span class="mf">2.9760244</span> <span class="o">-</span><span class="mf">0.37851727</span>

<span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
          <span class="mi">0</span> <span class="mf">1.0000001</span> <span class="mf">3.9999993</span>       <span class="mf">4.0</span>
          <span class="mi">1</span>       <span class="mf">2.0</span> <span class="mf">2.0000002</span> <span class="mf">1.9999999</span>
          <span class="mi">2</span> <span class="mf">4.9999995</span> <span class="mf">1.0000006</span>       <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
<dt><strong>mean_</strong><span class="classifier">array</span></dt><dd><p>The column wise mean of X. Used to mean - center the data first.</p>
</dd>
<dt><strong>noise_variance_</strong><span class="classifier">float</span></dt><dd><p>From Bishop 1999’s Textbook. Used in later tasks like calculating the
estimated covariance of X.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.fit" title="cuml.PCA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, _transform])</p></td>
<td><p>Fit the model with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.fit_transform" title="cuml.PCA.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, y])</p></td>
<td><p>Fit the model with X and apply the dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.get_param_names" title="cuml.PCA.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.inverse_transform" title="cuml.PCA.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Transform data back to its original space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.transform" title="cuml.PCA.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Apply dimensionality reduction to X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.PCA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">_transform=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>cluster labels</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and apply the dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>training data (floats or doubles), where n_samples is the number of
samples, and n_features is the number of features.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>y</strong><span class="classifier">ignored</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform data back to its original space.</p>
<p>In other words, return an input X_original whose transform would be X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>New data (floats or doubles), where n_samples is the number of
samples and n_components is the number of components.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the inverse_transform method will automatically
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_original</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_features)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply dimensionality reduction to X.</p>
<p>X is projected on the first principal components previously extracted
from a training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>New data (floats or doubles), where n_samples is the number of
samples and n_components is the number of components.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will automatically
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="truncated-svd">
<h3>Truncated SVD<a class="headerlink" href="#truncated-svd" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.TruncatedSVD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">TruncatedSVD</code><a class="headerlink" href="#cuml.TruncatedSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>TruncatedSVD is used to compute the top K singular values and vectors of a
large matrix X. It is much faster when n_components is small, such as in
the use of PCA when 3 components is used for 3D visualization.</p>
<p>cuML’s TruncatedSVD an array-like object or cuDF DataFrame, and provides 2
algorithms Full and Jacobi. Full (default) uses a full eigendecomposition
then selects the top K singular vectors. The Jacobi algorithm is much
faster as it iteratively tries to correct the top K singular vectors, but
might be less accurate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algorithm</strong><span class="classifier">‘full’ or ‘jacobi’ or ‘auto’ (default = ‘full’)</span></dt><dd><p>Full uses a eigendecomposition of the covariance matrix then discards
components.
Jacobi is much faster as it iteratively corrects, but is less accurate.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want.
Must be &lt;= number(columns).</p>
</dd>
<dt><strong>n_iter</strong><span class="classifier">int (default = 15)</span></dt><dd><p>Used in Jacobi solver. The more iterations, the more accurate, but
slower.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int / None (default = None)</span></dt><dd><p>If you want results to be the same when you restart Python, select a
state.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-7)</span></dt><dd><p>Used if algorithm = “jacobi”. Smaller tolerance can increase accuracy,
but but will slow down the algorithm’s convergence.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool</span></dt><dd><p>Whether to print debug spews</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>TruncatedSVD (the randomized version [Jacobi]) is fantastic when the number
of components you want is much smaller than the number of features. The
approximation to the largest singular values and vectors is very robust,
however, this method loses a lot of accuracy when you want many many
components.</p>
<p><strong>Applications of TruncatedSVD</strong></p>
<blockquote>
<div><p>TruncatedSVD is also known as Latent Semantic Indexing (LSI) which
tries to find topics of a word count matrix. If X previously was
centered with mean removal, TruncatedSVD is the same as TruncatedPCA.
TruncatedSVD is also used in information retrieval tasks,
recommendation systems and data compression.</p>
</div></blockquote>
<p>For additional examples, see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/tsvd_demo.ipynb">the Truncated SVD  notebook</a>.
For additional documentation, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">scikitlearn’s TruncatedSVD docs</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">cuml.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">tsvd_float</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;jacobi&quot;</span><span class="p">,</span>
                          <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">)</span>
<span class="n">tsvd_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;components: {tsvd_float.components_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance: {tsvd_float.explained_variance_}&#39;</span><span class="p">)</span>
<span class="n">exp_var</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance ratio: {exp_var}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;singular values: {tsvd_float.singular_values_}&#39;</span><span class="p">)</span>

<span class="n">trans_gdf_float</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Transformed matrix: {trans_gdf_float}&#39;</span><span class="p">)</span>

<span class="n">input_gdf_float</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">trans_gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Input matrix: {input_gdf_float}&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">components</span><span class="p">:</span>            <span class="mi">0</span>           <span class="mi">1</span>          <span class="mi">2</span>
<span class="mi">0</span> <span class="mf">0.58725953</span>  <span class="mf">0.57233137</span>  <span class="mf">0.5723314</span>
<span class="mi">1</span> <span class="mf">0.80939883</span> <span class="o">-</span><span class="mf">0.41525528</span> <span class="o">-</span><span class="mf">0.4152552</span>
<span class="n">explained</span> <span class="n">variance</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">55.33908</span>
<span class="mi">1</span> <span class="mf">16.660923</span>

<span class="n">explained</span> <span class="n">variance</span> <span class="n">ratio</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">0.7685983</span>
<span class="mi">1</span> <span class="mf">0.23140171</span>

<span class="n">singular</span> <span class="n">values</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">7.439024</span>
<span class="mi">1</span> <span class="mf">4.0817795</span>

<span class="n">Transformed</span> <span class="n">Matrix</span><span class="p">:</span>
<span class="mi">0</span>           <span class="mi">1</span>         <span class="mi">2</span>
<span class="mi">0</span>   <span class="mf">5.1659107</span>    <span class="o">-</span><span class="mf">2.512643</span>
<span class="mi">1</span>   <span class="mf">3.4638448</span>    <span class="o">-</span><span class="mf">0.042223275</span>
<span class="mi">2</span>    <span class="mf">4.0809603</span>   <span class="mf">3.2164836</span>

<span class="n">Input</span> <span class="n">matrix</span><span class="p">:</span>           <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
<span class="mi">0</span>       <span class="mf">1.0</span>  <span class="mf">4.000001</span>  <span class="mf">4.000001</span>
<span class="mi">1</span> <span class="mf">2.0000005</span> <span class="mf">2.0000005</span> <span class="mf">2.0000007</span>
<span class="mi">2</span>  <span class="mf">5.000001</span> <span class="mf">0.9999999</span> <span class="mf">1.0000004</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.fit" title="cuml.TruncatedSVD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, _transform])</p></td>
<td><p>Fit LSI model on training cudf DataFrame X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.fit_transform" title="cuml.TruncatedSVD.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X)</p></td>
<td><p>Fit LSI model to X and perform dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.get_param_names" title="cuml.TruncatedSVD.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.inverse_transform" title="cuml.TruncatedSVD.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Transform X back to its original space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.transform" title="cuml.TruncatedSVD.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Perform dimensionality reduction on X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.TruncatedSVD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">_transform=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit LSI model on training cudf DataFrame X.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit LSI model to X and perform dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd><p>Reduced version of X as a dense cuDF DataFrame</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X back to its original space.</p>
<p>Returns a cuDF DataFrame X_original whose transform would be X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the inverse_transform method will automatically
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_original</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_features)</span></dt><dd><p>Note that this is always a dense cuDF DataFrame.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will automatically
convert the input to the data type which was used to train the
model.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd><p>Reduced version of X. This will always be a dense DataFrame.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="umap">
<h3>UMAP<a class="headerlink" href="#umap" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.UMAP">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">UMAP</code><a class="headerlink" href="#cuml.UMAP" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniform Manifold Approximation and Projection
Finds a low dimensional embedding of the data that approximates
an underlying manifold.</p>
<p>Adapted from <a class="reference external" href="https://github.com/lmcinnes/umap/blob/master/umap/">https://github.com/lmcinnes/umap/blob/master/umap/</a><a class="reference internal" href="#umap">umap</a>.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors: float (optional, default 15)</strong></dt><dd><p>The size of local neighborhood (in terms of number of neighboring
sample points) used for manifold approximation. Larger values
result in more global views of the manifold, while smaller
values result in more local data being preserved. In general
values should be in the range 2 to 100.</p>
</dd>
<dt><strong>n_components: int (optional, default 2)</strong></dt><dd><p>The dimension of the space to embed into. This defaults to 2 to
provide easy visualization, but can reasonably be set to any</p>
</dd>
<dt><strong>n_epochs: int (optional, default None)</strong></dt><dd><p>The number of training epochs to be used in optimizing the
low dimensional embedding. Larger values result in more accurate
embeddings. If None is specified a value will be selected based on
the size of the input dataset (200 for large datasets, 500 for small).</p>
</dd>
<dt><strong>learning_rate: float (optional, default 1.0)</strong></dt><dd><p>The initial learning rate for the embedding optimization.</p>
</dd>
<dt><strong>init: string (optional, default ‘spectral’)</strong></dt><dd><dl class="simple">
<dt>How to initialize the low dimensional embedding. Options are:</dt><dd><ul class="simple">
<li><p>‘spectral’: use a spectral embedding of the fuzzy 1-skeleton</p></li>
<li><p>‘random’: assign initial embedding positions at random.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt><strong>min_dist: float (optional, default 0.1)</strong></dt><dd><p>The effective minimum distance between embedded points. Smaller values
will result in a more clustered/clumped embedding where nearby points
on the manifold are drawn closer together, while larger values will
result on a more even dispersal of points. The value should be set
relative to the <code class="docutils literal notranslate"><span class="pre">spread</span></code> value, which determines the scale at which
embedded points will be spread out.</p>
</dd>
<dt><strong>spread: float (optional, default 1.0)</strong></dt><dd><p>The effective scale of embedded points. In combination with
<code class="docutils literal notranslate"><span class="pre">min_dist</span></code> this determines how clustered/clumped the embedded
points are.</p>
</dd>
<dt><strong>set_op_mix_ratio: float (optional, default 1.0)</strong></dt><dd><p>Interpolate between (fuzzy) union and intersection as the set operation
used to combine local fuzzy simplicial sets to obtain a global fuzzy
simplicial sets. Both fuzzy set operations use the product t-norm.
The value of this parameter should be between 0.0 and 1.0; a value of
1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
intersection.</p>
</dd>
<dt><strong>local_connectivity: int (optional, default 1)</strong></dt><dd><p>The local connectivity required – i.e. the number of nearest
neighbors that should be assumed to be connected at a local level.
The higher this value the more connected the manifold becomes
locally. In practice this should be not more than the local intrinsic
dimension of the manifold.</p>
</dd>
<dt><strong>repulsion_strength: float (optional, default 1.0)</strong></dt><dd><p>Weighting applied to negative samples in low dimensional embedding
optimization. Values higher than one will result in greater weight
being given to negative samples.</p>
</dd>
<dt><strong>negative_sample_rate: int (optional, default 5)</strong></dt><dd><p>The number of negative samples to select per positive sample
in the optimization process. Increasing this value will result
in greater repulsive force being applied, greater optimization
cost, but slightly more accuracy.</p>
</dd>
<dt><strong>transform_queue_size: float (optional, default 4.0)</strong></dt><dd><p>For transform operations (embedding new points using a trained <a href="#id23"><span class="problematic" id="id24">model_</span></a>
this will control how aggressively to search for nearest neighbors.
Larger values will result in slower performance but more accurate
nearest neighbor evaluation.</p>
</dd>
<dt><strong>a: float (optional, default None)</strong></dt><dd><p>More specific parameters controlling the embedding. If None these
values are set automatically as determined by <code class="docutils literal notranslate"><span class="pre">min_dist</span></code> and
<code class="docutils literal notranslate"><span class="pre">spread</span></code>.</p>
</dd>
<dt><strong>b: float (optional, default None)</strong></dt><dd><p>More specific parameters controlling the embedding. If None these
values are set automatically as determined by <code class="docutils literal notranslate"><span class="pre">min_dist</span></code> and
<code class="docutils literal notranslate"><span class="pre">spread</span></code>.</p>
</dd>
<dt><strong>verbose: bool (optional, default False)</strong></dt><dd><p>Controls verbosity of logging.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This module is heavily based on Leland McInnes’ reference UMAP package.
However, there are a number of differences and features that are not yet
implemented in cuml.umap:</p>
<blockquote>
<div><ul class="simple">
<li><p>Specifying the random seed</p></li>
<li><p>Using a non-euclidean distance metric (support for a fixed set
of non-euclidean metrics is planned for an upcoming release).</p></li>
<li><p>Using a pre-computed pairwise distance matrix (under consideration
for future releases)</p></li>
<li><p>Manual initialization of initial embedding positions</p></li>
</ul>
</div></blockquote>
<p>In addition to these missing features, you should expect to see
the final embeddings differing between cuml.umap and the reference
UMAP. In particular, the reference UMAP uses an approximate kNN
algorithm for large data sizes while cuml.umap always uses exact
kNN.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Leland McInnes, John Healy, James Melville
UMAP: Uniform Manifold Approximation and Projection for Dimension
Reduction
<a class="reference external" href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</a></p></li>
</ul>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.fit" title="cuml.UMAP.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, y, convert_dtype])</p></td>
<td><p>Fit X into an embedded space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.UMAP.fit_transform" title="cuml.UMAP.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, y, convert_dtype])</p></td>
<td><p>Fit X into an embedded space and return that transformed output.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.transform" title="cuml.UMAP.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Transform X into the existing embedded space and return that transformed output.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.UMAP.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y=None</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit X into an embedded space.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>X contains a sample per row.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>y contains a label per row.
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.UMAP.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y=None</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit X into an embedded space and return that transformed
output.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>X contains a sample per row.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>X_new<span class="classifier">array, shape (n_samples, n_components)</span></dt><dd><p>Embedding of the training data in low-dimensional space.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.UMAP.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X into the existing embedded space and return that
transformed output.</p>
<p>Please refer to the reference UMAP implementation for information
on the differences between fit_transform() and running fit()
transform().</p>
<p>Specifically, the transform() function is stochastic:
<a class="reference external" href="https://github.com/lmcinnes/umap/issues/158">https://github.com/lmcinnes/umap/issues/158</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>New data to be transformed.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>——-</strong></dt><dd></dd>
<dt><strong>X_new</strong><span class="classifier">array, shape (n_samples, n_components)</span></dt><dd><p>Embedding of the new data in low-dimensional space.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="random-projections">
<h3>Random Projections<a class="headerlink" href="#random-projections" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.random_projection.GaussianRandomProjection">
<em class="property">class </em><code class="sig-prename descclassname">cuml.random_projection.</code><code class="sig-name descname">GaussianRandomProjection</code><a class="headerlink" href="#cuml.random_projection.GaussianRandomProjection" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian Random Projection method derivated from BaseRandomProjection
class.</p>
<p>Random projection is a dimensionality reduction technique. Random
projection methods are powerful methods known for their simplicity,
computational efficiency and restricted model size.
This algorithm also has the advantage to preserve distances well between
any two samples and is thus suitable for methods having this requirement.</p>
<p>The components of the random matrix are drawn from N(0, 1 / n_components).</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = ‘auto’)</span></dt><dd><p>Dimensionality of the target projection space. If set to ‘auto’,
the parameter is deducted thanks to Johnson–Lindenstrauss lemma.
The automatic deduction make use of the number of samples and
the eps parameter.</p>
<p>The Johnson–Lindenstrauss lemma can produce very conservative
n_components parameter as it makes no assumption on dataset structure.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float (default = 0.1)</span></dt><dd><p>Error tolerance during projection. Used by Johnson–Lindenstrauss
automatic deduction when n_components is set to ‘auto’.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed used to initilize random generator</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Inspired from sklearn’s implementation :
<a class="reference external" href="https://scikit-learn.org/stable/modules/random_projection.html">https://scikit-learn.org/stable/modules/random_projection.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>gaussian_method</strong><span class="classifier">boolean</span></dt><dd><p>To be passed to base class in order to determine
random matrix generation method</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="cuml.random_projection.SparseRandomProjection">
<em class="property">class </em><code class="sig-prename descclassname">cuml.random_projection.</code><code class="sig-name descname">SparseRandomProjection</code><a class="headerlink" href="#cuml.random_projection.SparseRandomProjection" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse Random Projection method derivated from BaseRandomProjection class.</p>
<p>Random projection is a dimensionality reduction technique. Random
projection methods are powerful methods known for their simplicity,
computational efficiency and restricted model size.
This algorithm also has the advantage to preserve distances well between
any two samples and is thus suitable for methods having this requirement.</p>
<p>Sparse random matrix is an alternative to dense random projection matrix
(e.g. Gaussian) that guarantees similar embedding quality while being much
more memory efficient and allowing faster computation of the projected data
(with sparse enough matrices).
If we note ‘s = 1 / density’ the components of the random matrix are
drawn from:</p>
<blockquote>
<div><ul class="simple">
<li><p>-sqrt(s) / sqrt(n_components)   with probability 1 / 2s</p></li>
<li><p>0                              with probability 1 - 1 / s</p></li>
<li><p>+sqrt(s) / sqrt(n_components)   with probability 1 / 2s</p></li>
</ul>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = ‘auto’)</span></dt><dd><p>Dimensionality of the target projection space. If set to ‘auto’,
the parameter is deducted thanks to Johnson–Lindenstrauss lemma.
The automatic deduction make use of the number of samples and
the eps parameter.</p>
<p>The Johnson–Lindenstrauss lemma can produce very conservative
n_components parameter as it makes no assumption on dataset structure.</p>
</dd>
<dt><strong>density</strong><span class="classifier">float in range (0, 1] (default = ‘auto’)</span></dt><dd><p>Ratio of non-zero component in the random projection matrix.</p>
<p>If density = ‘auto’, the value is set to the minimum density
as recommended by Ping Li et al.: 1 / sqrt(n_features).</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float (default = 0.1)</span></dt><dd><p>Error tolerance during projection. Used by Johnson–Lindenstrauss
automatic deduction when n_components is set to ‘auto’.</p>
</dd>
<dt><strong>dense_output</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If set to True transformed matrix will be dense otherwise sparse.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed used to initilize random generator</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Inspired from sklearn’s implementation :
<a class="reference external" href="https://scikit-learn.org/stable/modules/random_projection.html">https://scikit-learn.org/stable/modules/random_projection.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>gaussian_method</strong><span class="classifier">boolean</span></dt><dd><p>To be passed to base class in order to determine
random matrix generation method</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tsne">
<h3>TSNE<a class="headerlink" href="#tsne" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.TSNE">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">TSNE</code><a class="headerlink" href="#cuml.TSNE" title="Permalink to this definition">¶</a></dt>
<dd><p>TSNE (T-Distributed Stochastic Neighbor Embedding) is an extremely
powerful dimensionality reduction technique that aims to maintain
local distances between data points. It is extremely robust to whatever
dataset you give it, and is used in many areas including cancer research,
music analysis and neural network weight visualizations.</p>
<p>The current cuML TSNE implementation is a first experimental release. It
defaults to use the ‘exact’ fitting algorithm, which is signficantly slower
then the Barnes-Hut algorithm as data sizes grow. A preview implementation
of Barnes-Hut (derived from CannyLabs’ BH open source CUDA code) is also
available for problems with n_components = 2, though this implementation
currently has outstanding issues that can lead to crashes in rare
scenarios. Future releases of TSNE will fix these issues (tracked as cuML
Issue #1002) and switch Barnes-Hut to be the default.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_components</strong><span class="classifier">int (default 2)</span></dt><dd><p>The output dimensionality size. Currently only size=2 is tested, but
the ‘exact’ algorithm will support greater dimensionality in future.</p>
</dd>
<dt><strong>perplexity</strong><span class="classifier">float (default 30.0)</span></dt><dd><p>Larger datasets require a larger value. Consider choosing different
perplexity values from 5 to 50 and see the output differences.</p>
</dd>
<dt><strong>early_exaggeration</strong><span class="classifier">float (default 12.0)</span></dt><dd><p>Controls the space between clusters. Not critical to tune this.</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">float (default 200.0)</span></dt><dd><p>The learning rate usually between (10, 1000). If this is too high,
TSNE could look like a cloud / ball of points.</p>
</dd>
<dt><strong>n_iter</strong><span class="classifier">int (default 1000)</span></dt><dd><p>The more epochs, the more stable/accruate the final embedding.</p>
</dd>
<dt><strong>n_iter_without_progress</strong><span class="classifier">int (default 300)</span></dt><dd><p>When the KL Divergence becomes too small after some iterations,
terminate TSNE early.</p>
</dd>
<dt><strong>min_grad_norm</strong><span class="classifier">float (default 1e-07)</span></dt><dd><p>The minimum gradient norm for when TSNE will terminate early.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">str ‘euclidean’ only (default ‘euclidean’)</span></dt><dd><p>Currently only supports euclidean distance. Will support cosine in
a future release.</p>
</dd>
<dt><strong>init</strong><span class="classifier">str ‘random’ only (default ‘random’)</span></dt><dd><p>Currently only supports random intialization. Will support PCA
intialization in a future release.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int (default 0)</span></dt><dd><p>Level of verbosity. If &gt; 0, prints all help messages and warnings.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default None)</span></dt><dd><p>Setting this can allow future runs of TSNE to look the same.</p>
</dd>
<dt><strong>method</strong><span class="classifier">str ‘barnes_hut’ or ‘exact’ (default ‘barnes_hut’)</span></dt><dd><p>Options are either barnes_hut or exact. It is recommend that you use
the barnes hut approximation for superior O(nlogn) complexity.</p>
</dd>
<dt><strong>angle</strong><span class="classifier">float (default 0.5)</span></dt><dd><p>Tradeoff between accuracy and speed. Choose between (0,2 0.8) where
closer to one indicates full accuracy but slower speeds.</p>
</dd>
<dt><strong>learning_rate_method</strong><span class="classifier">str ‘adaptive’ or ‘none’ (default ‘adaptive’)</span></dt><dd><p>Either adaptive or None. Uses a special adpative method that tunes
the learning rate, early exaggeration and perplexity automatically
based on input size.</p>
</dd>
<dt><strong>n_neighbors</strong><span class="classifier">int (default 90)</span></dt><dd><p>The number of datapoints you want to use in the
attractive forces. Smaller values are better for preserving
local structure, whilst larger values can improve global structure
preservation. Default is 3 * 30 (perplexity)</p>
</dd>
<dt><strong>perplexity_max_iter</strong><span class="classifier">int (default 100)</span></dt><dd><p>The number of epochs the best guassian bands are found for.</p>
</dd>
<dt><strong>exaggeration_iter</strong><span class="classifier">int (default 250)</span></dt><dd><p>To promote the growth of clusters, set this higher.</p>
</dd>
<dt><strong>pre_momentum</strong><span class="classifier">float (default 0.5)</span></dt><dd><p>During the exaggeration iteration, more forcefully apply gradients.</p>
</dd>
<dt><strong>post_momentum</strong><span class="classifier">float (default 0.8)</span></dt><dd><p>During the late phases, less forcefully apply gradients.</p>
</dd>
<dt><strong>should_downcast</strong><span class="classifier">bool (default True)</span></dt><dd><p>Whether to reduce to dataset to float32 or not.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">(cuML Handle, default None)</span></dt><dd><p>You can pass in a past handle that was intialized, or we will create
one for you anew!</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>van der Maaten, L.J.P.
t-Distributed Stochastic Neighbor Embedding
<a class="reference external" href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a></p></li>
<li><p>van der Maaten, L.J.P.; Hinton, G.E.
Visualizing High-Dimensional Data
Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</p></li>
<li><p>George C. Linderman, Manas Rachh, Jeremy G. Hoskins,
Stefan Steinerberger, Yuval Kluger Efficient Algorithms for
t-distributed Stochastic Neighborhood Embedding</p></li>
</ul>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TSNE.fit" title="cuml.TSNE.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Fit X into an embedded space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TSNE.fit_transform" title="cuml.TSNE.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X)</p></td>
<td><p>Fit X into an embedded space and return that transformed output.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.TSNE.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TSNE.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit X into an embedded space.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>X contains a sample per row.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>y contains a label per row.
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TSNE.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TSNE.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit X into an embedded space and return that transformed output.
Parameters
———-
X : array-like (device or host) shape = (n_samples, n_features)</p>
<blockquote>
<div><p>X contains a sample per row.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>X_new<span class="classifier">array, shape (n_samples, n_components)</span></dt><dd><p>Embedding of the training data in low-dimensional space.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="neighbors">
<h2>Neighbors<a class="headerlink" href="#neighbors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nearest-neighbors">
<h3>Nearest Neighbors<a class="headerlink" href="#nearest-neighbors" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.NearestNeighbors">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">NearestNeighbors</code><a class="headerlink" href="#cuml.NearestNeighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>NearestNeighbors is an unsupervised algorithm for querying neighborhoods
from a given set of datapoints. Currently, cuML supports k-NN queries,
which define the neighborhood as the closest <cite>k</cite> neighbors to each query
point.
Note: Should_downcast is deprecated and will be removed in 0.10,</p>
<blockquote>
<div><p>please use the convert_dtypes variable instead.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">np_float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
  <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="c1"># Point 1</span>
  <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="c1"># Point 2</span>
  <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>  <span class="c1"># Point 3</span>
<span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;dim_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">np_float</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;dim_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">np_float</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;dim_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">np_float</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;n_samples = 3, n_dims = 3&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="n">nn_float</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">()</span>
<span class="n">nn_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="c1"># get 3 nearest neighbors</span>
<span class="n">distances</span><span class="p">,</span><span class="n">indices</span> <span class="o">=</span> <span class="n">nn_float</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="c1"># from cuml.neighbors import NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_dims</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">dim_0</span> <span class="n">dim_1</span> <span class="n">dim_2</span>

<span class="mi">0</span>   <span class="mf">1.0</span>   <span class="mf">2.0</span>   <span class="mf">3.0</span>
<span class="mi">1</span>   <span class="mf">1.0</span>   <span class="mf">2.0</span>   <span class="mf">4.0</span>
<span class="mi">2</span>   <span class="mf">2.0</span>   <span class="mf">2.0</span>   <span class="mf">4.0</span>

<span class="c1"># indices:</span>

         <span class="n">index_neighbor_0</span> <span class="n">index_neighbor_1</span> <span class="n">index_neighbor_2</span>
<span class="mi">0</span>                <span class="mi">0</span>                <span class="mi">1</span>                <span class="mi">2</span>
<span class="mi">1</span>                <span class="mi">1</span>                <span class="mi">0</span>                <span class="mi">2</span>
<span class="mi">2</span>                <span class="mi">2</span>                <span class="mi">1</span>                <span class="mi">0</span>
<span class="c1"># distances:</span>

         <span class="n">distance_neighbor_0</span> <span class="n">distance_neighbor_1</span> <span class="n">distance_neighbor_2</span>
<span class="mi">0</span>                 <span class="mf">0.0</span>                 <span class="mf">1.0</span>                 <span class="mf">2.0</span>
<span class="mi">1</span>                 <span class="mf">0.0</span>                 <span class="mf">1.0</span>                 <span class="mf">1.0</span>
<span class="mi">2</span>                 <span class="mf">0.0</span>                 <span class="mf">1.0</span>                 <span class="mf">2.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors: int (default = 5)</strong></dt><dd><p>The top K closest datapoints you want the algorithm to return.
Currently, this value must be &lt; 1024.</p>
</dd>
<dt><strong>should_downcast</strong><span class="classifier">bool (default = None)</span></dt><dd><p>Currently only single precision is supported in the underlying undex.
Setting this to true will allow single-precision input arrays to be
automatically downcasted to single precision.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/notebook/blob/master/python/notebooks/knn_demo.ipynb">the NearestNeighbors notebook</a>.</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors">scikitlearn’s NearestNeighbors</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.NearestNeighbors.fit" title="cuml.NearestNeighbors.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Fit GPU index for performing nearest neighbor queries.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.NearestNeighbors.kneighbors" title="cuml.NearestNeighbors.kneighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kneighbors</span></code></a>(self, X[, k, convert_dtype])</p></td>
<td><p>Query the GPU index for the k nearest neighbors of column vectors in X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.NearestNeighbors.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">convert_dtype=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.NearestNeighbors.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit GPU index for performing nearest neighbor queries.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the fit method will automatically
convert the inputs to np.float32.
Note: Convert dtype will be set to False once should_downcast is</p>
<blockquote>
<div><p>deprecated in 0.10</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.NearestNeighbors.kneighbors">
<code class="sig-name descname">kneighbors</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">k=None</em>, <em class="sig-param">convert_dtype=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.NearestNeighbors.kneighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Query the GPU index for the k nearest neighbors of column vectors in X.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>k: Integer</strong></dt><dd><p>Number of neighbors to search</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the kneighbors method will automatically
convert the inputs to np.float32.
Note: Convert dtype will be set to False once should_downcast is</p>
<blockquote>
<div><p>deprecated in 0.10</p>
</div></blockquote>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>———-</strong></dt><dd></dd>
<dt><strong>distances: cuDF DataFrame or numpy ndarray</strong></dt><dd><p>The distances of the k-nearest neighbors for each column vector
in X</p>
</dd>
<dt><strong>indices: cuDF DataFrame of numpy ndarray</strong></dt><dd><p>The indices of the k-nearest neighbors for each column vector in X</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="time-series">
<h2>Time Series<a class="headerlink" href="#time-series" title="Permalink to this headline">¶</a></h2>
<div class="section" id="holtwinters">
<h3>HoltWinters<a class="headerlink" href="#holtwinters" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.ExponentialSmoothing">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">ExponentialSmoothing</code><a class="headerlink" href="#cuml.ExponentialSmoothing" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Implements a HoltWinters time series analysis model which is used in
both forecasting future entries in a time series as well as in providing
exponential smoothing, where weights are assigned against historical
data with exponentially decreasing impact. This is done by analyzing
three components of the data: level, trend, and seasonality.</p>
<p><strong>Known Limitations</strong>:
This version of ExponentialSmoothing currently provides only a limited
number of features when compared to the
statsmodels.holtwinters.ExponentialSmoothing model. Noticeably, it lacks:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>.predict()<span class="classifier">no support for in-sample prediction.</span></dt><dd><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/875">https://github.com/rapidsai/cuml/issues/875</a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>.hessian()<span class="classifier">no support for returning Hessian matrix.</span></dt><dd><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/880">https://github.com/rapidsai/cuml/issues/880</a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>.information()<span class="classifier">no support for returning Fisher matrix.</span></dt><dd><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/880">https://github.com/rapidsai/cuml/issues/880</a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>.loglike()<span class="classifier">no support for returning Log-likelihood.</span></dt><dd><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/880">https://github.com/rapidsai/cuml/issues/880</a></p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>Additionally, be warned that there may exist floating point instability
issues in this model. Small values in endog may lead to faulty results.
See <a class="reference external" href="https://github.com/rapidsai/cuml/issues/888">https://github.com/rapidsai/cuml/issues/888</a> for more information.</p>
<p><strong>Known Differences</strong>:
This version of ExponentialSmoothing differs from statsmodels in some
other minor ways:</p>
<blockquote>
<div><ul class="simple">
<li><p>.__init__() : Cannot pass trend component or damped trend component</p></li>
<li><dl class="simple">
<dt>.__init__()<span class="classifier">this version can take additional parameter eps,</span></dt><dd><p>start_periods, ts_num, and handle</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>.score()<span class="classifier">returns SSE rather than gradient logL</span></dt><dd><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/876">https://github.com/rapidsai/cuml/issues/876</a></p>
</dd>
</dl>
</li>
<li><p>this version provides get_level(), get_trend(), get_season()</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>endog</strong><span class="classifier">array-like (device or host)</span></dt><dd><blockquote>
<div><p>Acceptable formats: cuDF DataFrame, cuDF Series,
NumPy ndarray, Numba device ndarray, cuda array interface
compliant array like CuPy.
Note: cuDF.DataFrame types assumes data is in columns,
while all other datatypes assume data is in rows.
The endogenous dataset to be operated on.</p>
</div></blockquote>
<dl class="simple">
<dt>seasonal<span class="classifier">‘additive’, ‘add’, ‘multiplicative’, ‘mul’</span></dt><dd><p>(default = ‘additive’)
whether the seasonal trend should be calculated
additively or multiplicatively.</p>
</dd>
<dt>seasonal_periods<span class="classifier">int (default=2)</span></dt><dd><p>the seasonality of the data (how often it
repeats). For monthly data this should be 12,
for weekly data, this should be 7.</p>
</dd>
<dt>start_periods<span class="classifier">int (default=2)</span></dt><dd><p>number of seasons to be used for seasonal
seed values</p>
</dd>
<dt>ts_num<span class="classifier">int (default=1)</span></dt><dd><p>the number of different time series that were passed
in the endog param.</p>
</dd>
<dt>eps<span class="classifier">np.number &gt; 0 (default=2.24e-3)</span></dt><dd><p>the accuracy to which gradient descent should achieve.
Note that changing this value may affect the forecasted results.</p>
</dd>
<dt>handle<span class="classifier">cuml.Handle (default=None)</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>“, cu_pred)</p>
<blockquote>
<div><p>Output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Forecasted points :
0    4.000143766093652
1    5.000000163513641
2    6.000000000174092
3    7.000000000000178
</pre></div>
</div>
</div></blockquote>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.fit" title="cuml.ExponentialSmoothing.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self)</p></td>
<td><p>Performing fitting on the given <cite>endog</cite> dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.forecast" title="cuml.ExponentialSmoothing.forecast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forecast</span></code></a>(self[, h, index])</p></td>
<td><p>Forecasts future points based on the fitted model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_level" title="cuml.ExponentialSmoothing.get_level"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_level</span></code></a>(self[, index])</p></td>
<td><p>Returns the level component of the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_season" title="cuml.ExponentialSmoothing.get_season"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_season</span></code></a>(self[, index])</p></td>
<td><p>Returns the season component of the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_trend" title="cuml.ExponentialSmoothing.get_trend"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_trend</span></code></a>(self[, index])</p></td>
<td><p>Returns the trend component of the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.score" title="cuml.ExponentialSmoothing.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self[, index])</p></td>
<td><p>Returns the score of the model.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.ExponentialSmoothing.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performing fitting on the given <cite>endog</cite> dataset.
Calculates the level, trend, season, and SSE components.</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ExponentialSmoothing.forecast">
<code class="sig-name descname">forecast</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">h=1</em>, <em class="sig-param">index=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing.forecast" title="Permalink to this definition">¶</a></dt>
<dd><p>Forecasts future points based on the fitted model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h</strong><span class="classifier">int (default=1)</span></dt><dd><p>the number of points for each series to be forecasted</p>
</dd>
<dt><strong>index</strong><span class="classifier">int (default=None)</span></dt><dd><p>the index of the time series from which you want
forecasted points. if None, then a cudf.DataFrame of
the forecasted points from all time series is returned.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>preds</strong><span class="classifier">cudf.DataFrame or cudf.Series</span></dt><dd><p>Series of forecasted points if index is provided.
DataFrame of all forecasted points if index=None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ExponentialSmoothing.get_level">
<code class="sig-name descname">get_level</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">index=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing.get_level" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the level component of the model.</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ExponentialSmoothing.get_season">
<code class="sig-name descname">get_season</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">index=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing.get_season" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the season component of the model.</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ExponentialSmoothing.get_trend">
<code class="sig-name descname">get_trend</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">index=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing.get_trend" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the trend component of the model.</p>
</dd></dl>

<dl class="attribute">
<dt id="cuml.ExponentialSmoothing.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">index=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the score of the model.</p>
<dl class="simple">
<dt><a href="#id3"><span class="problematic" id="id4">**</span></a>Note: Currently returns the SSE, rather than the gradient of the</dt><dd><p>LogLikelihood. <a class="reference external" href="https://github.com/rapidsai/cuml/issues/876">https://github.com/rapidsai/cuml/issues/876</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="kalman-filter">
<h3>Kalman Filter<a class="headerlink" href="#kalman-filter" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.KalmanFilter">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">KalmanFilter</code><a class="headerlink" href="#cuml.KalmanFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Kalman filter. You are responsible for setting the
various state variables to reasonable values; defaults  will
not give you a functional filter.
After construction the filter will have default matrices created for you,
but you must specify the values for each.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dim_x</strong><span class="classifier">int</span></dt><dd><p>Number of state variables for the Kalman filter.
This is used to set the default size of P, Q, and u</p>
</dd>
<dt><strong>dim_z</strong><span class="classifier">int</span></dt><dd><p>Number of of measurement inputs.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml</span> <span class="k">import</span> <span class="n">KalmanFilter</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">KalmanFilter</span><span class="p">(</span><span class="n">dim_x</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim_z</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">],</span>    <span class="c1"># position</span>
                <span class="p">[</span><span class="mf">0.</span><span class="p">]])</span>   <span class="c1"># velocity</span>
<span class="n">f</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]])</span>
<span class="n">f</span><span class="o">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">]])</span>
<span class="n">f</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1000.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span>   <span class="mf">0.</span><span class="p">,</span> <span class="mf">1000.</span><span class="p">]</span> <span class="p">])</span>
<span class="n">f</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Now just perform the standard predict/update loop:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">while</span> <span class="n">some_condition_is_true</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">numba</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>
    <span class="n">f</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
    <span class="n">f</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">numba device array, numpy array or cuDF series (dim_x, 1),</span></dt><dd><p>Current state estimate. Any call to update() or predict() updates
this variable.</p>
</dd>
<dt><strong>P</strong><span class="classifier">numba device array, numpy array or cuDF dataframe(dim_x, dim_x)</span></dt><dd><p>Current state covariance matrix. Any call to update() or predict()
updates this variable.</p>
</dd>
<dt><strong>x_prior</strong><span class="classifier">numba device array, numpy array or cuDF series(dim_x, 1)</span></dt><dd><p>Prior (predicted) state estimate. The <a href="#id5"><span class="problematic" id="id6">*</span></a>_prior and <a href="#id7"><span class="problematic" id="id8">*</span></a>_post attributes
are for convienence; they store the  prior and posterior of the
current epoch. Read Only.</p>
</dd>
<dt><strong>P_prior</strong><span class="classifier">numba device array, numpy array or cuDF dataframe(dim_x, dim_x)</span></dt><dd><p>Prior (predicted) state covariance matrix. Read Only.</p>
</dd>
<dt><strong>x_post</strong><span class="classifier">numba device array, numpy array or cuDF series(dim_x, 1)</span></dt><dd><p>Posterior (updated) state estimate. Read Only.</p>
</dd>
<dt><strong>P_post</strong><span class="classifier">numba device array, numpy array or cuDF dataframe(dim_x, dim_x)</span></dt><dd><p>Posterior (updated) state covariance matrix. Read Only.</p>
</dd>
<dt><strong>z</strong><span class="classifier">numba device array or cuDF series (dim_x, 1)</span></dt><dd><p>Last measurement used in update(). Read only.</p>
</dd>
<dt><strong>R</strong><span class="classifier">numba device array(dim_z, dim_z)</span></dt><dd><p>Measurement noise matrix</p>
</dd>
<dt><strong>Q</strong><span class="classifier">numba device array(dim_x, dim_x)</span></dt><dd><p>Process noise matrix</p>
</dd>
<dt><strong>F</strong><span class="classifier">numba device array()</span></dt><dd><p>State Transition matrix</p>
</dd>
<dt><strong>H</strong><span class="classifier">numba device array(dim_z, dim_x)</span></dt><dd><p>Measurement function</p>
</dd>
<dt><strong>y</strong><span class="classifier">numba device array</span></dt><dd><p>Residual of the update step. Read only.</p>
</dd>
<dt><strong>K</strong><span class="classifier">numba device array(dim_x, dim_z)</span></dt><dd><p>Kalman gain of the update step. Read only.</p>
</dd>
<dt><strong>precision: ‘single’ or ‘double’</strong></dt><dd><p>Whether the Kalman Filter uses single or double precision</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KalmanFilter.predict" title="cuml.KalmanFilter.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self[, B, F, Q])</p></td>
<td><p>Predict next state (prior) using the Kalman filter state propagation equations.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KalmanFilter.update" title="cuml.KalmanFilter.update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update</span></code></a>(self, z[, R, H])</p></td>
<td><p>Add a new measurement (z) to the Kalman filter.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.KalmanFilter.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">B=None</em>, <em class="sig-param">F=None</em>, <em class="sig-param">Q=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KalmanFilter.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict next state (prior) using the Kalman filter state propagation
equations.
Parameters
———-
u : np.array</p>
<blockquote>
<div><p>Optional control vector. If not <cite>None</cite>, it is multiplied by B
to create the control input into the system.</p>
</div></blockquote>
<dl class="simple">
<dt>B<span class="classifier">np.array(dim_x, dim_z), or None</span></dt><dd><p>Optional control transition matrix; a value of None
will cause the filter to use <cite>self.B</cite>.</p>
</dd>
<dt>F<span class="classifier">np.array(dim_x, dim_x), or None</span></dt><dd><p>Optional state transition matrix; a value of None
will cause the filter to use <cite>self.F</cite>.</p>
</dd>
<dt>Q<span class="classifier">np.array(dim_x, dim_x), scalar, or None</span></dt><dd><p>Optional process noise matrix; a value of None will cause the
filter to use <cite>self.Q</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KalmanFilter.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">z</em>, <em class="sig-param">R=None</em>, <em class="sig-param">H=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KalmanFilter.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a new measurement (z) to the Kalman filter.
If z is None, nothing is computed. However, x_post and P_post are
updated with the prior (x_prior, P_prior), and self.z is set to None.
Parameters
———-
z : (dim_z, 1): array_like</p>
<blockquote>
<div><p>measurement for this update. z can be a scalar if dim_z is 1,
otherwise it must be convertible to a column vector.</p>
</div></blockquote>
<dl class="simple">
<dt>R<span class="classifier">np.array, scalar, or None</span></dt><dd><p>Optionally provide R to override the measurement noise for this
one call, otherwise  self.R will be used.</p>
</dd>
<dt>H<span class="classifier">np.array, or None</span></dt><dd><p>Optionally provide H to override the measurement function for this
one call, otherwise self.H will be used.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="multi-node-multi-gpu-algorithms">
<h2>Multi-Node, Multi-GPU Algorithms<a class="headerlink" href="#multi-node-multi-gpu-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id9">
<h3>K-Means Clustering<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.dask.cluster.KMeans">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.cluster.</code><code class="sig-name descname">KMeans</code><span class="sig-paren">(</span><em class="sig-param">n_clusters=8</em>, <em class="sig-param">max_iter=300</em>, <em class="sig-param">tol=0.0001</em>, <em class="sig-param">verbose=0</em>, <em class="sig-param">random_state=1</em>, <em class="sig-param">precompute_distances='auto'</em>, <em class="sig-param">init='scalable-k-means++'</em>, <em class="sig-param">n_init=1</em>, <em class="sig-param">algorithm='auto'</em>, <em class="sig-param">client=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-Node Multi-GPU implementation of KMeans.</p>
<p>This version minimizes data transfer by sharing only
the centroids between workers in each iteration.</p>
<p>Predictions are done embarrassingly parallel, using cuML’s
single-GPU version.</p>
<p>For more information on this implementation, refer to the
documentation for single-GPU K-Means.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.fit" title="cuml.dask.cluster.KMeans.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Fits a distributed KMeans model :param X: dask_cudf.Dataframe to fit :return: This KMeans instance</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.fit_transform" title="cuml.dask.cluster.KMeans.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X)</p></td>
<td><p>Calls fit followed by transform using a distributed KMeans model :param X: dask_cudf.Dataframe to fit &amp; predict :return: A dask_cudf.Dataframe containing label predictions</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.func_fit" title="cuml.dask.cluster.KMeans.func_fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">func_fit</span></code></a>(sessionId, n_clusters, max_iter, …)</p></td>
<td><p>Runs on each worker to call fit on local KMeans instance.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.func_predict" title="cuml.dask.cluster.KMeans.func_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">func_predict</span></code></a>(model, dfs, r)</p></td>
<td><p>Runs on each worker to call fit on local KMeans instance :param model: Local KMeans instance :param dfs: List of cudf.Dataframes to use :param r: Stops memoization caching :return: cudf.Series with predictions</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.func_score" title="cuml.dask.cluster.KMeans.func_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">func_score</span></code></a>(model, dfs, r)</p></td>
<td><p>Runs on each worker to call fit on local KMeans instance :param model: Local KMeans instance :param dfs: List of cudf.Dataframes to use :param r: Stops memoization caching :return: cudf.Series with predictions</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.func_transform" title="cuml.dask.cluster.KMeans.func_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">func_transform</span></code></a>(model, dfs, r)</p></td>
<td><p>Runs on each worker to call fit on local KMeans instance :param model: Local KMeans instance :param dfs: List of cudf.Dataframes to use :param r: Stops memoizatiion caching :return: The fit model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.parallel_func" title="cuml.dask.cluster.KMeans.parallel_func"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parallel_func</span></code></a>(self, X, func)</p></td>
<td><p>Predicts the labels using a distributed KMeans model :param X: dask_cudf.Dataframe to predict :return: A dask_cudf.Dataframe containing label predictions</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.predict" title="cuml.dask.cluster.KMeans.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the labels using a distributed KMeans model :param X: dask_cudf.Dataframe to predict :return: A dask_cudf.Dataframe containing label predictions</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.transform" title="cuml.dask.cluster.KMeans.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X)</p></td>
<td><p>Predicts the labels using a distributed KMeans model :param X: dask_cudf.Dataframe to predict :return: A dask_cudf.Dataframe containing label predictions</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 60%" />
<col style="width: 40%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>fit_predict</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>score</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cuml.dask.cluster.KMeans.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a distributed KMeans model
:param X: dask_cudf.Dataframe to fit
:return: This KMeans instance</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls fit followed by transform using a distributed KMeans model
:param X: dask_cudf.Dataframe to fit &amp; predict
:return: A dask_cudf.Dataframe containing label predictions</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.func_fit">
<em class="property">static </em><code class="sig-name descname">func_fit</code><span class="sig-paren">(</span><em class="sig-param">sessionId</em>, <em class="sig-param">n_clusters</em>, <em class="sig-param">max_iter</em>, <em class="sig-param">tol</em>, <em class="sig-param">verbose</em>, <em class="sig-param">random_state</em>, <em class="sig-param">precompute_distances</em>, <em class="sig-param">init</em>, <em class="sig-param">n_init</em>, <em class="sig-param">algorithm</em>, <em class="sig-param">dfs</em>, <em class="sig-param">r</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.func_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs on each worker to call fit on local KMeans instance.
Extracts centroids
:param model: Local KMeans instance
:param dfs: List of cudf.Dataframes to use
:param r: Stops memoizatiion caching
:return: The fit model</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.func_predict">
<em class="property">static </em><code class="sig-name descname">func_predict</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">dfs</em>, <em class="sig-param">r</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.func_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs on each worker to call fit on local KMeans instance
:param model: Local KMeans instance
:param dfs: List of cudf.Dataframes to use
:param r: Stops memoization caching
:return: cudf.Series with predictions</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.func_score">
<em class="property">static </em><code class="sig-name descname">func_score</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">dfs</em>, <em class="sig-param">r</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.func_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs on each worker to call fit on local KMeans instance
:param model: Local KMeans instance
:param dfs: List of cudf.Dataframes to use
:param r: Stops memoization caching
:return: cudf.Series with predictions</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.func_transform">
<em class="property">static </em><code class="sig-name descname">func_transform</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">dfs</em>, <em class="sig-param">r</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.func_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs on each worker to call fit on local KMeans instance
:param model: Local KMeans instance
:param dfs: List of cudf.Dataframes to use
:param r: Stops memoizatiion caching
:return: The fit model</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.parallel_func">
<code class="sig-name descname">parallel_func</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">func</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.parallel_func" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels using a distributed KMeans model
:param X: dask_cudf.Dataframe to predict
:return: A dask_cudf.Dataframe containing label predictions</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels using a distributed KMeans model
:param X: dask_cudf.Dataframe to predict
:return: A dask_cudf.Dataframe containing label predictions</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.cluster.KMeans.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.cluster.KMeans.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels using a distributed KMeans model
:param X: dask_cudf.Dataframe to predict
:return: A dask_cudf.Dataframe containing label predictions</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id10">
<h3>Random Forest<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="cuml.dask.ensemble.RandomForestClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.ensemble.</code><code class="sig-name descname">RandomForestClassifier</code><span class="sig-paren">(</span><em class="sig-param">n_estimators=10</em>, <em class="sig-param">max_depth=-1</em>, <em class="sig-param">handle=None</em>, <em class="sig-param">max_features=1.0</em>, <em class="sig-param">n_bins=8</em>, <em class="sig-param">split_algo=1</em>, <em class="sig-param">split_criterion=0</em>, <em class="sig-param">min_rows_per_node=2</em>, <em class="sig-param">bootstrap=True</em>, <em class="sig-param">bootstrap_features=False</em>, <em class="sig-param">type_model='classifier'</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">rows_sample=1.0</em>, <em class="sig-param">max_leaves=-1</em>, <em class="sig-param">n_streams=4</em>, <em class="sig-param">quantile_per_tree=False</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">criterion=None</em>, <em class="sig-param">min_samples_leaf=None</em>, <em class="sig-param">min_weight_fraction_leaf=None</em>, <em class="sig-param">max_leaf_nodes=None</em>, <em class="sig-param">min_impurity_decrease=None</em>, <em class="sig-param">min_impurity_split=None</em>, <em class="sig-param">oob_score=None</em>, <em class="sig-param">n_jobs=None</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">warm_start=None</em>, <em class="sig-param">class_weight=None</em>, <em class="sig-param">workers=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Experimental API implementing a multi-GPU Random Forest classifier
model which fits multiple decision tree classifiers in an
ensemble. This uses Dask to partition data over multiple GPUs
(possibly on different nodes).</p>
<dl class="simple">
<dt>Currently, this API makes the following assumptions:</dt><dd><ul class="simple">
<li><p>The set of Dask workers used between instantiation, fit,
and predict are all consistent</p></li>
<li><p>Training data comes in the form of cuDF dataframes,
distributed so that each worker has at least one partition.</p></li>
</ul>
</dd>
</dl>
<p>Future versions of the API will support more flexible data
distribution and additional input types.</p>
<p>The distributed algorithm uses an embarrassingly-parallel
approach. For a forest with N trees being built on w workers, each
worker simply builds N/w trees on the data it has available
locally. In many cases, partitioning the data so that each worker
builds trees on a subset of the total dataset works well, but
it generally requires the data to be well-shuffled in advance.
Alternatively, callers can replicate all of the data across
workers so that rf.fit receives w partitions, each containing the
same data. This would produce results approximately identical to
single-GPU fitting.</p>
<p>Please check the single-GPU implementation of Random Forest
classifier for more information about the underlying algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 10)</span></dt><dd><p>total number of trees in the forest (not per-worker)</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
<dt><strong>split_criterion: The criterion used to split nodes.</strong></dt><dd><p>0 for GINI, 1 for ENTROPY, 4 for CRITERION_END.
2 and 3 not valid for classification
(default = 0)</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">0 for HIST and 1 for GLOBAL_QUANTILE</span></dt><dd><p>(default = 1)
the algorithm to determine how nodes are split in the tree.</p>
</dd>
<dt><strong>split_criterion: The criterion used to split nodes.</strong></dt><dd><p>0 for GINI, 1 for ENTROPY, 4 for CRITERION_END.
2 and 3 not valid for classification
(default = 0)</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If set, each tree in the forest is built
on a bootstrapped sample with replacement.
If false, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure),
if -1.</p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited,
if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed
to split a node.</p>
</dd>
<dt><strong>quantile_per_tree</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Whether quantile is computed for individal trees in RF.
Only relevant for GLOBAL_QUANTILE split_algo.</p>
</dd>
<dt><strong>n_streams</strong><span class="classifier">int (default = 4 )</span></dt><dd><p>Number of parallel streams used for forest building</p>
</dd>
<dt><strong>workers</strong><span class="classifier">optional, list of strings</span></dt><dd><p>Dask addresses of workers to use for computation.
If None, all available Dask workers will be used.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>For usage examples, please see the RAPIDS notebookss repository:
<a class="reference external" href="https://github.com/rapidsai/notebooks/blob/branch-0.9/cuml/random_forest_demo_mnmg.ipynb">https://github.com/rapidsai/notebooks/blob/branch-0.9/cuml/random_forest_demo_mnmg.ipynb</a></p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.fit" title="cuml.dask.ensemble.RandomForestClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Fit the input data with a Random Forest classifier</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.get_params" title="cuml.dask.ensemble.RandomForestClassifier.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Returns the value of all parameters required to configure this estimator as a dictionary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.predict" title="cuml.dask.ensemble.RandomForestClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.set_params" title="cuml.dask.ensemble.RandomForestClassifier.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, \*\*params)</p></td>
<td><p>Sets the value of parameters required to configure this estimator, it functions similar to the sklearn set_params.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the input data with a Random Forest classifier</p>
<p>IMPORTANT: X is expected to be partitioned with at least one partition
on each Dask worker being used by the forest (self.workers).</p>
<p>If a worker has multiple data partitions, they will be concatenated
before fitting, which will lead to additional memory usage. To minimize
memory consumption, ensure that each worker has exactly one partition.</p>
<p>When persisting data, you can use
cuml.dask.common.utils.persist_across_workers to simplify this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">y_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">X_dask_cudf</span><span class="p">,</span> <span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">persist_across_workers</span><span class="p">(</span><span class="n">dask_client</span><span class="p">,</span>
                                                  <span class="p">[</span><span class="n">X_dask_cudf</span><span class="p">,</span>
                                                   <span class="n">y_dask_cudf</span><span class="p">])</span>
</pre></div>
</div>
<dl>
<dt>(this is equivalent to calling <cite>persist</cite> with the data and workers)::</dt><dd><dl>
<dt>X_dask_cudf, y_dask_cudf = dask_client.persist([X_dask_cudf,</dt><dd><blockquote>
<div><p>y_dask_cudf],</p>
</div></blockquote>
<p>workers={
X_dask_cudf=workers,
y_dask_cudf=workers
})</p>
</dd>
</dl>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask_cudf.Dataframe</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Features of training examples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">dask_cudf.Dataframe</span></dt><dd><p>Dense  matrix (floats or doubles) of shape (n_samples, 1)
Labels of training examples.
<strong>y must be partitioned the same way as X</strong></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of all parameters
required to configure this estimator as a dictionary.
Parameters
———–
deep : boolean (default = True)</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">np.array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Features of examples to predict.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: np.array</dt><dd><p>Dense vector (int) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of parameters required to
configure this estimator, it functions similar to
the sklearn set_params.
Parameters
———–
params : dict of new params</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="cuml.dask.ensemble.RandomForestRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.ensemble.</code><code class="sig-name descname">RandomForestRegressor</code><span class="sig-paren">(</span><em class="sig-param">n_estimators=10</em>, <em class="sig-param">max_depth=-1</em>, <em class="sig-param">handle=None</em>, <em class="sig-param">max_features='auto'</em>, <em class="sig-param">n_bins=8</em>, <em class="sig-param">split_algo=1</em>, <em class="sig-param">split_criterion=2</em>, <em class="sig-param">bootstrap=True</em>, <em class="sig-param">bootstrap_features=False</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">min_rows_per_node=2</em>, <em class="sig-param">rows_sample=1.0</em>, <em class="sig-param">max_leaves=-1</em>, <em class="sig-param">n_streams=4</em>, <em class="sig-param">accuracy_metric='mse'</em>, <em class="sig-param">min_samples_leaf=None</em>, <em class="sig-param">min_weight_fraction_leaf=None</em>, <em class="sig-param">n_jobs=None</em>, <em class="sig-param">max_leaf_nodes=None</em>, <em class="sig-param">min_impurity_decrease=None</em>, <em class="sig-param">min_impurity_split=None</em>, <em class="sig-param">oob_score=None</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">warm_start=None</em>, <em class="sig-param">class_weight=None</em>, <em class="sig-param">quantile_per_tree=False</em>, <em class="sig-param">criterion=None</em>, <em class="sig-param">workers=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Experimental API implementing a multi-GPU Random Forest classifier
model which fits multiple decision tree classifiers in an
ensemble. This uses Dask to partition data over multiple GPUs
(possibly on different nodes).</p>
<dl class="simple">
<dt>Currently, this API makes the following assumptions:</dt><dd><ul class="simple">
<li><p>The set of Dask workers used between instantiation, fit,
and predict are all consistent</p></li>
<li><p>Training data is comes in the form of cuDF dataframes,
distributed so that each worker has at least one partition.</p></li>
</ul>
</dd>
</dl>
<p>Future versions of the API will support more flexible data
distribution and additional input types. User-facing APIs are
expected to change in upcoming versions.</p>
<p>The distributed algorithm uses an embarrassingly-parallel
approach. For a forest with N trees being built on w workers, each
worker simply builds N/w trees on the data it has available
locally. In many cases, partitioning the data so that each worker
builds trees on a subset of the total dataset works well, but
it generally requires the data to be well-shuffled in advance.
Alternatively, callers can replicate all of the data across
workers so that rf.fit receives w partitions, each containing the
same data. This would produce results approximately identical to
single-GPU fitting.</p>
<p>Please check the single-GPU implementation of Random Forest
classifier for more information about the underlying algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 10)</span></dt><dd><p>total number of trees in the forest (not per-worker)</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">int (default = 1)</span></dt><dd><p>0 for HIST, 1 for GLOBAL_QUANTILE
The type of algorithm to be used to create the trees.</p>
</dd>
<dt><strong>split_criterion: int (default = 2)</strong></dt><dd><p>The criterion used to split nodes.
0 for GINI, 1 for ENTROPY,
2 for MSE, 3 for MAE and 4 for CRITERION_END.
0 and 1 not valid for regression</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If set, each tree in the forest is built
on a bootstrapped sample with replacement.
If false, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure),
if -1.</p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited,
if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int or float or string or None (default = ‘auto’)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.
If int then max_features/n_features.
If float then max_features is a fraction.
If ‘auto’ then max_features=n_features which is 1.0.
If ‘sqrt’ then max_features=1/sqrt(n_features).
If ‘log2’ then max_features=log2(n_features)/n_features.
If None, then max_features=n_features which is 1.0.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int or float (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed
to split a node.
If int then number of sample rows
If float the min_rows_per_sample*n_rows</p>
</dd>
<dt><strong>accuracy_metric</strong><span class="classifier">string (default = ‘mse’)</span></dt><dd><p>Decides the metric used to evaluate the performance
of the model.
for median of abs error : ‘median_ae’
for mean of abs error : ‘mean_ae’
for mean square error’ : ‘mse’</p>
</dd>
<dt><strong>n_streams</strong><span class="classifier">int (default = 4 )</span></dt><dd><p>Number of parallel streams used for forest building</p>
</dd>
<dt><strong>workers</strong><span class="classifier">optional, list of strings</span></dt><dd><p>Dask addresses of workers to use for computation.
If None, all available Dask workers will be used.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.fit" title="cuml.dask.ensemble.RandomForestRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Fit the input data with a Random Forest regression model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.get_params" title="cuml.dask.ensemble.RandomForestRegressor.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Returns the value of all parameters required to configure this estimator as a dictionary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.predict" title="cuml.dask.ensemble.RandomForestRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the regressor outputs for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.set_params" title="cuml.dask.ensemble.RandomForestRegressor.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, \*\*params)</p></td>
<td><p>Sets the value of parameters required to configure this estimator, it functions similar to the sklearn set_params.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the input data with a Random Forest regression model</p>
<p>IMPORTANT: X is expected to be partitioned with at least one partition
on each Dask worker being used by the forest (self.workers).</p>
<p>When persisting data, you can use
cuml.dask.common.utils.persist_across_workers to simplify this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">y_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">X_dask_cudf</span><span class="p">,</span> <span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">persist_across_workers</span><span class="p">(</span><span class="n">dask_client</span><span class="p">,</span>
                                                  <span class="p">[</span><span class="n">X_dask_cudf</span><span class="p">,</span>
                                                   <span class="n">y_dask_cudf</span><span class="p">])</span>
</pre></div>
</div>
<dl>
<dt>(this is equivalent to calling <cite>persist</cite> with the data and workers)::</dt><dd><dl>
<dt>X_dask_cudf, y_dask_cudf = dask_client.persist([X_dask_cudf,</dt><dd><blockquote>
<div><p>y_dask_cudf],</p>
</div></blockquote>
<p>workers={
X_dask_cudf=workers,
y_dask_cudf=workers
})</p>
</dd>
</dl>
</dd>
</dl>
<dl class="simple">
<dt>X<span class="classifier">dask_cudf.Dataframe</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).
Features of training examples.</p>
</dd>
<dt>y<span class="classifier">dask_cudf.Dataframe</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, 1)
Labels of training examples.
y must be partitioned the same way as X</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of all parameters
required to configure this estimator as a dictionary.
Parameters
———–
deep : boolean (default = True)</p>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the regressor outputs for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dense matrix (floats or doubles) of shape (n_samples, n_features).</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: NumPy</dt><dd><p>Dense vector (float) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of parameters required to
configure this estimator, it functions similar to
the sklearn set_params.
Parameters
———–
params : dict of new params</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to cuML’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, nvidia

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>