
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>cuML API Reference — cuml 0.16.0 documentation</title>
<link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="_static/params.css" rel="stylesheet" type="text/css"/>
<link href="_static/references.css" rel="stylesheet" type="text/css"/>
<!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="./" id="documentation_options" src="_static/documentation_options.js" type="text/javascript"></script>
<script src="_static/jquery.js"></script>
<script src="_static/underscore.js"></script>
<script src="_static/doctools.js"></script>
<script src="_static/language_data.js"></script>
<script src="_static/example_mod.js"></script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
<script src="_static/js/theme.js" type="text/javascript"></script>
<link href="genindex.html" rel="index" title="Index"/>
<link href="search.html" rel="search" title="Search"/>
<link href="cuml_intro.html" rel="next" title="Intro and key concepts for cuML"/>
<link href="index.html" rel="prev" title="Welcome to cuML’s documentation!"/>
<link href="/assets/css/custom.css" id="rapids-selector-css" rel="stylesheet"/></head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search"><div id="rapids-sphinx-container"><div class="rapids-home-container"><a class="rapids-home-container__home-btn" href="/api">Home</a></div><div class="rapids-selector__container rapids-selector--hidden"><div class="rapids-selector__selected">cuml</div><div class="rapids-selector__menu"><a class="rapids-selector__menu-item" href="/api/clx/stable/api.html">clx</a><a class="rapids-selector__menu-item" href="/api/cudf-java/legacy">cudf-java</a><a class="rapids-selector__menu-item" href="/api/cudf/stable/api.html">cudf</a><a class="rapids-selector__menu-item" href="/api/cugraph/stable/api.html">cugraph</a><a class="rapids-selector__menu-item rapids-selector__menu-item--selected" href="/api/cuml/stable/api.html">cuml</a><a class="rapids-selector__menu-item" href="/api/cusignal/stable/api.html">cusignal</a><a class="rapids-selector__menu-item" href="/api/cuspatial/stable/api.html">cuspatial</a><a class="rapids-selector__menu-item" href="/api/cuxfilter/stable">cuxfilter</a><a class="rapids-selector__menu-item" href="/api/libcudf/stable/namespacecudf.html">libcudf</a><a class="rapids-selector__menu-item" href="/api/libcugraph/stable">libcugraph</a><a class="rapids-selector__menu-item" href="/api/libcuml/stable">libcuml</a><a class="rapids-selector__menu-item" href="/api/rmm/stable/annotated.html">rmm</a></div></div><div class="rapids-selector__container rapids-selector--hidden"><div class="rapids-selector__selected">stable (0.16)</div><div class="rapids-selector__menu"><a class="rapids-selector__menu-item" href="/api/cuml/nightly/api.html">nightly (0.17)</a><a class="rapids-selector__menu-item rapids-selector__menu-item--selected" href="/api/cuml/stable/api.html">stable (0.16)</a><a class="rapids-selector__menu-item" href="/api/cuml/legacy/api.html">legacy (0.15)</a></div></div></div>
<div role="search">
<form action="search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">cuML API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-configuration">Module Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#output-data-type-configuration">Output Data Type Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#verbosity-levels">Verbosity Levels</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#preprocessing-metrics-and-utilities">Preprocessing, Metrics, and Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-selection-and-data-splitting">Model Selection and Data Splitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-and-label-encoding-single-gpu">Feature and Label Encoding (Single-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#text-preprocessing-single-gpu">Text Preprocessing (Single-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-and-label-encoding-dask-based-multi-gpu">Feature and Label Encoding (Dask-based Multi-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-extraction-single-gpu">Feature Extraction (Single-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset-generation-single-gpu">Dataset Generation (Single-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset-generation-dask-based-multi-gpu">Dataset Generation (Dask-based Multi-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#array-wrappers-internal-api">Array Wrappers (Internal API)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metrics-regression-classification-and-distance">Metrics (regression, classification, and distance)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metrics-clustering-and-trustworthiness">Metrics (clustering and trustworthiness)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression-and-classification">Regression and Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasso-regression">Lasso Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#elasticnet-regression">ElasticNet Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mini-batch-sgd-classifier">Mini Batch SGD Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mini-batch-sgd-regressor">Mini Batch SGD Regressor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mutinomial-naive-bayes">Mutinomial Naive Bayes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#forest-inferencing">Forest Inferencing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#coordinate-descent">Coordinate Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quasi-newton">Quasi-Newton</a></li>
<li class="toctree-l3"><a class="reference internal" href="#support-vector-machines">Support Vector Machines</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nearest-neighbors-classification">Nearest Neighbors Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nearest-neighbors-regression">Nearest Neighbors Regression</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#clustering">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means-clustering">K-Means Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dbscan">DBSCAN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dimensionality-reduction-and-manifold-learning">Dimensionality Reduction and Manifold Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principal-component-analysis">Principal Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#truncated-svd">Truncated SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#umap">UMAP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-projections">Random Projections</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tsne">TSNE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neighbors">Neighbors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nearest-neighbors">Nearest Neighbors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id21">Nearest Neighbors Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23">Nearest Neighbors Regression</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#time-series">Time Series</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#holtwinters">HoltWinters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#arima">ARIMA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multi-node-multi-gpu-algorithms">Multi-Node, Multi-GPU Algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id26">K-Means Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id27">Nearest Neighbors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id28">Principal Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30">Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id31">Truncated SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#manifold">Manifold</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-models">Linear Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#naive-bayes">Naive Bayes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#solvers">Solvers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dask-base-classes-and-mixins">Dask Base Classes and Mixins</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#experimental">Experimental</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#decomposition">Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-cuml.experimental.preprocessing">Preprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cuml_intro.html">Intro and key concepts for cuML</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuml_blogs.html">cuML blogs and other references</a></li>
<li class="toctree-l1"><a class="reference internal" href="estimator_intro.html">Training and Evaluating Machine Learning Models in cuML</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="index.html">cuml</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="index.html"></a> »</li>
<li>cuML API Reference</li>
<li class="wy-breadcrumbs-aside">
<a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="cuml-api-reference">
<h1>cuML API Reference<a class="headerlink" href="#cuml-api-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-configuration">
<h2>Module Configuration<a class="headerlink" href="#module-configuration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="output-data-type-configuration">
<span id="id1"></span><h3>Output Data Type Configuration<a class="headerlink" href="#output-data-type-configuration" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="py method">
<dt id="cuml.common.memory_utils.set_global_output_type">
<code class="sig-prename descclassname">memory_utils.</code><code class="sig-name descname">set_global_output_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/memory_utils.py#L235"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.memory_utils.set_global_output_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to set cuML’s single GPU estimators global output type.
It will be used by all estimators unless overriden in their initialization
with their own output_type parameter. Can also be overriden by the context
manager method <a class="reference internal" href="#cuml.common.memory_utils.using_output_type" title="cuml.common.memory_utils.using_output_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">using_output_type()</span></code></a>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’} (default = ‘input’)</span></dt><dd><p>Desired output type of results and attributes of the estimators.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'input'</span></code> will mean that the parameters and methods will mirror the
format of the data sent to the estimators/methods as much as
possible. Specifically:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 60%"/>
<col style="width: 40%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Input type</p></th>
<th class="head"><p>Output type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>cuDF DataFrame or Series</p></td>
<td><p>cuDF DataFrame or Series</p></td>
</tr>
<tr class="row-odd"><td><p>NumPy arrays</p></td>
<td><p>NumPy arrays</p></td>
</tr>
<tr class="row-even"><td><p>Pandas DataFrame or Series</p></td>
<td><p>NumPy arrays</p></td>
</tr>
<tr class="row-odd"><td><p>Numba device arrays</p></td>
<td><p>Numba device arrays</p></td>
</tr>
<tr class="row-even"><td><p>CuPy arrays</p></td>
<td><p>CuPy arrays</p></td>
</tr>
<tr class="row-odd"><td><p>Other <cite>__cuda_array_interface__</cite> objs</p></td>
<td><p>CuPy arrays</p></td>
</tr>
</tbody>
</table>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'cudf'</span></code> will return cuDF Series for single dimensional results and
DataFrames for the rest.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'cupy'</span></code> will return CuPy arrays.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'numpy'</span></code> will return NumPy arrays.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p><code class="docutils literal notranslate"><span class="pre">'cupy'</span></code> and <code class="docutils literal notranslate"><span class="pre">'numba'</span></code> options (as well as <code class="docutils literal notranslate"><span class="pre">'input'</span></code> when using Numba
and CuPy ndarrays for input) have the least overhead. cuDF add memory
consumption and processing time needed to build the Series and DataFrames.
<code class="docutils literal notranslate"><span class="pre">'numpy'</span></code> has the biggest overhead due to the need to transfer data to
CPU memory.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cuml</span>
<span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>

<span class="n">ary</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="n">ary</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>

<span class="n">cuml</span><span class="o">.</span><span class="n">set_global_output_type</span><span class="p">(</span><span class="s1">'cudf'</span><span class="p">):</span>
<span class="n">dbscan_float</span> <span class="o">=</span> <span class="n">cuml</span><span class="o">.</span><span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"cuML output type"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cuML output type
0    0
1    1
2    2
dtype: int32
&lt;class 'cudf.core.series.Series'&gt;
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.common.memory_utils.using_output_type">
<code class="sig-prename descclassname">memory_utils.</code><code class="sig-name descname">using_output_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/memory_utils.py#L326"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.memory_utils.using_output_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager method to set cuML’s global output type inside a <cite>with</cite>
statement. It gets reset to the prior value it had once the <cite>with</cite> code
block is executer.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’} (default = ‘input’)</span></dt><dd><p>Desired output type of results and attributes of the estimators.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'input'</span></code> will mean that the parameters and methods will mirror the
format of the data sent to the estimators/methods as much as
possible. Specifically:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 60%"/>
<col style="width: 40%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Input type</p></th>
<th class="head"><p>Output type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>cuDF DataFrame or Series</p></td>
<td><p>cuDF DataFrame or Series</p></td>
</tr>
<tr class="row-odd"><td><p>NumPy arrays</p></td>
<td><p>NumPy arrays</p></td>
</tr>
<tr class="row-even"><td><p>Pandas DataFrame or Series</p></td>
<td><p>NumPy arrays</p></td>
</tr>
<tr class="row-odd"><td><p>Numba device arrays</p></td>
<td><p>Numba device arrays</p></td>
</tr>
<tr class="row-even"><td><p>CuPy arrays</p></td>
<td><p>CuPy arrays</p></td>
</tr>
<tr class="row-odd"><td><p>Other <cite>__cuda_array_interface__</cite> objs</p></td>
<td><p>CuPy arrays</p></td>
</tr>
</tbody>
</table>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'cudf'</span></code> will return cuDF Series for single dimensional results and
DataFrames for the rest.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'cupy'</span></code> will return CuPy arrays.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'numpy'</span></code> will return NumPy arrays.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cuml</span>
<span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>

<span class="n">ary</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="n">ary</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>

<span class="k">with</span> <span class="n">cuml</span><span class="o">.</span><span class="n">using_output_type</span><span class="p">(</span><span class="s1">'cudf'</span><span class="p">):</span>
    <span class="n">dbscan_float</span> <span class="o">=</span> <span class="n">cuml</span><span class="o">.</span><span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dbscan_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"cuML output inside 'with' context"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">))</span>

<span class="c1"># use cuml again outside the context manager</span>
<span class="n">dbscan_float2</span> <span class="o">=</span> <span class="n">cuml</span><span class="o">.</span><span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan_float2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"cuML default output"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dbscan_float2</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dbscan_float2</span><span class="o">.</span><span class="n">labels_</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cuML output inside 'with' context
0    0
1    1
2    2
dtype: int32
&lt;class 'cudf.core.series.Series'&gt;

cuML default output
[0 1 2]
&lt;class 'cupy.core.core.ndarray'&gt;
</pre></div>
</div>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="verbosity-levels">
<span id="id2"></span><h3>Verbosity Levels<a class="headerlink" href="#verbosity-levels" title="Permalink to this headline">¶</a></h3>
<p>cuML follows a verbosity model similar to Scikit-learn’s: The verbose parameter
can be a boolean, or a numeric value, and higher numeric values mean more verbosity. The exact values can be set directly, or through the cuml.common.logger module, and
they are:</p>
<table class="colwidths-given docutils align-default" id="id43">
<caption><span class="caption-text">Verbosity Levels</span><a class="headerlink" href="#id43" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%"/>
<col style="width: 25%"/>
<col style="width: 50%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Numeric value</p></th>
<th class="head"><p>cuml.common.logger value</p></th>
<th class="head"><p>Verbosity level</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>cuml.common.logger.level_off</p></td>
<td><p>Disables all log messages</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>cuml.common.logger.level_critical</p></td>
<td><p>Enables only critical messages</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>cuml.common.logger.level_error</p></td>
<td><p>Enables all messages up to and including errors.</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>cuml.common.logger.level_warn</p></td>
<td><p>Enables all messages up to and including warnings.</p></td>
</tr>
<tr class="row-even"><td><p>4 or False</p></td>
<td><p>cuml.common.logger.level_info</p></td>
<td><p>Enables all messages up to and including information messages.</p></td>
</tr>
<tr class="row-odd"><td><p>5 or True</p></td>
<td><p>cuml.common.logger.level_debug</p></td>
<td><p>Enables all messages up to and including debug messages.</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>cuml.common.logger.level_trace</p></td>
<td><p>Enables all messages up to and including trace messages.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="preprocessing-metrics-and-utilities">
<h2>Preprocessing, Metrics, and Utilities<a class="headerlink" href="#preprocessing-metrics-and-utilities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="model-selection-and-data-splitting">
<h3>Model Selection and Data Splitting<a class="headerlink" href="#model-selection-and-data-splitting" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="py method">
<dt id="cuml.preprocessing.model_selection.train_test_split">
<code class="sig-prename descclassname">model_selection.</code><code class="sig-name descname">train_test_split</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">test_size</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">train_size</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span>cupy.random._generator.RandomState<span class="p">, </span>numpy.random.mtrand.RandomState<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">seed</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span>cupy.random._generator.RandomState<span class="p">, </span>numpy.random.mtrand.RandomState<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stratify</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/model_selection.py#L213"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.model_selection.train_test_split" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions device data into four collated objects, mimicking
Scikit-learn’s <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cudf.DataFrame or cuda_array_interface compliant device array</span></dt><dd><p>Data to split, has shape (n_samples, n_features)</p>
</dd>
<dt><strong>y</strong><span class="classifier">str, cudf.Series or cuda_array_interface compliant device array</span></dt><dd><p>Set of labels for the data, either a series of shape (n_samples) or
the string label of a column in X (if it is a cuDF DataFrame)
containing the labels</p>
</dd>
<dt><strong>train_size</strong><span class="classifier">float or int, optional</span></dt><dd><p>If float, represents the proportion [0, 1] of the data
to be assigned to the training set. If an int, represents the number
of instances to be assigned to the training set. Defaults to 0.8</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">bool, optional</span></dt><dd><p>Whether or not to shuffle inputs before splitting</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, CuPy RandomState or NumPy RandomState optional</span></dt><dd><p>If shuffle is true, seeds the generator. Unseeded by default</p>
</dd>
<dt><strong>seed: random_state</strong><span class="classifier">int, CuPy RandomState or NumPy RandomState optional</span></dt><dd><p>Deprecated in favor of <cite>random_state</cite>.
If shuffle is true, seeds the generator. Unseeded by default</p>
</dd>
<dt><strong>stratify: bool, optional</strong></dt><dd><p>Whether to stratify the input data based on class labels.
None by default</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_train, X_test, y_train, y_test</strong><span class="classifier">cudf.DataFrame or array-like objects</span></dt><dd><p>Partitioned dataframes if X and y were cuDF objects. If <cite>y</cite> was
provided as a column name, the column was dropped from <cite>X</cite>.
Partitioned numba device arrays if X and y were Numba device arrays.
Partitioned CuPy arrays for any other input.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.preprocessing.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Generate some sample data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">'x'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
                     <span class="s1">'y'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Original data: </span><span class="si">{</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> elements'</span><span class="p">)</span>

<span class="c1"># Suppose we want an 80/20 split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span>
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'X_train: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> elements'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'X_test: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> elements'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'y_train: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> elements'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'y_test: </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> elements'</span><span class="p">)</span>

<span class="c1"># Alternatively, if our labels are stored separately</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'y'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># we can also do</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Original</span> <span class="n">data</span><span class="p">:</span> <span class="mi">10</span> <span class="n">elements</span>
<span class="n">X_train</span><span class="p">:</span> <span class="mi">8</span> <span class="n">elements</span>
<span class="n">X_test</span><span class="p">:</span> <span class="mi">2</span> <span class="n">elements</span>
<span class="n">y_train</span><span class="p">:</span> <span class="mi">8</span> <span class="n">elements</span>
<span class="n">y_test</span><span class="p">:</span> <span class="mi">2</span> <span class="n">elements</span>
</pre></div>
</div>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="feature-and-label-encoding-single-gpu">
<h3>Feature and Label Encoding (Single-GPU)<a class="headerlink" href="#feature-and-label-encoding-single-gpu" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="py class">
<dt id="cuml.preprocessing.LabelEncoder.LabelEncoder">
<em class="property">class </em><code class="sig-prename descclassname">cuml.preprocessing.LabelEncoder.</code><code class="sig-name descname">LabelEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">handle_unknown</span><span class="o">=</span><span class="default_value">'error'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/LabelEncoder.py#L26"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelEncoder.LabelEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>An nvcategory based implementation of ordinal label encoding</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle_unknown</strong><span class="classifier">{‘error’, ‘ignore’}, default=’error’</span></dt><dd><p>Whether to raise an error or ignore if an unknown categorical feature
is present during transform (default is to raise). When this parameter
is set to ‘ignore’ and an unknown category is encountered during
transform or inverse transform, the resulting encoding will be null.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Converting a categorical implementation to a numerical one</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cudf</span> <span class="kn">import</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">Series</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s1">'category'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'c'</span><span class="p">,</span> <span class="s1">'d'</span><span class="p">]})</span>

<span class="c1"># There are two functionally equivalent ways to do this</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">category</span><span class="p">)</span>  <span class="c1"># le = le.fit(data.category) also works</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">category</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="c1"># This method is preferred</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">category</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="c1"># We can assign this to a new column</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">encoded</span><span class="o">=</span><span class="n">encoded</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># We can also encode more data</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">Series</span><span class="p">([</span><span class="s1">'c'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">])</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="c1"># After train, ordinal label can be inverse_transform() back to</span>
<span class="c1"># string labels</span>
<span class="n">ord_label</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ord_label</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">str_label</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">ord_label</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">str_label</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>    <span class="mi">0</span>
<span class="mi">1</span>    <span class="mi">1</span>
<span class="mi">2</span>    <span class="mi">2</span>
<span class="mi">3</span>    <span class="mi">3</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>

<span class="mi">0</span>    <span class="mi">0</span>
<span class="mi">1</span>    <span class="mi">1</span>
<span class="mi">2</span>    <span class="mi">2</span>
<span class="mi">3</span>    <span class="mi">3</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">int32</span>

<span class="n">category</span>  <span class="n">encoded</span>
<span class="mi">0</span>         <span class="n">a</span>        <span class="mi">0</span>
<span class="mi">1</span>         <span class="n">b</span>        <span class="mi">1</span>
<span class="mi">2</span>         <span class="n">c</span>        <span class="mi">2</span>
<span class="mi">3</span>         <span class="n">d</span>        <span class="mi">3</span>

<span class="mi">0</span>    <span class="mi">2</span>
<span class="mi">1</span>    <span class="mi">0</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>

<span class="mi">0</span>    <span class="n">a</span>
<span class="mi">1</span>    <span class="n">a</span>
<span class="mi">2</span>    <span class="n">b</span>
<span class="mi">3</span>    <span class="n">c</span>
<span class="mi">4</span>    <span class="n">b</span>
<span class="n">dtype</span><span class="p">:</span> <span class="nb">object</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.fit" title="cuml.preprocessing.LabelEncoder.LabelEncoder.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(y[, _classes])</p></td>
<td><p>Fit a LabelEncoder (nvcategory) instance to a set of categories</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.fit_transform" title="cuml.preprocessing.LabelEncoder.LabelEncoder.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(y)</p></td>
<td><p>Simultaneously fit and transform an input</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.get_param_names" title="cuml.preprocessing.LabelEncoder.LabelEncoder.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p>Returns a list of hyperparameter names owned by this class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.inverse_transform" title="cuml.preprocessing.LabelEncoder.LabelEncoder.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(y)</p></td>
<td><p>Revert ordinal label to original label</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.transform" title="cuml.preprocessing.LabelEncoder.LabelEncoder.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(y)</p></td>
<td><p>Transform an input into its categorical keys.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.preprocessing.LabelEncoder.LabelEncoder.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">_classes</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/LabelEncoder.py#L155"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a LabelEncoder (nvcategory) instance to a set of categories</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cudf.Series</span></dt><dd><p>Series containing the categories to be encoded. It’s elements
may or may not be unique</p>
</dd>
<dt><strong>_classes: int or None.</strong></dt><dd><p>Passed by the dask client when dask LabelEncoder is used.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">LabelEncoder</span></dt><dd><p>A fitted instance of itself to allow method chaining</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelEncoder.LabelEncoder.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span><span class="p">:</span> <span class="n">cudf.core.series.Series</span></em><span class="sig-paren">)</span> → cudf.core.series.Series<a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/LabelEncoder.py#L223"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Simultaneously fit and transform an input</p>
<p>This is functionally equivalent to (but faster than)
<cite>LabelEncoder().fit(y).transform(y)</cite></p>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelEncoder.LabelEncoder.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/LabelEncoder.py#L277"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of hyperparameter names owned by this class. It is
expected that every child class overrides this method and appends its
extra set of parameters that it in-turn owns. This is to simplify the
implementation of <cite>get_params</cite> and <cite>set_params</cite> methods.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelEncoder.LabelEncoder.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span><span class="p">:</span> <span class="n">cudf.core.series.Series</span></em><span class="sig-paren">)</span> → cudf.core.series.Series<a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/LabelEncoder.py#L238"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Revert ordinal label to original label</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cudf.Series, dtype=int32</span></dt><dd><p>Ordinal labels to be reverted</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>reverted</strong><span class="classifier">cudf.Series</span></dt><dd><p>Reverted labels</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelEncoder.LabelEncoder.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span><span class="p">:</span> <span class="n">cudf.core.series.Series</span></em><span class="sig-paren">)</span> → cudf.core.series.Series<a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/LabelEncoder.py#L186"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelEncoder.LabelEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform an input into its categorical keys.</p>
<p>This is intended for use with small inputs relative to the size of the
dataset. For fitting and transforming an entire dataset, prefer
<cite>fit_transform</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cudf.Series</span></dt><dd><p>Input keys to be transformed. Its values should match the
categories given to <cite>fit</cite></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>encoded</strong><span class="classifier">cudf.Series</span></dt><dd><p>The ordinally encoded input series</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><dl class="simple">
<dt>KeyError</dt><dd><p>if a category appears that was not seen in <cite>fit</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.preprocessing.LabelBinarizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.preprocessing.</code><code class="sig-name descname">LabelBinarizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">neg_label</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pos_label</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">sparse_output</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L72"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelBinarizer" title="Permalink to this definition">¶</a></dt>
<dd><p>A multi-class dummy encoder for labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>neg_label</strong><span class="classifier">integer</span></dt><dd><p>label to be used as the negative binary label</p>
</dd>
<dt><strong>pos_label</strong><span class="classifier">integer</span></dt><dd><p>label to be used as the positive binary label</p>
</dd>
<dt><strong>sparse_output</strong><span class="classifier">bool</span></dt><dd><p>whether to return sparse arrays for transformed output</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Create an array with labels and dummy encode them</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="kn">import</span> <span class="nn">cupyx</span>
<span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

<span class="n">encoded</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="n">decoded</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]]</span>

 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">5</span> <span class="mi">10</span>  <span class="mi">7</span>  <span class="mi">2</span>  <span class="mi">4</span>  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">4</span>  <span class="mi">3</span>  <span class="mi">2</span>  <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelBinarizer.fit" title="cuml.preprocessing.LabelBinarizer.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(y)</p></td>
<td><p>Fit label binarizer</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelBinarizer.fit_transform" title="cuml.preprocessing.LabelBinarizer.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(y)</p></td>
<td><p>Fit label binarizer and transform multi-class labels to their dummy-encoded representation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelBinarizer.get_param_names" title="cuml.preprocessing.LabelBinarizer.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p>Returns a list of hyperparameter names owned by this class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelBinarizer.inverse_transform" title="cuml.preprocessing.LabelBinarizer.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(y[, threshold])</p></td>
<td><p>Transform binary labels back to original multi-class labels</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.LabelBinarizer.transform" title="cuml.preprocessing.LabelBinarizer.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(y)</p></td>
<td><p>Transform multi-class labels to their dummy-encoded representation labels.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.preprocessing.LabelBinarizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L176"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelBinarizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit label binarizer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array of shape [n_samples,] or [n_samples, n_classes]</span></dt><dd><p>Target values. The 2-d matrix should only contain 0 and 1,
represents multilabel classification.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">returns an instance of self.</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelBinarizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L211"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelBinarizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit label binarizer and transform multi-class labels to their
dummy-encoded representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array of shape [n_samples,] or [n_samples, n_classes]</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>arr</strong><span class="classifier">array with encoded labels</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelBinarizer.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L283"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelBinarizer.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of hyperparameter names owned by this class. It is
expected that every child class overrides this method and appends its
extra set of parameters that it in-turn owns. This is to simplify the
implementation of <cite>get_params</cite> and <cite>set_params</cite> methods.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelBinarizer.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L246"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelBinarizer.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform binary labels back to original multi-class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array of shape [n_samples, n_classes]</span></dt><dd></dd>
<dt><strong>threshold</strong><span class="classifier">float this value is currently ignored</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>arr</strong><span class="classifier">array with original labels</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.LabelBinarizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L228"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.LabelBinarizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform multi-class labels to their dummy-encoded representation
labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array of shape [n_samples,] or [n_samples, n_classes]</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>arr</strong><span class="classifier">array with encoded labels</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.label_binarize">
<code class="sig-prename descclassname">preprocessing.</code><code class="sig-name descname">label_binarize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">classes</span></em>, <em class="sig-param"><span class="n">neg_label</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pos_label</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">sparse_output</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/label.py#L27"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.label_binarize" title="Permalink to this definition">¶</a></dt>
<dd><p>A stateless helper function to dummy encode multi-class labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array-like of size [n_samples,] or [n_samples, n_classes]</span></dt><dd></dd>
<dt><strong>classes</strong><span class="classifier">the set of unique classes in the input</span></dt><dd></dd>
<dt><strong>neg_label</strong><span class="classifier">integer the negative value for transformed output</span></dt><dd></dd>
<dt><strong>pos_label</strong><span class="classifier">integer the positive value for transformed output</span></dt><dd></dd>
<dt><strong>sparse_output</strong><span class="classifier">bool whether to return sparse array</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.preprocessing.OneHotEncoder">
<em class="property">class </em><code class="sig-prename descclassname">cuml.preprocessing.</code><code class="sig-name descname">OneHotEncoder</code><span class="sig-paren">(</span><em class="sig-param">categories='auto'</em>, <em class="sig-param">drop=None</em>, <em class="sig-param">sparse=True</em>, <em class="sig-param">dtype=&lt;class 'float'&gt;</em>, <em class="sig-param">handle_unknown='error'</em>, <em class="sig-param">*</em>, <em class="sig-param">handle=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">output_type=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/encoders.py#L30"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode categorical features as a one-hot numeric array.
The input to this estimator should be a cuDF.DataFrame or a cupy.ndarray,
denoting the unique values taken on by categorical (discrete) features.
The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’)
encoding scheme. This creates a binary column for each category and
returns a sparse matrix or dense array (depending on the <code class="docutils literal notranslate"><span class="pre">sparse</span></code>
parameter).
By default, the encoder derives the categories based on the unique values
in each feature. Alternatively, you can also specify the <cite>categories</cite>
manually.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>a one-hot encoding of y labels should use a LabelBinarizer
instead.</p>
</div>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>categories</strong><span class="classifier">‘auto’ an cupy.ndarray or a cudf.DataFrame, default=’auto’</span></dt><dd><blockquote>
<div><p>Categories (unique values) per feature:</p>
</div></blockquote>
<ul class="simple">
<li><p>‘auto’ : Determine categories automatically from the training data.</p></li>
<li><p>DataFrame/ndarray : <code class="docutils literal notranslate"><span class="pre">categories[col]</span></code> holds the categories expected
in the feature col.</p></li>
</ul>
</dd>
<dt><strong>drop</strong><span class="classifier">‘first’, None, a dict or a list, default=None</span></dt><dd><p>Specifies a methodology to use to drop one of the categories per
feature. This is useful in situations where perfectly collinear
features cause problems, such as when feeding the resulting data
into a neural network or an unregularized regression.</p>
<ul class="simple">
<li><p>None : retain all features (the default).</p></li>
<li><p>‘first’ : drop the first category in each feature. If only one
category is present, the feature will be dropped entirely.</p></li>
<li><p>dict/list : <code class="docutils literal notranslate"><span class="pre">drop[col]</span></code> is the category in feature col that
should be dropped.</p></li>
</ul>
</dd>
<dt><strong>sparse</strong><span class="classifier">bool, default=False</span></dt><dd><p>This feature was deactivated and will give an exception when True.
The reason is because sparse matrix are not fully supported by cupy
yet, causing incorrect values when computing one hot encodings.
See <a class="reference external" href="https://github.com/cupy/cupy/issues/3223">https://github.com/cupy/cupy/issues/3223</a></p>
</dd>
<dt><strong>dtype</strong><span class="classifier">number type, default=np.float</span></dt><dd><p>Desired datatype of transform’s output.</p>
</dd>
<dt><strong>handle_unknown</strong><span class="classifier">{‘error’, ‘ignore’}, default=’error’</span></dt><dd><p>Whether to raise an error or ignore if an unknown categorical feature
is present during transform (default is to raise). When this parameter
is set to ‘ignore’ and an unknown category is encountered during
transform, the resulting one-hot encoded columns for this feature
will be all zeros. In the inverse transform, an unknown category
will be denoted as None.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>drop_idx_</strong><span class="classifier">array of shape (n_features,)</span></dt><dd><p><code class="docutils literal notranslate"><span class="pre">drop_idx_[i]</span></code> is the index in <code class="docutils literal notranslate"><span class="pre">categories_[i]</span></code> of the category to
be dropped for each feature. None if all the transformed features will
be retained.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.OneHotEncoder.fit" title="cuml.preprocessing.OneHotEncoder.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X)</p></td>
<td><p>Fit OneHotEncoder to X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.OneHotEncoder.fit_transform" title="cuml.preprocessing.OneHotEncoder.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(X)</p></td>
<td><p>Fit OneHotEncoder to X, then transform X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.OneHotEncoder.get_param_names" title="cuml.preprocessing.OneHotEncoder.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p>Returns a list of hyperparameter names owned by this class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.OneHotEncoder.inverse_transform" title="cuml.preprocessing.OneHotEncoder.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(X)</p></td>
<td><p>Convert the data back to the original representation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.OneHotEncoder.transform" title="cuml.preprocessing.OneHotEncoder.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X)</p></td>
<td><p>Transform X using one-hot encoding.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.preprocessing.OneHotEncoder.categories_">
<em class="property">property </em><code class="sig-name descname">categories_</code><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder.categories_" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns categories used for the one hot encoding in the correct order.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.OneHotEncoder.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/encoders.py#L230"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit OneHotEncoder to X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF.DataFrame or cupy.ndarray, shape = (n_samples, n_features)</span></dt><dd><p>The data to determine the categories of each feature.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.OneHotEncoder.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/encoders.py#L274"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit OneHotEncoder to X, then transform X.
Equivalent to fit(X).transform(X).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cudf.DataFrame or cupy.ndarray, shape = (n_samples, n_features)</span></dt><dd><p>The data to encode.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_out</strong><span class="classifier">sparse matrix if sparse=True else a 2-d array</span></dt><dd><p>Transformed input.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.OneHotEncoder.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/encoders.py#L456"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of hyperparameter names owned by this class. It is
expected that every child class overrides this method and appends its
extra set of parameters that it in-turn owns. This is to simplify the
implementation of <cite>get_params</cite> and <cite>set_params</cite> methods.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.OneHotEncoder.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/encoders.py#L384"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the data back to the original representation.
In case unknown categories are encountered (all zeros in the
one-hot encoding), <code class="docutils literal notranslate"><span class="pre">None</span></code> is used to represent this category.</p>
<p>The return type is the same as the type of the input used by the first
call to fit on this estimator instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like or sparse matrix, shape [n_samples, n_encoded_features]</span></dt><dd><p>The transformed data.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_tr</strong><span class="classifier">cudf.DataFrame or cupy.ndarray</span></dt><dd><p>Inverse transformed array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.OneHotEncoder.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/encoders.py#L293"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.OneHotEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X using one-hot encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cudf.DataFrame or cupy.ndarray</span></dt><dd><p>The data to encode.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_out</strong><span class="classifier">sparse matrix if sparse=True else a 2-d array</span></dt><dd><p>Transformed input.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.preprocessing.TargetEncoder.TargetEncoder">
<em class="property">class </em><code class="sig-prename descclassname">cuml.preprocessing.TargetEncoder.</code><code class="sig-name descname">TargetEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_folds</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">smooth</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">seed</span><span class="o">=</span><span class="default_value">42</span></em>, <em class="sig-param"><span class="n">split_method</span><span class="o">=</span><span class="default_value">'interleaved'</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/TargetEncoder.py#L24"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.TargetEncoder.TargetEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>A cudf based implementation of target encoding <a class="reference internal" href="#rc0631e306828-1" id="id3">[1]</a>, which converts
one or mulitple categorical variables, ‘Xs’, with the average of
corresponding values of the target variable, ‘Y’. The input data is
grouped by the columns <cite>Xs</cite> and the aggregated mean value of <cite>Y</cite> of
each group is calculated to replace each value of <cite>Xs</cite>. Several
optimizations are applied to prevent label leakage and parallelize
the execution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_folds</strong><span class="classifier">int (default=4)</span></dt><dd><p>Default number of folds for fitting training data. To prevent
label leakage in <cite>fit</cite>, we split data into <cite>n_folds</cite> and
encode one fold using the target variables of the remaining folds.</p>
</dd>
<dt><strong>smooth</strong><span class="classifier">float (default=0)</span></dt><dd><p>0 &lt;= smooth &lt;= 1
Percentage of samples to smooth the encoding</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (default=42)</span></dt><dd><p>Random seed</p>
</dd>
<dt><strong>split_method</strong><span class="classifier">{‘random’, ‘continuous’, ‘interleaved’},</span></dt><dd><p>default=’interleaved’
Method to split train data into <cite>n_folds</cite>.
‘random’: random split.
‘continuous’: consecutive samples are grouped into one folds.
‘interleaved’: samples are assign to each fold in a round robin way.</p>
</dd>
<dt><strong>output_type: {‘cupy’, ‘numpy’, ‘auto’}, default = ‘auto’</strong></dt><dd><p>The data type of output. If ‘auto’, it matches input data.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="rc0631e306828-1"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p><a class="reference external" href="https://maxhalford.github.io/blog/target-encoding/">https://maxhalford.github.io/blog/target-encoding/</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Converting a categorical implementation to a numerical one</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cudf</span> <span class="kn">import</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">Series</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s1">'category'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">],</span>
                   <span class="s1">'label'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]})</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s1">'category'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'c'</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">]})</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">TargetEncoder</span><span class="p">()</span>
<span class="n">train_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">category</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
<span class="n">test_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">category</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_encoded</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_encoded</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">0.</span> <span class="mf">1.</span><span class="p">]</span>
<span class="p">[</span><span class="mf">1.</span>   <span class="mf">0.75</span> <span class="mf">0.5</span>  <span class="mf">1.</span>  <span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.TargetEncoder.TargetEncoder.fit" title="cuml.preprocessing.TargetEncoder.TargetEncoder.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(x, y)</p></td>
<td><p>Fit a TargetEncoder instance to a set of categories</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.preprocessing.TargetEncoder.TargetEncoder.fit_transform" title="cuml.preprocessing.TargetEncoder.TargetEncoder.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(x, y)</p></td>
<td><p>Simultaneously fit and transform an input</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.TargetEncoder.TargetEncoder.transform" title="cuml.preprocessing.TargetEncoder.TargetEncoder.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(x)</p></td>
<td><p>Transform an input into its categorical keys.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.preprocessing.TargetEncoder.TargetEncoder.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/TargetEncoder.py#L118"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.TargetEncoder.TargetEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a TargetEncoder instance to a set of categories</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x: cudf.Series or cudf.DataFrame or cupy.ndarray</strong></dt><dd><p>categories to be encoded. It’s elements may or may
not be unique</p>
</dd>
<dt><strong>y</strong><span class="classifier">cudf.Series or cupy.ndarray</span></dt><dd><p>Series containing the target variable.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">TargetEncoder</span></dt><dd><p>A fitted instance of itself to allow method chaining</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.TargetEncoder.TargetEncoder.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/TargetEncoder.py#L141"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.TargetEncoder.TargetEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Simultaneously fit and transform an input</p>
<p>This is functionally equivalent to (but faster than)
<cite>TargetEncoder().fit(y).transform(y)</cite></p>
</dd></dl>
<dl class="py method">
<dt id="cuml.preprocessing.TargetEncoder.TargetEncoder.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/TargetEncoder.py#L151"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.TargetEncoder.TargetEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform an input into its categorical keys.</p>
<p>This is intended for test data. For fitting and transforming
the training data, prefer <cite>fit_transform</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">cudf.Series</span></dt><dd><p>Input keys to be transformed. Its values doesn’t have to
match the categories given to <cite>fit</cite></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>encoded</strong><span class="classifier">cupy.ndarray</span></dt><dd><p>The ordinally encoded input series</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="text-preprocessing-single-gpu">
<h3>Text Preprocessing (Single-GPU)<a class="headerlink" href="#text-preprocessing-single-gpu" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="py class">
<dt id="cuml.preprocessing.text.stem.PorterStemmer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.preprocessing.text.stem.</code><code class="sig-name descname">PorterStemmer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">mode</span><span class="o">=</span><span class="default_value">'NLTK_EXTENSIONS'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/text/stem/porter_stemmer.py#L49"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.text.stem.PorterStemmer" title="Permalink to this definition">¶</a></dt>
<dd><p>A word stemmer based on the Porter stemming algorithm.</p>
<p>Porter, M. “An algorithm for suffix stripping.”
Program 14.3 (1980): 130-137.</p>
<p>See <a class="reference external" href="http://www.tartarus.org/~martin/PorterStemmer/">http://www.tartarus.org/~martin/PorterStemmer/</a> for the homepage
of the algorithm.</p>
<p>Martin Porter has endorsed several modifications to the Porter
algorithm since writing his original paper, and those extensions are
included in the implementations on his website. Additionally, others
have proposed further improvements to the algorithm, including NLTK
contributors. Only below mode is supported currently
PorterStemmer.NLTK_EXTENSIONS</p>
<ul class="simple">
<li><p>Implementation that includes further improvements devised by
NLTK contributors or taken from other modified implementations
found on the web.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>mode: Modes of stemming (Only supports (NLTK_EXTENSIONS) currently)</strong></dt><dd><p>default(“NLTK_EXTENSIONS”)</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.preprocessing.text.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="n">word_str_ser</span> <span class="o">=</span>  <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="s1">'revival'</span><span class="p">,</span><span class="s1">'singing'</span><span class="p">,</span><span class="s1">'adjustable'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word_str_ser</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>     <span class="n">reviv</span>
<span class="mi">1</span>      <span class="n">sing</span>
<span class="mi">2</span>    <span class="n">adjust</span>
<span class="n">dtype</span><span class="p">:</span> <span class="nb">object</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.preprocessing.text.stem.PorterStemmer.stem" title="cuml.preprocessing.text.stem.PorterStemmer.stem"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stem</span></code></a>(word_str_ser)</p></td>
<td><p>Stem Words using Porter stemmer</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.preprocessing.text.stem.PorterStemmer.stem">
<code class="sig-name descname">stem</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">word_str_ser</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/preprocessing/text/stem/porter_stemmer.py#L103"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.preprocessing.text.stem.PorterStemmer.stem" title="Permalink to this definition">¶</a></dt>
<dd><p>Stem Words using Porter stemmer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>word_str_ser</strong><span class="classifier">cudf.Series</span></dt><dd><p>A string series of words to stem</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>stemmed_ser</strong><span class="classifier">cudf.Series</span></dt><dd><p>Stemmed words strings series</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="feature-and-label-encoding-dask-based-multi-gpu">
<h3>Feature and Label Encoding (Dask-based Multi-GPU)<a class="headerlink" href="#feature-and-label-encoding-dask-based-multi-gpu" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="py class">
<dt id="cuml.dask.preprocessing.LabelBinarizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.preprocessing.</code><code class="sig-name descname">LabelBinarizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/label.py#L27"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.LabelBinarizer" title="Permalink to this definition">¶</a></dt>
<dd><p>A distributed version of LabelBinarizer for one-hot encoding
a collection of labels.</p>
<p class="rubric">Examples</p>
<p>Create an array with labels and dummy encode them</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="kn">import</span> <span class="nn">cupyx</span>
<span class="kn">from</span> <span class="nn">cuml.dask.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>

<span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">import</span> <span class="nn">dask</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">array</span><span class="o">.</span><span class="n">from_array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

<span class="n">encoded</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">encoded</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>

<span class="n">decoded</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">decoded</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]]</span>

 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">5</span> <span class="mi">10</span>  <span class="mi">7</span>  <span class="mi">2</span>  <span class="mi">4</span>  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">4</span>  <span class="mi">3</span>  <span class="mi">2</span>  <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.LabelBinarizer.fit" title="cuml.dask.preprocessing.LabelBinarizer.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(y)</p></td>
<td><p>Fit label binarizer</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.LabelBinarizer.fit_transform" title="cuml.dask.preprocessing.LabelBinarizer.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(y)</p></td>
<td><p>Fit the label encoder and return transformed labels</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.LabelBinarizer.inverse_transform" title="cuml.dask.preprocessing.LabelBinarizer.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(y[, threshold])</p></td>
<td><p>Invert a set of encoded labels back to original labels</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.LabelBinarizer.transform" title="cuml.dask.preprocessing.LabelBinarizer.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(y)</p></td>
<td><p>Transform and return encoded labels</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.preprocessing.LabelBinarizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/label.py#L128"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.LabelBinarizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit label binarizer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask.Array of shape [n_samples,] or [n_samples, n_classes]</span></dt><dd><p>chunked by row.
Target values. The 2-d matrix should only contain 0 and 1,
represents multilabel classification.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">returns an instance of self.</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.preprocessing.LabelBinarizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/label.py#L157"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.LabelBinarizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the label encoder and return transformed labels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask.Array of shape [n_samples,] or [n_samples, n_classes]</span></dt><dd><p>target values. The 2-d matrix should only contain 0 and 1,
represents multilabel classification.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>arr</strong><span class="classifier">Dask.Array backed by CuPy arrays containing encoded labels</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.preprocessing.LabelBinarizer.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/label.py#L203"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.LabelBinarizer.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Invert a set of encoded labels back to original labels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask.Array of shape [n_samples, n_classes] containing encoded</span></dt><dd><p>labels</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float This value is currently ignored</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>arr</strong><span class="classifier">Dask.Array backed by CuPy arrays containing original labels</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.preprocessing.LabelBinarizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/label.py#L174"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.LabelBinarizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform and return encoded labels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask.Array of shape [n_samples,] or [n_samples, n_classes]</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>arr</strong><span class="classifier">Dask.Array backed by CuPy arrays containing encoded labels</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.preprocessing.OneHotEncoder">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.preprocessing.</code><code class="sig-name descname">OneHotEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/encoders.py#L28"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.OneHotEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode categorical features as a one-hot numeric array.
The input to this transformer should be a dask_cuDF.DataFrame or cupy
dask.Array, denoting the values taken on by categorical features.
The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’)
encoding scheme. This creates a binary column for each category and
returns a sparse matrix or dense array (depending on the <code class="docutils literal notranslate"><span class="pre">sparse</span></code>
parameter).
By default, the encoder derives the categories based on the unique values
in each feature. Alternatively, you can also specify the <cite>categories</cite>
manually.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>categories</strong><span class="classifier">‘auto’, cupy.ndarray or cudf.DataFrame, default=’auto’</span></dt><dd><p>Categories (unique values) per feature. All categories are expected to
fit on one GPU.</p>
<ul class="simple">
<li><p>‘auto’ : Determine categories automatically from the training data.</p></li>
<li><p>DataFrame/ndarray : <code class="docutils literal notranslate"><span class="pre">categories[col]</span></code> holds the categories expected
in the feature col.</p></li>
</ul>
</dd>
<dt><strong>drop</strong><span class="classifier">‘first’, None or a dict, default=None</span></dt><dd><p>Specifies a methodology to use to drop one of the categories per
feature. This is useful in situations where perfectly collinear
features cause problems, such as when feeding the resulting data
into a neural network or an unregularized regression.</p>
<ul class="simple">
<li><p>None : retain all features (the default).</p></li>
<li><p>‘first’ : drop the first category in each feature. If only one
category is present, the feature will be dropped entirely.</p></li>
<li><p>Dict : <code class="docutils literal notranslate"><span class="pre">drop[col]</span></code> is the category in feature col that
should be dropped.</p></li>
</ul>
</dd>
<dt><strong>sparse</strong><span class="classifier">bool, default=False</span></dt><dd><p>This feature was deactivated and will give an exception when True.
The reason is because sparse matrix are not fully supported by cupy
yet, causing incorrect values when computing one hot encodings.
See <a class="reference external" href="https://github.com/cupy/cupy/issues/3223">https://github.com/cupy/cupy/issues/3223</a></p>
</dd>
<dt><strong>dtype</strong><span class="classifier">number type, default=np.float</span></dt><dd><p>Desired datatype of transform’s output.</p>
</dd>
<dt><strong>handle_unknown</strong><span class="classifier">{‘error’, ‘ignore’}, default=’error’</span></dt><dd><p>Whether to raise an error or ignore if an unknown categorical feature
is present during transform (default is to raise). When this parameter
is set to ‘ignore’ and an unknown category is encountered during
transform, the resulting one-hot encoded columns for this feature
will be all zeros. In the inverse transform, an unknown category
will be denoted as None.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.OneHotEncoder.fit" title="cuml.dask.preprocessing.OneHotEncoder.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X)</p></td>
<td><p>Fit a multi-node multi-gpu OneHotEncoder to X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.OneHotEncoder.fit_transform" title="cuml.dask.preprocessing.OneHotEncoder.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(X[, delayed])</p></td>
<td><p>Fit OneHotEncoder to X, then transform X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.OneHotEncoder.inverse_transform" title="cuml.dask.preprocessing.OneHotEncoder.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(X[, delayed])</p></td>
<td><p>Convert the data back to the original representation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.preprocessing.OneHotEncoder.transform" title="cuml.dask.preprocessing.OneHotEncoder.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X[, delayed])</p></td>
<td><p>Transform X using one-hot encoding.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.preprocessing.OneHotEncoder.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/encoders.py#L88"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.OneHotEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a multi-node multi-gpu OneHotEncoder to X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>The data to determine the categories of each feature.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.preprocessing.OneHotEncoder.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/encoders.py#L112"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.OneHotEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit OneHotEncoder to X, then transform X.
Equivalent to fit(X).transform(X).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>The data to encode.</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to execute as a delayed task or eager.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>out</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Distributed object containing the transformed data</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.preprocessing.OneHotEncoder.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/encoders.py#L152"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.OneHotEncoder.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the data back to the original representation.
In case unknown categories are encountered (all zeros in the
one-hot encoding), <code class="docutils literal notranslate"><span class="pre">None</span></code> is used to represent this category.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">CuPy backed Dask Array, shape [n_samples, n_encoded_features]</span></dt><dd><p>The transformed data.</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to execute as a delayed task or eager.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_tr</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Distributed object containing the inverse transformed array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.preprocessing.OneHotEncoder.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/preprocessing/encoders.py#L131"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.preprocessing.OneHotEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X using one-hot encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>The data to encode.</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to execute as a delayed task or eager.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>out</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Distributed object containing the transformed input.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="feature-extraction-single-gpu">
<h3>Feature Extraction (Single-GPU)<a class="headerlink" href="#feature-extraction-single-gpu" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="py class">
<dt id="cuml.feature_extraction.text.CountVectorizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.feature_extraction.text.</code><code class="sig-name descname">CountVectorizer</code><span class="sig-paren">(</span><em class="sig-param">input=None</em>, <em class="sig-param">encoding=None</em>, <em class="sig-param">decode_error=None</em>, <em class="sig-param">strip_accents=None</em>, <em class="sig-param">lowercase=True</em>, <em class="sig-param">preprocessor=None</em>, <em class="sig-param">tokenizer=None</em>, <em class="sig-param">stop_words=None</em>, <em class="sig-param">token_pattern=None</em>, <em class="sig-param">ngram_range=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">analyzer='word'</em>, <em class="sig-param">max_df=1.0</em>, <em class="sig-param">min_df=1</em>, <em class="sig-param">max_features=None</em>, <em class="sig-param">vocabulary=None</em>, <em class="sig-param">binary=False</em>, <em class="sig-param">dtype=&lt;class 'numpy.float32'&gt;</em>, <em class="sig-param">delimiter=' '</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L313"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.CountVectorizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of text documents to a matrix of token counts</p>
<p>If you do not provide an a-priori dictionary then the number of features
will be equal to the vocabulary size found by analyzing the data.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>lowercase</strong><span class="classifier">boolean, True by default</span></dt><dd><p>Convert all characters to lowercase before tokenizing.</p>
</dd>
<dt><strong>preprocessor</strong><span class="classifier">callable or None (default)</span></dt><dd><p>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</p>
</dd>
<dt><strong>stop_words</strong><span class="classifier">string {‘english’}, list, or None (default)</span></dt><dd><p>If ‘english’, a built-in stop word list for English is used.
If a list, that list is assumed to contain stop words, all of which
will be removed from the input documents.
If None, no stop words will be used. max_df can be set to a value
to automatically detect and filter stop words based on intra corpus
document frequency of terms.</p>
</dd>
<dt><strong>ngram_range</strong><span class="classifier">tuple (min_n, max_n), default=(1, 1)</span></dt><dd><p>The lower and upper boundary of the range of n-values for different
word n-grams or char n-grams to be extracted. All values of n such
such that min_n &lt;= n &lt;= max_n will be used. For example an
<code class="docutils literal notranslate"><span class="pre">ngram_range</span></code> of <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">1)</span></code> means only unigrams, <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">2)</span></code> means
unigrams and bigrams, and <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">2)</span></code> means only bigrams.</p>
</dd>
<dt><strong>analyzer</strong><span class="classifier">string, {‘word’, ‘char’, ‘char_wb’}</span></dt><dd><p>Whether the feature should be made of word n-gram or character
n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.</p>
</dd>
<dt><strong>max_df</strong><span class="classifier">float in range [0.0, 1.0] or int, default=1.0</span></dt><dd><p>When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt><strong>min_df</strong><span class="classifier">float in range [0.0, 1.0] or int, default=1</span></dt><dd><p>When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int or None, default=None</span></dt><dd><p>If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.
This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt><strong>vocabulary</strong><span class="classifier">cudf.Series, optional</span></dt><dd><p>If not given, a vocabulary is determined from the input documents.</p>
</dd>
<dt><strong>binary</strong><span class="classifier">boolean, default=False</span></dt><dd><p>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">type, optional</span></dt><dd><p>Type of the matrix returned by fit_transform() or transform().</p>
</dd>
<dt><strong>delimiter</strong><span class="classifier">str, whitespace by default</span></dt><dd><p>String used as a replacement for stop words if stop_words is not None.
Typically the delimiting character between words is a good choice.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl>
<dt><strong>vocabulary_</strong><span class="classifier">cudf.Series[str]</span></dt><dd><p>Array mapping from feature integer indices to feature name.</p>
</dd>
<dt><strong>stop_words_</strong><span class="classifier">cudf.Series[str]</span></dt><dd><dl class="simple">
<dt>Terms that were ignored because they either:</dt><dd><ul class="simple">
<li><p>occurred in too many documents (<cite>max_df</cite>)</p></li>
<li><p>occurred in too few documents (<cite>min_df</cite>)</p></li>
<li><p>were cut off by feature selection (<cite>max_features</cite>).</p></li>
</ul>
</dd>
</dl>
<p>This is only available if no vocabulary was given.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer.fit" title="cuml.feature_extraction.text.CountVectorizer.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(raw_documents)</p></td>
<td><p>Build a vocabulary of all tokens in the raw documents.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer.fit_transform" title="cuml.feature_extraction.text.CountVectorizer.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(raw_documents)</p></td>
<td><p>Build the vocabulary and return document-term matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer.get_feature_names" title="cuml.feature_extraction.text.CountVectorizer.get_feature_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_feature_names</span></code></a>()</p></td>
<td><p>Array mapping from feature integer indices to feature name.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer.inverse_transform" title="cuml.feature_extraction.text.CountVectorizer.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(X)</p></td>
<td><p>Return terms per document with nonzero entries in X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer.transform" title="cuml.feature_extraction.text.CountVectorizer.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(raw_documents)</p></td>
<td><p>Transform documents to document-term matrix.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.feature_extraction.text.CountVectorizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L508"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.CountVectorizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a vocabulary of all tokens in the raw documents.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.CountVectorizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L526"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.CountVectorizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the vocabulary and return document-term matrix.</p>
<p>Equivalent to <code class="docutils literal notranslate"><span class="pre">self.fit(X).transform(X)</span></code> but preprocess <cite>X</cite> only
once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cupy csr array of shape (n_samples, n_features)</span></dt><dd><p>Document-term matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.CountVectorizer.get_feature_names">
<code class="sig-name descname">get_feature_names</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L637"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.CountVectorizer.get_feature_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Array mapping from feature integer indices to feature name.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>feature_names</strong><span class="classifier">Series</span></dt><dd><p>A list of feature names.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.CountVectorizer.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L620"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.CountVectorizer.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Return terms per document with nonzero entries in X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Document-term matrix.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_inv</strong><span class="classifier">list of cudf.Series of shape (n_samples,)</span></dt><dd><p>List of Series of terms.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.CountVectorizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L584"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.CountVectorizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform documents to document-term matrix.</p>
<p>Extract token counts out of raw text documents using the vocabulary
fitted with fit or the one provided to the constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cupy csr array of shape (n_samples, n_features)</span></dt><dd><p>Document-term matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.feature_extraction.text.HashingVectorizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.feature_extraction.text.</code><code class="sig-name descname">HashingVectorizer</code><span class="sig-paren">(</span><em class="sig-param">input=None</em>, <em class="sig-param">encoding=None</em>, <em class="sig-param">decode_error=None</em>, <em class="sig-param">strip_accents=None</em>, <em class="sig-param">lowercase=True</em>, <em class="sig-param">preprocessor=None</em>, <em class="sig-param">tokenizer=None</em>, <em class="sig-param">stop_words=None</em>, <em class="sig-param">token_pattern=None</em>, <em class="sig-param">ngram_range=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">analyzer='word'</em>, <em class="sig-param">n_features=1048576</em>, <em class="sig-param">binary=False</em>, <em class="sig-param">norm='l2'</em>, <em class="sig-param">alternate_sign=True</em>, <em class="sig-param">dtype=&lt;class 'numpy.float32'&gt;</em>, <em class="sig-param">delimiter=' '</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L651"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.HashingVectorizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of text documents to a matrix of token occurrences</p>
<p>It turns a collection of text documents into a cupyx.scipy.sparse matrix
holding token occurrence counts (or binary occurrence information),
possibly normalized as token frequencies if norm=’l1’ or projected on the
euclidean unit sphere if norm=’l2’.</p>
<p>This text vectorizer implementation uses the hashing trick to find the
token string name to feature integer index mapping.</p>
<p>This strategy has several advantages:</p>
<blockquote>
<div><ul class="simple">
<li><p>it is very low memory scalable to large datasets as there is no need to
store a vocabulary dictionary in memory which is even more important
as GPU’s that are often memory constrained</p></li>
<li><p>it is fast to pickle and un-pickle as it holds no state besides the
constructor parameters</p></li>
<li><p>it can be used in a streaming (partial fit) or parallel pipeline as
there is no state computed during fit.</p></li>
</ul>
</div></blockquote>
<p>There are also a couple of cons (vs using a CountVectorizer with an
in-memory vocabulary):</p>
<blockquote>
<div><ul class="simple">
<li><p>there is no way to compute the inverse transform (from feature indices
to string feature names) which can be a problem when trying to
introspect which features are most important to a model.</p></li>
<li><p>there can be collisions: distinct tokens can be mapped to the same
feature index. However in practice this is rarely an issue if n_features
is large enough (e.g. 2 ** 18 for text classification problems).</p></li>
<li><p>no IDF weighting as this would render the transformer stateful.</p></li>
</ul>
</div></blockquote>
<p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>lowercase</strong><span class="classifier">bool, default=True</span></dt><dd><p>Convert all characters to lowercase before tokenizing.</p>
</dd>
<dt><strong>preprocessor</strong><span class="classifier">callable or None (default)</span></dt><dd><p>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</p>
</dd>
<dt><strong>stop_words</strong><span class="classifier">string {‘english’}, list, default=None</span></dt><dd><p>If ‘english’, a built-in stop word list for English is used.
There are several known issues with ‘english’ and you should
consider an alternative.
If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal notranslate"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
</dd>
<dt><strong>ngram_range</strong><span class="classifier">tuple (min_n, max_n), default=(1, 1)</span></dt><dd><p>The lower and upper boundary of the range of n-values for different
word n-grams or char n-grams to be extracted. All values of n such
such that min_n &lt;= n &lt;= max_n will be used. For example an
<code class="docutils literal notranslate"><span class="pre">ngram_range</span></code> of <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">1)</span></code> means only unigrams, <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">2)</span></code> means
unigrams and bigrams, and <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">2)</span></code> means only bigrams.</p>
</dd>
<dt><strong>analyzer</strong><span class="classifier">string, {‘word’, ‘char’, ‘char_wb’}</span></dt><dd><p>Whether the feature should be made of word n-gram or character
n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, default=(2 ** 20)</span></dt><dd><p>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</p>
</dd>
<dt><strong>binary</strong><span class="classifier">bool, default=False.</span></dt><dd><p>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</p>
</dd>
<dt><strong>norm</strong><span class="classifier">{‘l1’, ‘l2’}, default=’l2’</span></dt><dd><p>Norm used to normalize term vectors. None for no normalization.</p>
</dd>
<dt><strong>alternate_sign</strong><span class="classifier">bool, default=True</span></dt><dd><p>When True, an alternating sign is added to the features as to
approximately conserve the inner product in the hashed space even for
small n_features. This approach is similar to sparse random projection.</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">type, optional</span></dt><dd><p>Type of the matrix returned by fit_transform() or transform().</p>
</dd>
<dt><strong>delimiter</strong><span class="classifier">str, whitespace by default</span></dt><dd><p>String used as a replacement for stop words if <cite>stop_words</cite> is not
None. Typically the delimiting character between words is a good
choice.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer" title="cuml.feature_extraction.text.CountVectorizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>, <a class="reference internal" href="#cuml.feature_extraction.text.TfidfVectorizer" title="cuml.feature_extraction.text.TfidfVectorizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">'This is the first document.'</span><span class="p">,</span>
    <span class="s1">'This document is the second document.'</span><span class="p">,</span>
    <span class="s1">'And this is the third one.'</span><span class="p">,</span>
    <span class="s1">'Is this the first document?'</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.HashingVectorizer.fit" title="cuml.feature_extraction.text.HashingVectorizer.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X[, y])</p></td>
<td><p>This method only checks the input type and the model parameter.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.HashingVectorizer.fit_transform" title="cuml.feature_extraction.text.HashingVectorizer.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(X[, y])</p></td>
<td><p>Transform a sequence of documents to a document-term matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.HashingVectorizer.partial_fit" title="cuml.feature_extraction.text.HashingVectorizer.partial_fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">partial_fit</span></code></a>(X[, y])</p></td>
<td><p>Does nothing: This transformer is stateless This method is just there to mark the fact that this transformer can work in a streaming setup.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.HashingVectorizer.transform" title="cuml.feature_extraction.text.HashingVectorizer.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(raw_documents)</p></td>
<td><p>Transform documents to document-term matrix.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.feature_extraction.text.HashingVectorizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L820"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.HashingVectorizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method only checks the input type and the model parameter.
It does not do anything meaningful as this transformer is stateless</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.HashingVectorizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L866"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.HashingVectorizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a sequence of documents to a document-term matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">iterable over raw text documents, length = n_samples</span></dt><dd><p>Samples. Each sample must be a text document (either bytes or
unicode strings, file name or file object depending on the
constructor argument) which will be tokenized and hashed.</p>
</dd>
<dt><strong>y</strong><span class="classifier">any</span></dt><dd><p>Ignored. This parameter exists only for compatibility with
sklearn.pipeline.Pipeline.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">sparse CuPy CSR matrix of shape (n_samples, n_features)</span></dt><dd><p>Document-term matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.HashingVectorizer.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L808"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.HashingVectorizer.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Does nothing: This transformer is stateless
This method is just there to mark the fact that this transformer
can work in a streaming setup.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cudf.Series(A Series of string documents).</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.HashingVectorizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_vectorizers.py#L887"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.HashingVectorizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform documents to document-term matrix.</p>
<p>Extract token counts out of raw text documents using the vocabulary
fitted with fit or the one provided to the constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">sparse CuPy CSR matrix of shape (n_samples, n_features)</span></dt><dd><p>Document-term matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.feature_extraction.text.TfidfVectorizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.feature_extraction.text.</code><code class="sig-name descname">TfidfVectorizer</code><span class="sig-paren">(</span><em class="sig-param">input=None</em>, <em class="sig-param">encoding=None</em>, <em class="sig-param">decode_error=None</em>, <em class="sig-param">strip_accents=None</em>, <em class="sig-param">lowercase=True</em>, <em class="sig-param">preprocessor=None</em>, <em class="sig-param">tokenizer=None</em>, <em class="sig-param">stop_words=None</em>, <em class="sig-param">token_pattern=None</em>, <em class="sig-param">ngram_range=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">analyzer='word'</em>, <em class="sig-param">max_df=1.0</em>, <em class="sig-param">min_df=1</em>, <em class="sig-param">max_features=None</em>, <em class="sig-param">vocabulary=None</em>, <em class="sig-param">binary=False</em>, <em class="sig-param">dtype=&lt;class 'numpy.float32'&gt;</em>, <em class="sig-param">delimiter=' '</em>, <em class="sig-param">norm='l2'</em>, <em class="sig-param">use_idf=True</em>, <em class="sig-param">smooth_idf=True</em>, <em class="sig-param">sublinear_tf=False</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_tfidf_vectorizer.py#L35"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.TfidfVectorizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of raw documents to a matrix of TF-IDF features.</p>
<p>Equivalent to <a class="reference internal" href="#cuml.feature_extraction.text.CountVectorizer" title="cuml.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> followed by
<code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfTransformer</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>lowercase</strong><span class="classifier">boolean, True by default</span></dt><dd><p>Convert all characters to lowercase before tokenizing.</p>
</dd>
<dt><strong>preprocessor</strong><span class="classifier">callable or None (default)</span></dt><dd><p>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</p>
</dd>
<dt><strong>stop_words</strong><span class="classifier">string {‘english’}, list, or None (default)</span></dt><dd><p>If ‘english’, a built-in stop word list for English is used.
If a list, that list is assumed to contain stop words, all of which
will be removed from the input documents.
If None, no stop words will be used. max_df can be set to a value
to automatically detect and filter stop words based on intra corpus
document frequency of terms.</p>
</dd>
<dt><strong>ngram_range</strong><span class="classifier">tuple (min_n, max_n), default=(1, 1)</span></dt><dd><p>The lower and upper boundary of the range of n-values for different
word n-grams or char n-grams to be extracted. All values of n such
such that min_n &lt;= n &lt;= max_n will be used. For example an
<code class="docutils literal notranslate"><span class="pre">ngram_range</span></code> of <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">1)</span></code> means only unigrams, <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">2)</span></code> means
unigrams and bigrams, and <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">2)</span></code> means only bigrams.</p>
</dd>
<dt><strong>analyzer</strong><span class="classifier">string, {‘word’, ‘char’, ‘char_wb’}</span></dt><dd><p>Whether the feature should be made of word n-gram or character
n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries; n-grams at the edges of words are padded with space.</p>
</dd>
<dt><strong>max_df</strong><span class="classifier">float in range [0.0, 1.0] or int, default=1.0</span></dt><dd><p>When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt><strong>min_df</strong><span class="classifier">float in range [0.0, 1.0] or int, default=1</span></dt><dd><p>When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int or None, default=None</span></dt><dd><p>If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.
This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt><strong>vocabulary</strong><span class="classifier">cudf.Series, optional</span></dt><dd><p>If not given, a vocabulary is determined from the input documents.</p>
</dd>
<dt><strong>binary</strong><span class="classifier">boolean, default=False</span></dt><dd><p>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">type, optional</span></dt><dd><p>Type of the matrix returned by fit_transform() or transform().</p>
</dd>
<dt><strong>delimiter</strong><span class="classifier">str, whitespace by default</span></dt><dd><p>String used as a replacement for stop words if stop_words is not None.
Typically the delimiting character between words is a good choice.</p>
</dd>
<dt><strong>norm</strong><span class="classifier">{‘l1’, ‘l2’}, default=’l2’</span></dt><dd><dl class="simple">
<dt>Each output row will have unit norm, either:</dt><dd><ul class="simple">
<li><p>‘l2’: Sum of squares of vector elements is 1. The cosine similarity
between two vectors is their dot product when l2 norm has been
applied.</p></li>
<li><p>‘l1’: Sum of absolute values of vector elements is 1.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt><strong>use_idf</strong><span class="classifier">bool, default=True</span></dt><dd><p>Enable inverse-document-frequency reweighting.</p>
</dd>
<dt><strong>smooth_idf</strong><span class="classifier">bool, default=True</span></dt><dd><p>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</p>
</dd>
<dt><strong>sublinear_tf</strong><span class="classifier">bool, default=False</span></dt><dd><p>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The <code class="docutils literal notranslate"><span class="pre">stop_words_</span></code> attribute can get large and increase the model size
when pickling. This attribute is provided only for introspection and can
be safely removed using delattr or set to None before pickling.</p>
<p>This class is largely based on scikit-learn 0.23.1’s TfIdfVectorizer code,
which is provided under the BSD-3 license.</p>
<dl class="field-list">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl>
<dt><strong>idf_</strong><span class="classifier">array of shape (n_features)</span></dt><dd><p>The inverse document frequency (IDF) vector; only defined
if  <cite>use_idf</cite> is True.</p>
</dd>
<dt><strong>vocabulary_</strong><span class="classifier">cudf.Series[str]</span></dt><dd><p>Array mapping from feature integer indices to feature name.</p>
</dd>
<dt><strong>stop_words_</strong><span class="classifier">cudf.Series[str]</span></dt><dd><dl class="simple">
<dt>Terms that were ignored because they either:</dt><dd><ul class="simple">
<li><p>occurred in too many documents (<cite>max_df</cite>)</p></li>
<li><p>occurred in too few documents (<cite>min_df</cite>)</p></li>
<li><p>were cut off by feature selection (<cite>max_features</cite>).</p></li>
</ul>
</dd>
</dl>
<p>This is only available if no vocabulary was given.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.TfidfVectorizer.fit" title="cuml.feature_extraction.text.TfidfVectorizer.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(raw_documents)</p></td>
<td><p>Learn vocabulary and idf from training set.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.TfidfVectorizer.fit_transform" title="cuml.feature_extraction.text.TfidfVectorizer.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(raw_documents)</p></td>
<td><p>Learn vocabulary and idf, return document-term matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.feature_extraction.text.TfidfVectorizer.transform" title="cuml.feature_extraction.text.TfidfVectorizer.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(raw_documents)</p></td>
<td><p>Transform documents to document-term matrix.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.feature_extraction.text.TfidfVectorizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_tfidf_vectorizer.py#L204"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.TfidfVectorizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn vocabulary and idf from training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Fitted vectorizer.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.TfidfVectorizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_tfidf_vectorizer.py#L221"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.TfidfVectorizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn vocabulary and idf, return document-term matrix.
This is equivalent to fit followed by transform, but more efficiently
implemented.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cupy csr array of shape (n_samples, n_features)</span></dt><dd><p>Tf-idf-weighted document-term matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.feature_extraction.text.TfidfVectorizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_documents</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/feature_extraction/_tfidf_vectorizer.py#L242"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.feature_extraction.text.TfidfVectorizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform documents to document-term matrix.
Uses the vocabulary and document frequencies (df) learned by fit (or
fit_transform).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>raw_documents</strong><span class="classifier">cudf.Series</span></dt><dd><p>A Series of string documents</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cupy csr array of shape (n_samples, n_features)</span></dt><dd><p>Tf-idf-weighted document-term matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="dataset-generation-single-gpu">
<h3>Dataset Generation (Single-GPU)<a class="headerlink" href="#dataset-generation-single-gpu" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><dl class="glossary simple">
<dt id="term-random_state">random_state</dt><dd><p>Determines random number generation for dataset creation. Pass an int
for reproducible output across multiple function calls.</p>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.datasets.make_blobs">
<code class="sig-prename descclassname">datasets.</code><code class="sig-name descname">make_blobs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">centers</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cluster_std</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">center_box</span><span class="o">=</span><span class="default_value">- 10.0, 10.0</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_centers</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/datasets/blobs.py#L68"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.datasets.make_blobs" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate isotropic Gaussian blobs for clustering.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int or array-like, optional (default=100)</span></dt><dd><p>If int, it is the total number of points equally divided among
clusters.
If array-like, each element of the sequence indicates
the number of samples per cluster.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of features for each sample.</p>
</dd>
<dt><strong>centers</strong><span class="classifier">int or array of shape [n_centers, n_features], optional</span></dt><dd><p>(default=None)
The number of centers to generate, or the fixed center locations.
If n_samples is an int and centers is None, 3 centers are generated.
If n_samples is array-like, centers must be
either None or an array of length equal to the length of n_samples.</p>
</dd>
<dt><strong>cluster_std</strong><span class="classifier">float or sequence of floats, optional (default=1.0)</span></dt><dd><p>The standard deviation of the clusters.</p>
</dd>
<dt><strong>center_box</strong><span class="classifier">pair of floats (min, max), optional (default=(-10.0, 10.0))</span></dt><dd><p>The bounding box for each cluster center when centers are
generated at random.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Shuffle the samples.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance, default=None</span></dt><dd><p>Determines random number generation for dataset creation. Pass an int
for reproducible output across multiple function calls.</p>
</dd>
<dt><strong>return_centers</strong><span class="classifier">bool, optional (default=False)</span></dt><dd><p>If True, then return the centers of each cluster</p>
</dd>
<dt><strong>order: str, optional (default=’F’)</strong></dt><dd><p>The order of the generated samples</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">str, optional (default=’float32’)</span></dt><dd><p>Dtype of the generated samples</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">device array of shape [n_samples, n_features]</span></dt><dd><p>The generated samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">device array of shape [n_samples]</span></dt><dd><p>The integer labels for cluster membership of each sample.</p>
</dd>
<dt><strong>centers</strong><span class="classifier">device array, shape [n_centers, n_features]</span></dt><dd><p>The centers of each cluster. Only returned if
<code class="docutils literal notranslate"><span class="pre">return_centers=True</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.datasets.make_classification" title="cuml.datasets.make_classification"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_classification</span></code></a></dt><dd><p>a more intricate variant</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">centers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">array([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.datasets.make_classification">
<code class="sig-prename descclassname">datasets.</code><code class="sig-name descname">make_classification</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">20</span></em>, <em class="sig-param"><span class="n">n_informative</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_redundant</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_repeated</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">n_classes</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_clusters_per_class</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">flip_y</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">class_sep</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">hypercube</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">shift</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em>, <em class="sig-param"><span class="n">_centroids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">_informative_covariance</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">_redundant_covariance</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">_repeated_indices</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/datasets/classification.py#L45"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.datasets.make_classification" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a random n-class classification problem.
This initially creates clusters of points normally distributed (std=1)
about vertices of an <code class="docutils literal notranslate"><span class="pre">n_informative</span></code>-dimensional hypercube with sides of
length <code class="docutils literal notranslate"><span class="pre">2*class_sep</span></code> and assigns an equal number of clusters to each
class. It introduces interdependence between these features and adds
various types of further noise to the data.
Without shuffling, <code class="docutils literal notranslate"><span class="pre">X</span></code> horizontally stacks features in the following
order: the primary <code class="docutils literal notranslate"><span class="pre">n_informative</span></code> features, followed by <code class="docutils literal notranslate"><span class="pre">n_redundant</span></code>
linear combinations of the informative features, followed by <code class="docutils literal notranslate"><span class="pre">n_repeated</span></code>
duplicates, drawn randomly with replacement from the informative and
redundant features. The remaining features are filled with random noise.
Thus, without shuffling, all useful features are contained in the columns
<code class="docutils literal notranslate"><span class="pre">X[:,</span> <span class="pre">:n_informative</span> <span class="pre">+</span> <span class="pre">n_redundant</span> <span class="pre">+</span> <span class="pre">n_repeated]</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=20)</span></dt><dd><p>The total number of features. These comprise <code class="docutils literal notranslate"><span class="pre">n_informative</span></code>
informative features, <code class="docutils literal notranslate"><span class="pre">n_redundant</span></code> redundant features,
<code class="docutils literal notranslate"><span class="pre">n_repeated</span></code> duplicated features and
<code class="docutils literal notranslate"><span class="pre">n_features-n_informative-n_redundant-n_repeated</span></code> useless features
drawn at random.</p>
</dd>
<dt><strong>n_informative</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of informative features. Each class is composed of a number
of gaussian clusters each located around the vertices of a hypercube
in a subspace of dimension <code class="docutils literal notranslate"><span class="pre">n_informative</span></code>. For each cluster,
informative features are drawn independently from  N(0, 1) and then
randomly linearly combined within each cluster in order to add
covariance. The clusters are then placed on the vertices of the
hypercube.</p>
</dd>
<dt><strong>n_redundant</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of redundant features. These features are generated as
random linear combinations of the informative features.</p>
</dd>
<dt><strong>n_repeated</strong><span class="classifier">int, optional (default=0)</span></dt><dd><p>The number of duplicated features, drawn randomly from the informative
and the redundant features.</p>
</dd>
<dt><strong>n_classes</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of classes (or labels) of the classification problem.</p>
</dd>
<dt><strong>n_clusters_per_class</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of clusters per class.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">array-like of shape (n_classes,) or (n_classes - 1,),              (default=None)</span></dt><dd><p>The proportions of samples assigned to each class. If None, then
classes are balanced. Note that if <code class="docutils literal notranslate"><span class="pre">len(weights)</span> <span class="pre">==</span> <span class="pre">n_classes</span> <span class="pre">-</span> <span class="pre">1</span></code>,
then the last class weight is automatically inferred.
More than <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> samples may be returned if the sum of
<code class="docutils literal notranslate"><span class="pre">weights</span></code> exceeds 1.</p>
</dd>
<dt><strong>flip_y</strong><span class="classifier">float, optional (default=0.01)</span></dt><dd><p>The fraction of samples whose class is assigned randomly. Larger
values introduce noise in the labels and make the classification
task harder.</p>
</dd>
<dt><strong>class_sep</strong><span class="classifier">float, optional (default=1.0)</span></dt><dd><p>The factor multiplying the hypercube size.  Larger values spread
out the clusters/classes and make the classification task easier.</p>
</dd>
<dt><strong>hypercube</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>If True, the clusters are put on the vertices of a hypercube. If
False, the clusters are put on the vertices of a random polytope.</p>
</dd>
<dt><strong>shift</strong><span class="classifier">float, array of shape [n_features] or None, optional (default=0.0)</span></dt><dd><p>Shift features by the specified value. If None, then features
are shifted by a random value drawn in [-class_sep, class_sep].</p>
</dd>
<dt><strong>scale</strong><span class="classifier">float, array of shape [n_features] or None, optional (default=1.0)</span></dt><dd><p>Multiply features by the specified value. If None, then features
are scaled by a random value drawn in [1, 100]. Note that scaling
happens after shifting.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Shuffle the samples and the features.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None (default)</span></dt><dd><p>Determines random number generation for dataset creation. Pass an int
for reproducible output across multiple function calls.
See <a class="reference internal" href="#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p>
</dd>
<dt><strong>order: str, optional (default=’F’)</strong></dt><dd><p>The order of the generated samples</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">str, optional (default=’float32’)</span></dt><dd><p>Dtype of the generated samples</p>
</dd>
<dt><strong>_centroids: array of centroids of shape (n_clusters, n_informative)</strong></dt><dd></dd>
<dt><strong>_informative_covariance: array for covariance between informative features</strong></dt><dd><p>of shape (n_clusters, n_informative, n_informative)</p>
</dd>
<dt><strong>_redundant_covariance: array for covariance between redundant features</strong></dt><dd><p>of shape (n_informative, n_redundant)</p>
</dd>
<dt><strong>_repeated_indices: array of indices for the repeated features</strong></dt><dd><p>of shape (n_repeated, )</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">device array of shape [n_samples, n_features]</span></dt><dd><p>The generated samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">device array of shape [n_samples]</span></dt><dd><p>The integer labels for class membership of each sample.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The algorithm is adapted from Guyon [1] and was designed to generate
the “Madelon” dataset. How we optimized for GPUs:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Firstly, we generate X from a standard univariate instead of zeros.
This saves memory as we don’t need to generate univariates each
time for each feature class (informative, repeated, etc.) while
also providing the added speedup of generating a big matrix
on GPU</p></li>
<li><p>We generate <cite>order=F</cite> construction. We exploit the
fact that X is a generated from a univariate normal, and
covariance is introduced with matrix multiplications. Which means,
we can generate X as a 1D array and just reshape it to the
desired order, which only updates the metadata and eliminates
copies</p></li>
<li><p>Lastly, we also shuffle by construction. Centroid indices are
permuted for each sample, and then we construct the data for
each centroid. This shuffle works for both <cite>order=C</cite> and
<cite>order=F</cite> and eliminates any need for secondary copies</p></li>
</ol>
</div></blockquote>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="raf8a76d479e9-1"><span class="brackets">1</span></dt>
<dd><p>I. Guyon, “Design of experiments for the NIPS 2003 variable
selection benchmark”, 2003.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.datasets.classification</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"y:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">2.3249989</span>  <span class="o">-</span><span class="mf">0.8679415</span>  <span class="o">-</span><span class="mf">1.1511791</span>   <span class="mf">1.3525577</span> <span class="p">]</span>
<span class="p">[</span> <span class="mf">2.2933831</span>   <span class="mf">1.3743551</span>   <span class="mf">0.63128835</span> <span class="o">-</span><span class="mf">0.84648645</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.6361488</span>  <span class="o">-</span><span class="mf">1.3233329</span>   <span class="mf">0.807027</span>   <span class="o">-</span><span class="mf">0.894092</span>  <span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.0093077</span>  <span class="o">-</span><span class="mf">0.9990691</span>  <span class="o">-</span><span class="mf">0.00808992</span>  <span class="mf">0.00950443</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.99803793</span>  <span class="mf">2.068382</span>    <span class="mf">0.49570698</span> <span class="o">-</span><span class="mf">0.8462848</span> <span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.2750955</span>  <span class="o">-</span><span class="mf">0.9725835</span>  <span class="o">-</span><span class="mf">0.2390058</span>   <span class="mf">0.28081596</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.3635055</span>  <span class="o">-</span><span class="mf">0.9637669</span>  <span class="o">-</span><span class="mf">0.31582272</span>  <span class="mf">0.37106958</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.1893625</span>   <span class="mf">2.227583</span>    <span class="mf">0.48750278</span> <span class="o">-</span><span class="mf">0.8737561</span> <span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.05753583</span> <span class="o">-</span><span class="mf">1.0939395</span>   <span class="mf">0.8188342</span>  <span class="o">-</span><span class="mf">0.9620734</span> <span class="p">]</span>
<span class="p">[</span> <span class="mf">0.47910076</span>  <span class="mf">0.7648213</span>  <span class="o">-</span><span class="mf">0.17165393</span>  <span class="mf">0.26144698</span><span class="p">]]</span>

<span class="n">y</span><span class="p">:</span>
<span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.datasets.make_regression">
<code class="sig-prename descclassname">datasets.</code><code class="sig-name descname">make_regression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_samples</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_informative</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_targets</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">effective_rank</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tail_strength</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">noise</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">coef</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'single'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/datasets/regression.pyx#L74"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.datasets.make_regression" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a random regression problem.</p>
<p>See <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of features.</p>
</dd>
<dt><strong>n_informative</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of informative features, i.e., the number of features used
to build the linear model used to generate the output.</p>
</dd>
<dt><strong>n_targets</strong><span class="classifier">int, optional (default=1)</span></dt><dd><p>The number of regression targets, i.e., the dimension of the y output
vector associated with a sample. By default, the output is a scalar.</p>
</dd>
<dt><strong>bias</strong><span class="classifier">float, optional (default=0.0)</span></dt><dd><p>The bias term in the underlying linear model.</p>
</dd>
<dt><strong>effective_rank</strong><span class="classifier">int or None, optional (default=None)</span></dt><dd><dl class="simple">
<dt>if not None:</dt><dd><p>The approximate number of singular vectors required to explain most
of the input data by linear combinations. Using this kind of
singular spectrum in the input allows the generator to reproduce
the correlations often observed in practice.</p>
</dd>
<dt>if None:</dt><dd><p>The input set is well conditioned, centered and gaussian with
unit variance.</p>
</dd>
</dl>
</dd>
<dt><strong>tail_strength</strong><span class="classifier">float between 0.0 and 1.0, optional (default=0.5)</span></dt><dd><p>The relative importance of the fat noisy tail of the singular values
profile if <cite>effective_rank</cite> is not None.</p>
</dd>
<dt><strong>noise</strong><span class="classifier">float, optional (default=0.0)</span></dt><dd><p>The standard deviation of the gaussian noise applied to the output.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Shuffle the samples and the features.</p>
</dd>
<dt><strong>coef</strong><span class="classifier">boolean, optional (default=False)</span></dt><dd><p>If True, the coefficients of the underlying linear model are returned.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None (default)</span></dt><dd><p>Seed for the random number generator for dataset creation.</p>
</dd>
<dt><strong>dtype: string or numpy dtype (default: ‘single’)</strong></dt><dd><p>Type of the data. Possible values: float32, float64, ‘single’, ‘float’
or ‘double’.</p>
</dd>
<dt><strong>handle: cuml.Handle</strong></dt><dd><p>If it is None, a new one is created just for this function call</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>out</strong><span class="classifier">device array of shape [n_samples, n_features]</span></dt><dd><p>The input samples.</p>
</dd>
<dt><strong>values</strong><span class="classifier">device array of shape [n_samples, n_targets]</span></dt><dd><p>The output values.</p>
</dd>
<dt><strong>coef</strong><span class="classifier">device array of shape [n_features, n_targets], optional</span></dt><dd><p>The coefficient of the underlying linear model. It is returned only if
coef is True.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.datasets.regression</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Create regression problem</span>
<span class="n">data</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                               <span class="n">n_informative</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">bias</span><span class="o">=-</span><span class="mf">4.2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Perform a linear regression on this problem</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                      <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">"eig"</span><span class="p">)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.datasets.make_arima">
<code class="sig-prename descclassname">datasets.</code><code class="sig-name descname">make_arima</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">n_obs</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">1, 1, 1</span></em>, <em class="sig-param"><span class="n">seasonal_order</span><span class="o">=</span><span class="default_value">0, 0, 0, 0</span></em>, <em class="sig-param"><span class="n">intercept</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'double'</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">'cupy'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/datasets/arima.pyx#L67"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.datasets.make_arima" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a dataset of time series by simulating an ARIMA process
of a given order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch_size: int</strong></dt><dd><p>Number of time series to generate</p>
</dd>
<dt><strong>n_obs: int</strong></dt><dd><p>Number of observations per series</p>
</dd>
<dt><strong>order</strong><span class="classifier">Tuple[int, int, int]</span></dt><dd><p>Order (p, d, q) of the simulated ARIMA process</p>
</dd>
<dt><strong>seasonal_order: Tuple[int, int, int, int]</strong></dt><dd><p>Seasonal ARIMA order (P, D, Q, s) of the simulated ARIMA process</p>
</dd>
<dt><strong>intercept: bool or int</strong></dt><dd><p>Whether to include a constant trend mu in the simulated ARIMA process</p>
</dd>
<dt><strong>random_state: int, RandomState instance or None (default)</strong></dt><dd><p>Seed for the random number generator for dataset creation.</p>
</dd>
<dt><strong>dtype: string or numpy dtype (default: ‘single’)</strong></dt><dd><p>Type of the data. Possible values: float32, float64, ‘single’, ‘float’
or ‘double’</p>
</dd>
<dt><strong>output_type: {‘cudf’, ‘cupy’, ‘numpy’}</strong></dt><dd><p>Type of the returned dataset</p>
</dd>
<dt><strong>handle: cuml.Handle</strong></dt><dd><p>If it is None, a new one is created just for this function call</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>out: array-like, shape (n_obs, batch_size)</dt><dd><p>Array of the requested type containing the generated dataset</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.datasets</span> <span class="kn">import</span> <span class="n">make_arima</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">make_arima</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="dataset-generation-dask-based-multi-gpu">
<h3>Dataset Generation (Dask-based Multi-GPU)<a class="headerlink" href="#dataset-generation-dask-based-multi-gpu" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><span class="target" id="module-cuml.dask.datasets.blobs"></span><dl class="py function">
<dt id="cuml.dask.datasets.blobs.make_blobs">
<code class="sig-prename descclassname">cuml.dask.datasets.blobs.</code><code class="sig-name descname">make_blobs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_samples</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">centers</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cluster_std</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">n_parts</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">center_box</span><span class="o">=</span><span class="default_value">- 10, 10</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_centers</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em>, <em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/datasets/blobs.py#L45"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.datasets.blobs.make_blobs" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes labeled Dask-Cupy arrays containing blobs
for a randomly generated set of centroids.</p>
<p>This function calls <cite>make_blobs</cite> from <cite>cuml.datasets</cite> on each Dask worker
and aggregates them into a single Dask Dataframe.</p>
<p>For more information on Scikit-learn’s <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs:</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int</span></dt><dd><p>number of rows</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int</span></dt><dd><p>number of features</p>
</dd>
<dt><strong>centers</strong><span class="classifier">int or array of shape [n_centers, n_features],</span></dt><dd><p>optional (default=None) The number of centers to generate, or the fixed
center locations. If n_samples is an int and centers is None, 3 centers
are generated. If n_samples is array-like, centers must be either None
or an array of length equal to the length of n_samples.</p>
</dd>
<dt><strong>cluster_std</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>standard deviation of points around centroid</p>
</dd>
<dt><strong>n_parts</strong><span class="classifier">int (default = None)</span></dt><dd><p>number of partitions to generate (this can be greater
than the number of workers)</p>
</dd>
<dt><strong>center_box</strong><span class="classifier">tuple (int, int) (default = (-10, 10))</span></dt><dd><p>the bounding box which constrains all the centroids</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>sets random seed (or use None to reinitialize each time)</p>
</dd>
<dt><strong>return_centers</strong><span class="classifier">bool, optional (default=False)</span></dt><dd><p>If True, then return the centers of each cluster</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean (default = False)</span></dt><dd><p>Logging level.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">bool (default=False)</span></dt><dd><p>Shuffles the samples on each worker.</p>
</dd>
<dt><strong>order: str, optional (default=’F’)</strong></dt><dd><p>The order of the generated samples</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">str, optional (default=’float32’)</span></dt><dd><p>Dtype of the generated samples</p>
</dd>
<dt><strong>client</strong><span class="classifier">dask.distributed.Client (optional)</span></dt><dd><p>Dask client to use</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask.array backed by CuPy array of shape [n_samples, n_features]</span></dt><dd><p>The input samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">dask.array backed by CuPy array of shape [n_samples]</span></dt><dd><p>The output values.</p>
</dd>
<dt><strong>centers</strong><span class="classifier">dask.array backed by CuPy array of shape</span></dt><dd><p>[n_centers, n_features], optional
The centers of the underlying blobs. It is returned only if
return_centers is True.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.dask.datasets.classification"></span><dl class="py function">
<dt id="cuml.dask.datasets.classification.make_classification">
<code class="sig-prename descclassname">cuml.dask.datasets.classification.</code><code class="sig-name descname">make_classification</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_samples</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">20</span></em>, <em class="sig-param"><span class="n">n_informative</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_redundant</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_repeated</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">n_classes</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_clusters_per_class</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">flip_y</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">class_sep</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">hypercube</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">shift</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em>, <em class="sig-param"><span class="n">n_parts</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/datasets/classification.py#L37"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.datasets.classification.make_classification" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a random n-class classification problem.</p>
<p>This initially creates clusters of points normally distributed (std=1)
about vertices of an <cite>n_informative</cite>-dimensional hypercube with sides of
length <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">class_sep</span></code> and assigns an equal number of clusters to each
class. It introduces interdependence between these features and adds
various types of further noise to the data.</p>
<p>Without shuffling, <code class="docutils literal notranslate"><span class="pre">X</span></code> horizontally stacks features in the following
order: the primary <cite>n_informative</cite> features, followed by <cite>n_redundant</cite>
linear combinations of the informative features, followed by <cite>n_repeated</cite>
duplicates, drawn randomly with replacement from the informative and
redundant features. The remaining features are filled with random noise.
Thus, without shuffling, all useful features are contained in the columns
<code class="docutils literal notranslate"><span class="pre">X[:,</span> <span class="pre">:n_informative</span> <span class="pre">+</span> <span class="pre">n_redundant</span> <span class="pre">+</span> <span class="pre">n_repeated]</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=20)</span></dt><dd><p>The total number of features. These comprise <cite>n_informative</cite>
informative features, <cite>n_redundant</cite> redundant features,
<cite>n_repeated</cite> duplicated features and
<code class="docutils literal notranslate"><span class="pre">n_features-n_informative-n_redundant-n_repeated</span></code> useless features
drawn at random.</p>
</dd>
<dt><strong>n_informative</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of informative features. Each class is composed of a number
of gaussian clusters each located around the vertices of a hypercube
in a subspace of dimension <cite>n_informative</cite>. For each cluster,
informative features are drawn independently from  N(0, 1) and then
randomly linearly combined within each cluster in order to add
covariance. The clusters are then placed on the vertices of the
hypercube.</p>
</dd>
<dt><strong>n_redundant</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of redundant features. These features are generated as
random linear combinations of the informative features.</p>
</dd>
<dt><strong>n_repeated</strong><span class="classifier">int, optional (default=0)</span></dt><dd><p>The number of duplicated features, drawn randomly from the informative
and the redundant features.</p>
</dd>
<dt><strong>n_classes</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of classes (or labels) of the classification problem.</p>
</dd>
<dt><strong>n_clusters_per_class</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of clusters per class.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">array-like of shape <code class="docutils literal notranslate"><span class="pre">(n_classes,)</span></code> or <code class="docutils literal notranslate"><span class="pre">(n_classes</span> <span class="pre">-</span> <span class="pre">1,)</span></code>,         (default=None)</span></dt><dd><p>The proportions of samples assigned to each class. If None, then
classes are balanced. Note that if <code class="docutils literal notranslate"><span class="pre">len(weights)</span> <span class="pre">==</span> <span class="pre">n_classes</span> <span class="pre">-</span> <span class="pre">1</span></code>,
then the last class weight is automatically inferred.
More than <cite>n_samples</cite> samples may be returned if the sum of
<cite>weights</cite> exceeds 1.</p>
</dd>
<dt><strong>flip_y</strong><span class="classifier">float, optional (default=0.01)</span></dt><dd><p>The fraction of samples whose class is assigned randomly. Larger
values introduce noise in the labels and make the classification
task harder.</p>
</dd>
<dt><strong>class_sep</strong><span class="classifier">float, optional (default=1.0)</span></dt><dd><p>The factor multiplying the hypercube size.  Larger values spread
out the clusters/classes and make the classification task easier.</p>
</dd>
<dt><strong>hypercube</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>If True, the clusters are put on the vertices of a hypercube. If
False, the clusters are put on the vertices of a random polytope.</p>
</dd>
<dt><strong>shift</strong><span class="classifier">float, array of shape [n_features] or None, optional (default=0.0)</span></dt><dd><p>Shift features by the specified value. If None, then features
are shifted by a random value drawn in [-class_sep, class_sep].</p>
</dd>
<dt><strong>scale</strong><span class="classifier">float, array of shape [n_features] or None, optional (default=1.0)</span></dt><dd><p>Multiply features by the specified value. If None, then features
are scaled by a random value drawn in [1, 100]. Note that scaling
happens after shifting.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Shuffle the samples and the features.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None (default)</span></dt><dd><p>Determines random number generation for dataset creation. Pass an int
for reproducible output across multiple function calls.
See <a class="reference internal" href="#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p>
</dd>
<dt><strong>order: str, optional (default=’F’)</strong></dt><dd><p>The order of the generated samples</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">str, optional (default=’float32’)</span></dt><dd><p>Dtype of the generated samples</p>
</dd>
<dt><strong>n_parts</strong><span class="classifier">int (default = None)</span></dt><dd><p>number of partitions to generate (this can be greater
than the number of workers)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask.array backed by CuPy array of shape [n_samples, n_features]</span></dt><dd><p>The generated samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">dask.array backed by CuPy array of shape [n_samples]</span></dt><dd><p>The integer labels for class membership of each sample.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>How we extended the dask MNMG version from the single GPU version:</p>
<ol class="arabic simple">
<li><p>We generate centroids of shape <code class="docutils literal notranslate"><span class="pre">(n_centroids,</span> <span class="pre">n_informative)</span></code></p></li>
<li><p>We generate an informative covariance of shape         <code class="docutils literal notranslate"><span class="pre">(n_centroids,</span> <span class="pre">n_informative,</span> <span class="pre">n_informative)</span></code></p></li>
<li><p>We generate a redundant covariance of shape         <code class="docutils literal notranslate"><span class="pre">(n_informative,</span> <span class="pre">n_redundant)</span></code></p></li>
<li><p>We generate the indices for the repeated features     We pass along the references to the futures of the above arrays     with each part to the single GPU     <cite>cuml.datasets.classification.make_classification</cite> so that each     part (and worker) has access to the correct values to generate     data from the same covariances</p></li>
</ol>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
<span class="kn">from</span> <span class="nn">cuml.dask.datasets.classification</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"y:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">1.6990056</span>  <span class="o">-</span><span class="mf">0.8241044</span>  <span class="o">-</span><span class="mf">0.06997631</span>  <span class="mf">0.45107925</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.8105277</span>   <span class="mf">1.7829906</span>   <span class="mf">0.492909</span>    <span class="mf">0.05390119</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.18290454</span> <span class="o">-</span><span class="mf">0.6155432</span>   <span class="mf">0.6667889</span>  <span class="o">-</span><span class="mf">1.0053712</span> <span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">2.7530136</span>  <span class="o">-</span><span class="mf">0.888528</span>   <span class="o">-</span><span class="mf">0.5023055</span>   <span class="mf">1.3983376</span> <span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.9788184</span>  <span class="o">-</span><span class="mf">0.89851004</span>  <span class="mf">0.10802134</span> <span class="o">-</span><span class="mf">0.10021686</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.76883423</span> <span class="o">-</span><span class="mf">1.0689086</span>   <span class="mf">0.01249526</span> <span class="o">-</span><span class="mf">0.1404741</span> <span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.5676656</span>  <span class="o">-</span><span class="mf">0.83082974</span> <span class="o">-</span><span class="mf">0.03072987</span>  <span class="mf">0.34499463</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.9381793</span>  <span class="o">-</span><span class="mf">1.0971068</span>  <span class="o">-</span><span class="mf">0.07465998</span>  <span class="mf">0.02618019</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.3021476</span>  <span class="o">-</span><span class="mf">0.87076336</span>  <span class="mf">0.02249984</span>  <span class="mf">0.15187258</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.1820307</span>   <span class="mf">1.7524253</span>   <span class="mf">1.5087451</span>  <span class="o">-</span><span class="mf">2.4626074</span> <span class="p">]]</span>

<span class="n">y</span><span class="p">:</span>
<span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
<span class="target" id="module-cuml.dask.datasets.regression"></span><dl class="py function">
<dt id="cuml.dask.datasets.regression.make_low_rank_matrix">
<code class="sig-prename descclassname">cuml.dask.datasets.regression.</code><code class="sig-name descname">make_low_rank_matrix</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_samples</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">effective_rank</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">tail_strength</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_parts</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">n_samples_per_part</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/datasets/regression.py#L208"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.datasets.regression.make_low_rank_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a mostly low rank matrix with bell-shaped singular values</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of features.</p>
</dd>
<dt><strong>effective_rank</strong><span class="classifier">int, optional (default=10)</span></dt><dd><p>The approximate number of singular vectors required to explain most of
the data by linear combinations.</p>
</dd>
<dt><strong>tail_strength</strong><span class="classifier">float between 0.0 and 1.0, optional (default=0.5)</span></dt><dd><p>The relative importance of the fat noisy tail of the singular values
profile.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, CuPy RandomState instance, Dask RandomState instance                    or None (default)</span></dt><dd><p>Determines random number generation for dataset creation. Pass an int
for reproducible output across multiple function calls.</p>
</dd>
<dt><strong>n_parts</strong><span class="classifier">int, optional (default=1)</span></dt><dd><p>The number of parts of work.</p>
</dd>
<dt><strong>dtype: str, optional (default=’float32’)</strong></dt><dd><p>dtype of generated data</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask-CuPy array of shape [n_samples, n_features]</span></dt><dd><p>The matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.dask.datasets.regression.make_regression">
<code class="sig-prename descclassname">cuml.dask.datasets.regression.</code><code class="sig-name descname">make_regression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_samples</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">n_informative</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">n_targets</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">effective_rank</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tail_strength</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">noise</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">coef</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_parts</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">n_samples_per_part</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em>, <em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_full_low_rank</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/datasets/regression.py#L273"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.datasets.regression.make_regression" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a random regression problem.</p>
<p>The input set can either be well conditioned (by default) or have a low
rank-fat tail singular profile.</p>
<p>The output is generated by applying a (potentially biased) random linear
regression model with “n_informative” nonzero regressors to the previously
generated input and some gaussian centered noise with some adjustable
scale.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_features</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of features.</p>
</dd>
<dt><strong>n_informative</strong><span class="classifier">int, optional (default=10)</span></dt><dd><p>The number of informative features, i.e., the number of features used
to build the linear model used to generate the output.</p>
</dd>
<dt><strong>n_targets</strong><span class="classifier">int, optional (default=1)</span></dt><dd><p>The number of regression targets, i.e., the dimension of the y output
vector associated with a sample. By default, the output is a scalar.</p>
</dd>
<dt><strong>bias</strong><span class="classifier">float, optional (default=0.0)</span></dt><dd><p>The bias term in the underlying linear model.</p>
</dd>
<dt><strong>effective_rank</strong><span class="classifier">int or None, optional (default=None)</span></dt><dd><dl class="simple">
<dt>if not None:</dt><dd><p>The approximate number of singular vectors required to explain most
of the input data by linear combinations. Using this kind of
singular spectrum in the input allows the generator to reproduce
the correlations often observed in practice.</p>
</dd>
<dt>if None:</dt><dd><p>The input set is well conditioned, centered and gaussian with
unit variance.</p>
</dd>
</dl>
</dd>
<dt><strong>tail_strength</strong><span class="classifier">float between 0.0 and 1.0, optional (default=0.5)</span></dt><dd><p>The relative importance of the fat noisy tail of the singular values
profile if “effective_rank” is not None.</p>
</dd>
<dt><strong>noise</strong><span class="classifier">float, optional (default=0.0)</span></dt><dd><p>The standard deviation of the gaussian noise applied to the output.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=False)</span></dt><dd><p>Shuffle the samples and the features.</p>
</dd>
<dt><strong>coef</strong><span class="classifier">boolean, optional (default=False)</span></dt><dd><p>If True, the coefficients of the underlying linear model are returned.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, CuPy RandomState instance, Dask RandomState instance                    or None (default)</span></dt><dd><p>Determines random number generation for dataset creation. Pass an int
for reproducible output across multiple function calls.</p>
</dd>
<dt><strong>n_parts</strong><span class="classifier">int, optional (default=1)</span></dt><dd><p>The number of parts of work.</p>
</dd>
<dt><strong>order</strong><span class="classifier">str, optional (default=’F’)</span></dt><dd><p>Row-major or Col-major</p>
</dd>
<dt><strong>dtype: str, optional (default=’float32’)</strong></dt><dd><p>dtype of generated data</p>
</dd>
<dt><strong>use_full_low_rank</strong><span class="classifier">boolean (default=True)</span></dt><dd><p>Whether to use the entire dataset to generate the low rank matrix.
If False, it creates a low rank covariance and uses the
corresponding covariance to generate a multivariate normal
distribution on the remaining chunks</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask-CuPy array of shape [n_samples, n_features]</span></dt><dd><p>The input samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask-CuPy array of shape [n_samples] or [n_samples, n_targets]</span></dt><dd><p>The output values.</p>
</dd>
<dt><strong>coef</strong><span class="classifier">Dask-CuPy array of shape [n_features]            or [n_features, n_targets], optional</span></dt><dd><p>The coefficient of the underlying linear model. It is returned only if
coef is True.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<dl class="simple">
<dt>Known Performance Limitations:</dt><dd><ol class="arabic simple">
<li><p>When <cite>effective_rank</cite> is set and <cite>use_full_low_rank</cite> is True,         we cannot generate order <cite>F</cite> by construction, and an explicit         transpose is performed on each part. This may cause memory to spike         (other parameters make order <cite>F</cite> by construction)</p></li>
<li><p>When <cite>n_targets &gt; 1</cite> and <cite>order = ‘F’</cite> as above, we have to         explicity transpose the <cite>y</cite> array. If <cite>coef = True</cite>, then we also         explicity transpose the <cite>ground_truth</cite> array</p></li>
<li><p>When <cite>shuffle = True</cite> and <cite>order = F</cite>, there are memory spikes to         shuffle the <cite>F</cite> order arrays</p></li>
</ol>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If out-of-memory errors are encountered in any of the above
configurations, try increasing the <cite>n_parts</cite> parameter.</p>
</div>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="array-wrappers-internal-api">
<h3>Array Wrappers (Internal API)<a class="headerlink" href="#array-wrappers-internal-api" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.common.CumlArray">
<em class="property">class </em><code class="sig-prename descclassname">cuml.common.</code><code class="sig-name descname">CumlArray</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">owner</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/array.py#L30"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.CumlArray" title="Permalink to this definition">¶</a></dt>
<dd><p>Array represents an abstracted array allocation. It can be instantiated by
itself, creating an rmm.DeviceBuffer underneath, or can be instantiated by
<code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> or <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code> compliant arrays,
in which case it’ll keep a reference to that data underneath. Also can be
created from a pointer, specifying the characteristics of the array, in
that case the owner of the data referred to by the pointer should be
specified explicitly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>data</strong><span class="classifier">rmm.DeviceBuffer, cudf.Buffer, array_like, int, bytes, bytearray or           memoryview</span></dt><dd><p>An array-like object or integer representing a
device or host pointer to pre-allocated memory.</p>
</dd>
<dt><strong>owner</strong><span class="classifier">object, optional</span></dt><dd><p>Python object to which the lifetime of the memory
allocation is tied. If provided, a reference to this
object is kept in this Buffer.</p>
</dd>
<dt><strong>dtype</strong><span class="classifier">data-type, optional</span></dt><dd><p>Any object that can be interpreted as a numpy or cupy data type.</p>
</dd>
<dt><strong>shape</strong><span class="classifier">int or tuple of ints, optional</span></dt><dd><p>Shape of created array.</p>
</dd>
<dt><strong>order: string, optional</strong></dt><dd><p>Whether to create a F-major or C-major array.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>cuml Array is not meant as an end-user array library. It is meant for
cuML/RAPIDS developer consumption. Therefore it contains the minimum
functionality. Its functionality is hidden by base.pyx to provide
automatic output format conversion so that the users see the important
attributes in whatever format they prefer.</p>
<p>Todo: support cuda streams in the constructor. See:
<a class="reference external" href="https://github.com/rapidsai/cuml/issues/1712">https://github.com/rapidsai/cuml/issues/1712</a>
<a class="reference external" href="https://github.com/rapidsai/cuml/pull/1396">https://github.com/rapidsai/cuml/pull/1396</a></p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>ptr</strong><span class="classifier">int</span></dt><dd><p>Pointer to the data</p>
</dd>
<dt><strong>size</strong><span class="classifier">int</span></dt><dd><p>Size of the array data in bytes</p>
</dd>
<dt><strong>_owner</strong><span class="classifier">Python Object</span></dt><dd><p>Object that owns the data of the array</p>
</dd>
<dt><strong>shape</strong><span class="classifier">tuple of ints</span></dt><dd><p>Shape of the array</p>
</dd>
<dt><strong>order</strong><span class="classifier">{‘F’, ‘C’}</span></dt><dd><p>‘F’ or ‘C’ to indicate Fortran-major or C-major order of the array</p>
</dd>
<dt><strong>strides</strong><span class="classifier">tuple of ints</span></dt><dd><p>Strides of the data</p>
</dd>
<dt><strong>__cuda_array_interface__</strong><span class="classifier">dictionary</span></dt><dd><p><code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> to interop with other libraries.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.common.CumlArray.empty" title="cuml.common.CumlArray.empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty</span></code></a>(shape, dtype[, order])</p></td>
<td><p>Create an empty Array with an allocated but uninitialized DeviceBuffer</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.common.CumlArray.full" title="cuml.common.CumlArray.full"><code class="xref py py-obj docutils literal notranslate"><span class="pre">full</span></code></a>(shape, value, dtype[, order])</p></td>
<td><p>Create an Array with an allocated DeviceBuffer initialized to value.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.common.CumlArray.ones" title="cuml.common.CumlArray.ones"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ones</span></code></a>(shape[, dtype, order])</p></td>
<td><p>Create an Array with an allocated DeviceBuffer initialized to zeros.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.common.CumlArray.to_output" title="cuml.common.CumlArray.to_output"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_output</span></code></a>([output_type, output_dtype])</p></td>
<td><p>Convert array to output format</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.common.CumlArray.zeros" title="cuml.common.CumlArray.zeros"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zeros</span></code></a>(shape[, dtype, order])</p></td>
<td><p>Create an Array with an allocated DeviceBuffer initialized to zeros.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 57%"/>
<col style="width: 43%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>serialize</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.common.CumlArray.empty">
<em class="property">classmethod </em><code class="sig-name descname">empty</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/array.py#L287"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.CumlArray.empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty Array with an allocated but uninitialized DeviceBuffer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dtype</strong><span class="classifier">data-type, optional</span></dt><dd><p>Any object that can be interpreted as a numpy or cupy data type.</p>
</dd>
<dt><strong>shape</strong><span class="classifier">int or tuple of ints, optional</span></dt><dd><p>Shape of created array.</p>
</dd>
<dt><strong>order: string, optional</strong></dt><dd><p>Whether to create a F-major or C-major array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.common.CumlArray.full">
<em class="property">classmethod </em><code class="sig-name descname">full</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/array.py#L306"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.CumlArray.full" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an Array with an allocated DeviceBuffer initialized to value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dtype</strong><span class="classifier">data-type, optional</span></dt><dd><p>Any object that can be interpreted as a numpy or cupy data type.</p>
</dd>
<dt><strong>shape</strong><span class="classifier">int or tuple of ints, optional</span></dt><dd><p>Shape of created array.</p>
</dd>
<dt><strong>order: string, optional</strong></dt><dd><p>Whether to create a F-major or C-major array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.common.CumlArray.ones">
<em class="property">classmethod </em><code class="sig-name descname">ones</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/array.py#L342"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.CumlArray.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an Array with an allocated DeviceBuffer initialized to zeros.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dtype</strong><span class="classifier">data-type, optional</span></dt><dd><p>Any object that can be interpreted as a numpy or cupy data type.</p>
</dd>
<dt><strong>shape</strong><span class="classifier">int or tuple of ints, optional</span></dt><dd><p>Shape of created array.</p>
</dd>
<dt><strong>order: string, optional</strong></dt><dd><p>Whether to create a F-major or C-major array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.common.CumlArray.to_output">
<code class="sig-name descname">to_output</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">'cupy'</span></em>, <em class="sig-param"><span class="n">output_dtype</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/array.py#L202"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.CumlArray.to_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert array to output format</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>output_type</strong><span class="classifier">string</span></dt><dd><p>Format to convert the array to. Acceptable formats are:</p>
<ul class="simple">
<li><p>‘cupy’ - to cupy array</p></li>
<li><p>‘numpy’ - to numpy (host) array</p></li>
<li><p>‘numba’ - to numba device array</p></li>
<li><p>‘dataframe’ - to cuDF DataFrame</p></li>
<li><p>‘series’ - to cuDF Series</p></li>
<li><dl class="simple">
<dt>‘cudf’ - to cuDF Series if array is single dimensional, to</dt><dd><p>DataFrame otherwise</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt><strong>output_dtype</strong><span class="classifier">string, optional</span></dt><dd><p>Optionally cast the array to a specified dtype, creating
a copy if necessary.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.common.CumlArray.zeros">
<em class="property">classmethod </em><code class="sig-name descname">zeros</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">'float32'</span></em>, <em class="sig-param"><span class="n">order</span><span class="o">=</span><span class="default_value">'F'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/common/array.py#L326"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.common.CumlArray.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an Array with an allocated DeviceBuffer initialized to zeros.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dtype</strong><span class="classifier">data-type, optional</span></dt><dd><p>Any object that can be interpreted as a numpy or cupy data type.</p>
</dd>
<dt><strong>shape</strong><span class="classifier">int or tuple of ints, optional</span></dt><dd><p>Shape of created array.</p>
</dd>
<dt><strong>order: string, optional</strong></dt><dd><p>Whether to create a F-major or C-major array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="metrics-regression-classification-and-distance">
<h3>Metrics (regression, classification, and distance)<a class="headerlink" href="#metrics-regression-classification-and-distance" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><span class="target" id="module-cuml.metrics.regression"></span><dl class="py function">
<dt id="cuml.metrics.regression.mean_absolute_error">
<code class="sig-prename descclassname">cuml.metrics.regression.</code><code class="sig-name descname">mean_absolute_error</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">multioutput</span><span class="o">=</span><span class="default_value">'uniform_average'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/regression.pyx#L202"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.regression.mean_absolute_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean absolute error regression loss</p>
<p>Be careful when using this metric with float32 inputs as the result can be
slightly incorrect because of floating point precision if the input is
large enough. float64 will have lower numerical error.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Ground truth (correct) target values.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Estimated target values.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), optional</span></dt><dd><p>Sample weights.</p>
</dd>
<dt><strong>multioutput</strong><span class="classifier">string in [‘raw_values’, ‘uniform_average’]</span></dt><dd><p>or array-like of shape (n_outputs)
Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.
‘raw_values’ :
Returns a full set of errors in case of multioutput input.
‘uniform_average’ :
Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>loss</strong><span class="classifier">float or ndarray of floats</span></dt><dd><p>If multioutput is ‘raw_values’, then mean absolute error is returned
for each output separately. If multioutput is ‘uniform_average’ or an
ndarray of weights, then the weighted average of all output errors is
returned.</p>
<p>MAE output is non-negative floating point. The best value is 0.0.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.metrics.regression.mean_squared_error">
<code class="sig-prename descclassname">cuml.metrics.regression.</code><code class="sig-name descname">mean_squared_error</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">multioutput</span><span class="o">=</span><span class="default_value">'uniform_average'</span></em>, <em class="sig-param"><span class="n">squared</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/regression.pyx#L157"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.regression.mean_squared_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean squared error regression loss</p>
<p>Be careful when using this metric with float32 inputs as the result can be
slightly incorrect because of floating point precision if the input is
large enough. float64 will have lower numerical error.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Ground truth (correct) target values.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Estimated target values.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), optional</span></dt><dd><p>Sample weights.</p>
</dd>
<dt><strong>multioutput</strong><span class="classifier">string in [‘raw_values’, ‘uniform_average’]</span></dt><dd><p>or array-like of shape (n_outputs)
Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.
‘raw_values’ :
Returns a full set of errors in case of multioutput input.
‘uniform_average’ :
Errors of all outputs are averaged with uniform weight.</p>
</dd>
<dt><strong>squared</strong><span class="classifier">boolean value, optional (default = True)</span></dt><dd><p>If True returns MSE value, if False returns RMSE value.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">float or ndarray of floats</span></dt><dd><p>A non-negative floating point value (the best value is 0.0), or an
array of floating point values, one for each individual target.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.metrics.regression.mean_squared_log_error">
<code class="sig-prename descclassname">cuml.metrics.regression.</code><code class="sig-name descname">mean_squared_log_error</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">multioutput</span><span class="o">=</span><span class="default_value">'uniform_average'</span></em>, <em class="sig-param"><span class="n">squared</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/regression.pyx#L253"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.regression.mean_squared_log_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean squared log error regression loss</p>
<p>Be careful when using this metric with float32 inputs as the result can be
slightly incorrect because of floating point precision if the input is
large enough. float64 will have lower numerical error.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Ground truth (correct) target values.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Estimated target values.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), optional</span></dt><dd><p>Sample weights.</p>
</dd>
<dt><strong>multioutput</strong><span class="classifier">string in [‘raw_values’, ‘uniform_average’]</span></dt><dd><p>or array-like of shape (n_outputs)
Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.
‘raw_values’ :
Returns a full set of errors in case of multioutput input.
‘uniform_average’ :
Errors of all outputs are averaged with uniform weight.</p>
</dd>
<dt><strong>squared</strong><span class="classifier">boolean value, optional (default = True)</span></dt><dd><p>If True returns MSE value, if False returns RMSE value.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">float or ndarray of floats</span></dt><dd><p>A non-negative floating point value (the best value is 0.0), or an
array of floating point values, one for each individual target.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.metrics.regression.r2_score">
<code class="sig-prename descclassname">cuml.metrics.regression.</code><code class="sig-name descname">r2_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">y_hat</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/regression.pyx#L31"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.regression.r2_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates r2 score between y and y_hat</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><blockquote>
<div><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>y_hat<span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1).
Acceptable formats: cuDF Series, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y_hat to be the same data type as y if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>trustworthiness score</strong><span class="classifier">double</span></dt><dd><p>Trustworthiness of the low-dimensional embedding</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.metrics.accuracy"></span><dl class="py function">
<dt id="cuml.metrics.accuracy.accuracy_score">
<code class="sig-prename descclassname">cuml.metrics.accuracy.</code><code class="sig-name descname">accuracy_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ground_truth</span></em>, <em class="sig-param"><span class="n">predictions</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/accuracy.pyx#L38"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.accuracy.accuracy_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Calcuates the accuracy score of a classification model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><dl class="simple">
<dt>prediction<span class="classifier">NumPy ndarray or Numba device</span></dt><dd><p>The labels predicted by the model for the test dataset</p>
</dd>
<dt>ground_truth<span class="classifier">NumPy ndarray, Numba device</span></dt><dd><p>The ground truth labels of the test dataset</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>float</dt><dd><p>The accuracy of the model used for prediction</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.metrics.confusion_matrix">
<code class="sig-prename descclassname">metrics.</code><code class="sig-name descname">confusion_matrix</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/confusion_matrix.py#L27"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute confusion matrix to evaluate the accuracy of a classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Ground truth (correct) target values.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>or (n_samples, n_outputs)
Estimated target values.</p>
</dd>
<dt><strong>labels</strong><span class="classifier">array-like (device or host) shape = (n_classes,), optional</span></dt><dd><p>List of labels to index the matrix. This may be used to reorder or
select a subset of labels. If None is given, those that appear at least
once in y_true or y_pred are used in sorted order.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), optional</span></dt><dd><p>Sample weights.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">string in [‘true’, ‘pred’, ‘all’]</span></dt><dd><p>Normalizes confusion matrix over the true (rows), predicted (columns)
conditions or all the population. If None, confusion matrix will not be
normalized.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>C</strong><span class="classifier">array-like (device or host) shape = (n_classes, n_classes)</span></dt><dd><p>Confusion matrix.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.metrics.roc_auc_score">
<code class="sig-prename descclassname">metrics.</code><code class="sig-name descname">roc_auc_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_score</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/_ranking.py#L119"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.roc_auc_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)
from prediction scores.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>this implementation can only be used with binary classification.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd><p>True labels. The binary cases
expect labels with shape (n_samples,)</p>
</dd>
<dt><strong>y_score</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd><p>Target scores. In the binary cases, these can be either
probability estimates or non-thresholded decision values (as returned
by <cite>decision_function</cite> on some classifiers). The binary
case expects a shape (n_samples,), and the scores must be the scores of
the class with the greater label.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>auc</strong><span class="classifier">float</span></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">))</span>
<span class="go">0.75</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.metrics.precision_recall_curve">
<code class="sig-prename descclassname">metrics.</code><code class="sig-name descname">precision_recall_curve</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">probs_pred</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/_ranking.py#L24"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.precision_recall_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute precision-recall pairs for different probability thresholds</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>this implementation is restricted to the binary classification
task. The precision is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fp)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the
number of true positives and <code class="docutils literal notranslate"><span class="pre">fp</span></code> the number of false positives. The
precision is intuitively the ability of the classifier not to label as
positive a sample that is negative.</p>
<p>The recall is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fn)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number
of true positives and <code class="docutils literal notranslate"><span class="pre">fn</span></code> the number of false negatives. The recall
is intuitively the ability of the classifier to find all the positive
samples. The last precision and recall values are 1. and 0.
respectively and do not have a corresponding threshold. This ensures
that the graph starts on the y axis.</p>
<p>Read more in the scikit-learn’s <a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics">User Guide</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">array, shape = [n_samples]</span></dt><dd><p>True binary labels, {0, 1}.</p>
</dd>
<dt><strong>probas_pred</strong><span class="classifier">array, shape = [n_samples]</span></dt><dd><p>Estimated probabilities or decision function.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>precision</strong><span class="classifier">array, shape = [n_thresholds + 1]</span></dt><dd><p>Precision values such that element i is the precision of
predictions with score &gt;= thresholds[i] and the last element is 1.</p>
</dd>
<dt><strong>recall</strong><span class="classifier">array, shape = [n_thresholds + 1]</span></dt><dd><p>Decreasing recall values such that element i is the recall of
predictions with score &gt;= thresholds[i] and the last element is 0.</p>
</dd>
<dt><strong>thresholds</strong><span class="classifier">array, shape = [n_thresholds &lt;= len(np.unique(probas_pred))]</span></dt><dd><p>Increasing thresholds on the decision function used to compute
precision and recall.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_curve</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span>
    <span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">recall</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">thresholds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">0.66666667</span><span class="p">,</span> <span class="mf">0.5</span>       <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">])</span>
<span class="n">array</span><span class="p">([</span><span class="mf">1.</span> <span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span> <span class="p">])</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.4</span> <span class="p">,</span> <span class="mf">0.8</span> <span class="p">])</span>
</pre></div>
</div>
</dd></dl>
<span class="target" id="module-cuml.metrics.pairwise_distances"></span><dl class="py function">
<dt id="cuml.metrics.pairwise_distances.pairwise_distances">
<code class="sig-prename descclassname">cuml.metrics.pairwise_distances.</code><code class="sig-name descname">pairwise_distances</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwds</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/pairwise_distances.pyx#L104"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.pairwise_distances.pairwise_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the distance matrix from a vector array <cite>X</cite> and optional <cite>Y</cite>.</p>
<p>This method takes either one or two vector arrays, and returns a distance
matrix.</p>
<p>If <cite>Y</cite> is given (default is <cite>None</cite>), then the returned matrix is the
pairwise distance between the arrays from both <cite>X</cite> and <cite>Y</cite>.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><dl class="simple">
<dt>From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,         ‘manhattan’].</dt><dd><p>Sparse matrices are not supported.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>From scipy.spatial.distance: [‘sqeuclidean’]</dt><dd><p>See the documentation for scipy.spatial.distance for details on this
metric. Sparse matrices are not supported.</p>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) of shape (n_samples_x, n_features)</span></dt><dd><p>Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>Y</strong><span class="classifier">array-like (device or host) of shape (n_samples_y, n_features),        optional</span></dt><dd><p>Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>metric</strong><span class="classifier">{“cityblock”, “cosine”, “euclidean”, “l1”, “l2”, “manhattan”,         “sqeuclidean”}</span></dt><dd><p>The metric to use when calculating distance between instances in a
feature array.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will, when necessary, convert
Y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>D</strong><span class="classifier">array [n_samples_x, n_samples_x] or [n_samples_x, n_samples_y]</span></dt><dd><p>A distance matrix D such that D_{i, j} is the distance between the
ith and jth vectors of the given matrix <cite>X</cite>, if <cite>Y</cite> is None.
If <cite>Y</cite> is not <cite>None</cite>, then D_{i, j} is the distance between the ith
array from <cite>X</cite> and the jth array from <cite>Y</cite>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Euclidean Pairwise Distance, Single Input:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'euclidean'</span><span class="p">)</span>
<span class="go">array([[0.        , 2.23606798, 5.83095189],</span>
<span class="go">    [2.23606798, 0.        , 3.60555128],</span>
<span class="go">    [5.83095189, 3.60555128, 0.        ]])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Cosine Pairwise Distance, Multi-Input:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'cosine'</span><span class="p">)</span>
<span class="go">array([[0.4452998 , 0.13175686],</span>
<span class="go">    [0.48550424, 0.15633851],</span>
<span class="go">    [0.47000106, 0.14671817]])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Manhattan Pairwise Distance, Multi-Input:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'manhattan'</span><span class="p">)</span>
<span class="go">array([[ 4.,  2.],</span>
<span class="go">    [ 7.,  5.],</span>
<span class="go">    [12., 10.]])</span>
</pre></div>
</div>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="metrics-clustering-and-trustworthiness">
<h3>Metrics (clustering and trustworthiness)<a class="headerlink" href="#metrics-clustering-and-trustworthiness" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><span class="target" id="module-cuml.metrics.trustworthiness"></span><dl class="py function">
<dt id="cuml.metrics.trustworthiness.trustworthiness">
<code class="sig-prename descclassname">cuml.metrics.trustworthiness.</code><code class="sig-name descname">trustworthiness</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">X_embedded</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">should_downcast</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">512</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/trustworthiness.pyx#L55"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.trustworthiness.trustworthiness" title="Permalink to this definition">¶</a></dt>
<dd><p>Expresses to what extent the local structure is retained in embedding.
The score is defined in the range [0, 1].</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><blockquote>
<div><p>Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</div></blockquote>
<dl class="simple">
<dt>X_embedded<span class="classifier">array-like (device or host) shape= (n_samples, n_features)</span></dt><dd><p>Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt>n_neighbors<span class="classifier">int, optional (default: 5)</span></dt><dd><p>Number of neighbors considered</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the trustworthiness method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>trustworthiness score</strong><span class="classifier">double</span></dt><dd><p>Trustworthiness of the low-dimensional embedding</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.metrics.cluster.adjustedrandindex"></span><dl class="py function">
<dt id="cuml.metrics.cluster.adjustedrandindex.adjusted_rand_score">
<code class="sig-prename descclassname">cuml.metrics.cluster.adjustedrandindex.</code><code class="sig-name descname">adjusted_rand_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labels_true</span></em>, <em class="sig-param"><span class="n">labels_pred</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/cluster/adjustedrandindex.pyx#L37"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.cluster.adjustedrandindex.adjusted_rand_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Adjusted_rand_score is a clustering similarity metric based on the Rand
index and is corrected for chance.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>labels_true</strong><span class="classifier">Ground truth labels to be used as a reference</span></dt><dd><p>labels_pred : Array of predicted labels used to evaluate the model</p>
<p>handle : cuml.Handle</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>float</dt><dd><p>The adjusted rand index value between -1.0 and 1.0</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.metrics.cluster.entropy"></span><dl class="py function">
<dt id="cuml.metrics.cluster.entropy.cython_entropy">
<code class="sig-prename descclassname">cuml.metrics.cluster.entropy.</code><code class="sig-name descname">cython_entropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">clustering</span></em>, <em class="sig-param"><span class="n">base</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/cluster/entropy.pyx#L54"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.cluster.entropy.cython_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the entropy of a distribution for given probability values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>clustering</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>Clustering of labels. Probabilities are computed based on occurrences
of labels. For instance, to represent a fair coin (2 equally possible
outcomes), the clustering could be [0,1]. For a biased coin with 2/3
probability for tail, the clustering could be [0, 0, 1].</p>
</dd>
<dt><strong>base: float, optional</strong></dt><dd><p>The logarithmic base to use, defaults to e (natural logarithm).</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>S</strong><span class="classifier">float</span></dt><dd><p>The calculated entropy.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.metrics.cluster.homogeneity_score"></span><dl class="py function">
<dt id="cuml.metrics.cluster.homogeneity_score.homogeneity_score">
<code class="sig-prename descclassname">cuml.metrics.cluster.homogeneity_score.</code><code class="sig-name descname">homogeneity_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labels_true</span></em>, <em class="sig-param"><span class="n">labels_pred</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/cluster/homogeneity_score.pyx#L32"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.cluster.homogeneity_score.homogeneity_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the homogeneity metric of a cluster labeling given a ground truth.</p>
<p>A clustering result satisfies homogeneity if all of its clusters contain
only data points which are members of a single class.</p>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the score
value in any way.</p>
<p>This metric is not symmetric: switching label_true with label_pred will
return the completeness_score which will be different in general.</p>
<p>The labels in labels_pred and labels_true are assumed to be drawn from a
contiguous set (Ex: drawn from {2, 3, 4}, but not from {2, 4}). If your
set of labels looks like {2, 4}, convert them to something like {0, 1}.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>labels_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>The labels predicted by the model for the test dataset.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>labels_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>The ground truth labels (ints) of the test dataset.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>float</dt><dd><p>The homogeneity of the predicted labeling given the ground truth.
Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.metrics.cluster.completeness_score"></span><dl class="py function">
<dt id="cuml.metrics.cluster.completeness_score.completeness_score">
<code class="sig-prename descclassname">cuml.metrics.cluster.completeness_score.</code><code class="sig-name descname">completeness_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labels_true</span></em>, <em class="sig-param"><span class="n">labels_pred</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/cluster/completeness_score.pyx#L32"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.cluster.completeness_score.completeness_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Completeness metric of a cluster labeling given a ground truth.</p>
<p>A clustering result satisfies completeness if all the data points that are
members of a given class are elements of the same cluster.</p>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the score
value in any way.</p>
<p>This metric is not symmetric: switching label_true with label_pred will
return the homogeneity_score which will be different in general.</p>
<p>The labels in labels_pred and labels_true are assumed to be drawn from a
contiguous set (Ex: drawn from {2, 3, 4}, but not from {2, 4}). If your
set of labels looks like {2, 4}, convert them to something like {0, 1}.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>labels_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>The labels predicted by the model for the test dataset.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>labels_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>The ground truth labels (ints) of the test dataset.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>float</dt><dd><p>The completeness of the predicted labeling given the ground truth.
Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.metrics.cluster.mutual_info_score"></span><dl class="py function">
<dt id="cuml.metrics.cluster.mutual_info_score.mutual_info_score">
<code class="sig-prename descclassname">cuml.metrics.cluster.mutual_info_score.</code><code class="sig-name descname">mutual_info_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labels_true</span></em>, <em class="sig-param"><span class="n">labels_pred</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/metrics/cluster/mutual_info_score.pyx#L35"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.metrics.cluster.mutual_info_score.mutual_info_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Mutual Information between two clusterings.</p>
<p>The Mutual Information is a measure of the similarity between two labels of
the same data.</p>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the score
value in any way.</p>
<p>This metric is furthermore symmetric: switching label_true with label_pred
will return the same score value. This can be useful to measure the
agreement of two independent label assignments strategies on the same
dataset when the real ground truth is not known.</p>
<p>The labels in labels_pred and labels_true are assumed to be drawn from a
contiguous set (Ex: drawn from {2, 3, 4}, but not from {2, 4}). If your
set of labels looks like {2, 4}, convert them to something like {0, 1}.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd></dd>
<dt><strong>labels_pred</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>A clustering of the data (ints) into disjoint subsets.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
<dt><strong>labels_true</strong><span class="classifier">array-like (device or host) shape = (n_samples,)</span></dt><dd><p>A clustering of the data (ints) into disjoint subsets.
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>float</dt><dd><p>Mutual information, a non-negative value</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</div></blockquote>
</div>
<div class="section" id="benchmarking">
<h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><span class="target" id="module-cuml.benchmark.algorithms"></span><dl class="py class">
<dt id="cuml.benchmark.algorithms.AlgorithmPair">
<em class="property">class </em><code class="sig-prename descclassname">cuml.benchmark.algorithms.</code><code class="sig-name descname">AlgorithmPair</code><span class="sig-paren">(</span><em class="sig-param">cpu_class</em>, <em class="sig-param">cuml_class</em>, <em class="sig-param">shared_args</em>, <em class="sig-param">cuml_args={}</em>, <em class="sig-param">cpu_args={}</em>, <em class="sig-param">name=None</em>, <em class="sig-param">accepts_labels=True</em>, <em class="sig-param">cpu_data_prep_hook=None</em>, <em class="sig-param">cuml_data_prep_hook=None</em>, <em class="sig-param">accuracy_function=None</em>, <em class="sig-param">bench_func=&lt;function fit&gt;</em>, <em class="sig-param">setup_cpu_func=None</em>, <em class="sig-param">setup_cuml_func=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/algorithms.py#L56"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.algorithms.AlgorithmPair" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps a cuML algorithm and (optionally) a cpu-based algorithm
(typically scikit-learn, but does not need to be as long as it offers
<cite>fit</cite> and <cite>predict</cite> or <cite>transform</cite> methods).
Provides mechanisms to run each version with default arguments.
If no CPU-based version of the algorithm is available, pass None for the
cpu_class when instantiating</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>cpu_class</strong><span class="classifier">class</span></dt><dd><p>Class for CPU version of algorithm. Set to None if not available.</p>
</dd>
<dt><strong>cuml_class</strong><span class="classifier">class</span></dt><dd><p>Class for cuML algorithm</p>
</dd>
<dt><strong>shared_args</strong><span class="classifier">dict</span></dt><dd><p>Arguments passed to both implementations’s initializer</p>
</dd>
<dt><strong>cuml_args</strong><span class="classifier">dict</span></dt><dd><p>Arguments <em>only</em> passed to cuml’s initializer</p>
</dd>
<dt><strong>cpu_args dict</strong></dt><dd><p>Arguments <em>only</em> passed to sklearn’s initializer</p>
</dd>
<dt><strong>accepts_labels</strong><span class="classifier">boolean</span></dt><dd><p>If True, the fit methods expects both X and y
inputs. Otherwise, it expects only an X input.</p>
</dd>
<dt><strong>data_prep_hook</strong><span class="classifier">function (data -&gt; data)</span></dt><dd><p>Optional function to run on input data before passing to fit</p>
</dd>
<dt><strong>accuracy_function</strong><span class="classifier">function (y_test, y_pred)</span></dt><dd><p>Function that returns a scalar representing accuracy</p>
</dd>
<dt><strong>bench_func</strong><span class="classifier">custom function to perform fit/predict/transform</span></dt><dd><p>calls.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.benchmark.algorithms.AlgorithmPair.run_cpu" title="cuml.benchmark.algorithms.AlgorithmPair.run_cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">run_cpu</span></code></a>(data, **override_args)</p></td>
<td><p>Runs the cpu-based algorithm’s fit method on specified data</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.benchmark.algorithms.AlgorithmPair.run_cuml" title="cuml.benchmark.algorithms.AlgorithmPair.run_cuml"><code class="xref py py-obj docutils literal notranslate"><span class="pre">run_cuml</span></code></a>(data, **override_args)</p></td>
<td><p>Runs the cuml-based algorithm’s fit method on specified data</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 58%"/>
<col style="width: 42%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>setup_cpu</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>setup_cuml</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.benchmark.algorithms.AlgorithmPair.run_cpu">
<code class="sig-name descname">run_cpu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">override_args</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/algorithms.py#L125"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.algorithms.AlgorithmPair.run_cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs the cpu-based algorithm’s fit method on specified data</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.benchmark.algorithms.AlgorithmPair.run_cuml">
<code class="sig-name descname">run_cuml</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">override_args</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/algorithms.py#L146"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.algorithms.AlgorithmPair.run_cuml" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs the cuml-based algorithm’s fit method on specified data</p>
</dd></dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.benchmark.algorithms.algorithm_by_name">
<code class="sig-prename descclassname">cuml.benchmark.algorithms.</code><code class="sig-name descname">algorithm_by_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/algorithms.py#L565"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.algorithms.algorithm_by_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the algorithm pair with the name ‘name’ (case-insensitive)</p>
</dd></dl>
<dl class="py function">
<dt id="cuml.benchmark.algorithms.all_algorithms">
<code class="sig-prename descclassname">cuml.benchmark.algorithms.</code><code class="sig-name descname">all_algorithms</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/algorithms.py#L199"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.algorithms.all_algorithms" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all defined AlgorithmPair objects</p>
</dd></dl>
<span class="target" id="module-cuml.benchmark.runners"></span><p>Wrappers to run ML benchmarks</p>
<dl class="py class">
<dt id="cuml.benchmark.runners.AccuracyComparisonRunner">
<em class="property">class </em><code class="sig-prename descclassname">cuml.benchmark.runners.</code><code class="sig-name descname">AccuracyComparisonRunner</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">bench_rows</span></em>, <em class="sig-param"><span class="n">bench_dims</span></em>, <em class="sig-param"><span class="n">dataset_name</span><span class="o">=</span><span class="default_value">'blobs'</span></em>, <em class="sig-param"><span class="n">input_type</span><span class="o">=</span><span class="default_value">'numpy'</span></em>, <em class="sig-param"><span class="n">test_fraction</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">n_reps</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/runners.py#L183"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.runners.AccuracyComparisonRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper to run an algorithm with multiple dataset sizes
and compute accuracy and speedup of cuml relative to sklearn
baseline.</p>
</dd></dl>
<dl class="py class">
<dt id="cuml.benchmark.runners.BenchmarkTimer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.benchmark.runners.</code><code class="sig-name descname">BenchmarkTimer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">reps</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/runners.py#L28"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.runners.BenchmarkTimer" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides a context manager that runs a code block <cite>reps</cite> times
and records results to the instance variable <cite>timings</cite>. Use like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">timer</span> <span class="o">=</span> <span class="n">BenchmarkTimer</span><span class="p">(</span><span class="n">rep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">timer</span><span class="o">.</span><span class="n">benchmark_runs</span><span class="p">():</span>
    <span class="o">...</span> <span class="n">do</span> <span class="n">something</span> <span class="o">...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">timings</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 64%"/>
<col style="width: 36%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>benchmark_runs</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="py class">
<dt id="cuml.benchmark.runners.SpeedupComparisonRunner">
<em class="property">class </em><code class="sig-prename descclassname">cuml.benchmark.runners.</code><code class="sig-name descname">SpeedupComparisonRunner</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">bench_rows</span></em>, <em class="sig-param"><span class="n">bench_dims</span></em>, <em class="sig-param"><span class="n">dataset_name</span><span class="o">=</span><span class="default_value">'blobs'</span></em>, <em class="sig-param"><span class="n">input_type</span><span class="o">=</span><span class="default_value">'numpy'</span></em>, <em class="sig-param"><span class="n">n_reps</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/runners.py#L52"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.runners.SpeedupComparisonRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper to run an algorithm with multiple dataset sizes
and compute speedup of cuml relative to sklearn baseline.</p>
<p class="rubric">Methods</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 41%"/>
<col style="width: 59%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>run</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="py function">
<dt id="cuml.benchmark.runners.run_variations">
<code class="sig-prename descclassname">cuml.benchmark.runners.</code><code class="sig-name descname">run_variations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">algos</span></em>, <em class="sig-param"><span class="n">dataset_name</span></em>, <em class="sig-param"><span class="n">bench_rows</span></em>, <em class="sig-param"><span class="n">bench_dims</span></em>, <em class="sig-param"><span class="n">param_override_list</span><span class="o">=</span><span class="default_value">[{}]</span></em>, <em class="sig-param"><span class="n">cuml_param_override_list</span><span class="o">=</span><span class="default_value">[{}]</span></em>, <em class="sig-param"><span class="n">cpu_param_override_list</span><span class="o">=</span><span class="default_value">[{}]</span></em>, <em class="sig-param"><span class="n">dataset_param_override_list</span><span class="o">=</span><span class="default_value">[{}]</span></em>, <em class="sig-param"><span class="n">input_type</span><span class="o">=</span><span class="default_value">'numpy'</span></em>, <em class="sig-param"><span class="n">test_fraction</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">run_cpu</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">raise_on_error</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">n_reps</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/runners.py#L298"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.runners.run_variations" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs each algo in <cite>algos</cite> once per
<cite>bench_rows X bench_dims X params_override_list X cuml_param_override_list</cite>
combination and returns a dataframe containing timing and accuracy data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algos</strong><span class="classifier">str or list</span></dt><dd><p>Name of algorithms to run and evaluate</p>
</dd>
<dt><strong>dataset_name</strong><span class="classifier">str</span></dt><dd><p>Name of dataset to use</p>
</dd>
<dt><strong>bench_rows</strong><span class="classifier">list of int</span></dt><dd><p>Dataset row counts to test</p>
</dd>
<dt><strong>bench_dims</strong><span class="classifier">list of int</span></dt><dd><p>Dataset column counts to test</p>
</dd>
<dt><strong>param_override_list</strong><span class="classifier">list of dict</span></dt><dd><p>Dicts containing parameters to pass to __init__.
Each dict specifies parameters to override in one run of the algorithm.</p>
</dd>
<dt><strong>cuml_param_override_list</strong><span class="classifier">list of dict</span></dt><dd><p>Dicts containing parameters to pass to __init__ of the cuml algo only.</p>
</dd>
<dt><strong>cpu_param_override_list</strong><span class="classifier">list of dict</span></dt><dd><p>Dicts containing parameters to pass to __init__ of the cpu algo only.</p>
</dd>
<dt><strong>dataset_param_override_list</strong><span class="classifier">dict</span></dt><dd><p>Dicts containing parameters to pass to dataset generator function</p>
</dd>
<dt><strong>test_fraction</strong><span class="classifier">float</span></dt><dd><p>The fraction of data to use for testing.</p>
</dd>
<dt><strong>run_cpu</strong><span class="classifier">boolean</span></dt><dd><p>If True, run the cpu-based algorithm for comparison</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-cuml.benchmark.datagen"></span><p>Data generators for cuML benchmarks</p>
<p>The main entry point for consumers is gen_data, which
wraps the underlying data generators.</p>
<p>Notes when writing new generators:</p>
<dl class="simple">
<dt>Each generator is a function that accepts:</dt><dd><ul class="simple">
<li><p>n_samples (set to 0 for ‘default’)</p></li>
<li><p>n_features (set to 0 for ‘default’)</p></li>
<li><p>random_state</p></li>
<li><p>(and optional generator-specific parameters)</p></li>
</ul>
</dd>
</dl>
<p>The function should return a 2-tuple (X, y), where X is a Pandas
dataframe and y is a Pandas series. If the generator does not produce
labels, it can return (X, None)</p>
<p>A set of helper functions (convert_*) can convert these to alternative
formats. Future revisions may support generating cudf dataframes or
GPU arrays directly instead.</p>
<dl class="py function">
<dt id="cuml.benchmark.datagen.gen_data">
<code class="sig-prename descclassname">cuml.benchmark.datagen.</code><code class="sig-name descname">gen_data</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset_name</span></em>, <em class="sig-param"><span class="n">dataset_format</span></em>, <em class="sig-param"><span class="n">n_samples</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">n_features</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">42</span></em>, <em class="sig-param"><span class="n">test_fraction</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/datagen.py#L296"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.datagen.gen_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tuple of data from the specified generator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dataset_name</strong><span class="classifier">str</span></dt><dd><p>Dataset to use. Can be a synthetic generator (blobs or regression)
or a specified dataset (higgs currently, others coming soon)</p>
</dd>
<dt><strong>dataset_format</strong><span class="classifier">str</span></dt><dd><p>Type of data to return. (One of cudf, numpy, pandas, gpuarray)</p>
</dd>
<dt><strong>n_samples</strong><span class="classifier">int</span></dt><dd><p>Number of samples to include in training set (regardless of test split)</p>
</dd>
<dt><strong>test_fraction</strong><span class="classifier">float</span></dt><dd><p>Fraction of the dataset to partition randomly into the test set.
If this is 0.0, no test set will be created.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.benchmark.datagen.load_higgs">
<code class="sig-prename descclassname">cuml.benchmark.datagen.</code><code class="sig-name descname">load_higgs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/benchmark/datagen.py#L141"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.benchmark.datagen.load_higgs" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Higgs Boson dataset as an X, y tuple of dataframes.</p>
</dd></dl>
</div></blockquote>
</div>
</div>
<div class="section" id="regression-and-classification">
<h2>Regression and Classification<a class="headerlink" href="#regression-and-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.LinearRegression">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">LinearRegression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">algorithm</span><span class="o">=</span><span class="default_value">'eig'</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>LinearRegression is a simple machine learning model where the response y is
modelled by a linear combination of the predictors in X.</p>
<p>cuML’s LinearRegression expects either a cuDF DataFrame or a NumPy matrix
and provides 2 algorithms SVD and Eig to fit a linear model. SVD is more
stable, but Eig (default) is much faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algorithm</strong><span class="classifier">‘eig’ or ‘svd’ (default = ‘eig’)</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much
faster.
SVD is slower, but guaranteed to be stable.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, LinearRegression tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>This parameter is ignored when <cite>fit_intercept</cite> is set to False.
If True, the predictors in X will be normalized by dividing by it’s
L2 norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>LinearRegression suffers from multicollinearity (when columns are
correlated with each other), and variance explosions from outliers.
Consider using Ridge Regression to fix the multicollinearity problem, and
consider maybe first DBSCAN to remove the outliers, or statistical analysis
to filter possible outliers.</p>
<p><strong>Applications of LinearRegression</strong></p>
<blockquote>
<div><p>LinearRegression is used in regression tasks where one wants to predict
say sales or house prices. It is also used in extrapolation or time
series tasks, dynamic systems modelling and many other machine learning
tasks. This model should be first tried if the machine learning problem
is a regression task (predicting a continuous variable).</p>
</div></blockquote>
<p>For additional information, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">scikitlearn’s OLS documentation</a>.</p>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/cuml/blob/branch-0.15/notebooks/linear_regression_demo.ipynb">the OLS notebook</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                      <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">"eig"</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Predictions:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">1.0000001</span>
            <span class="mi">1</span> <span class="mf">1.9999998</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>

<span class="n">Predictions</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">15.999999</span>
            <span class="mi">1</span> <span class="mf">14.999999</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LinearRegression.fit" title="cuml.LinearRegression.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LinearRegression.get_param_names" title="cuml.LinearRegression.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LinearRegression.predict" title="cuml.LinearRegression.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts <cite>y</cite> values for <cite>X</cite>.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.LinearRegression.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/linear_regression.pyx#L228"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LinearRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.LinearRegression.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/linear_regression.pyx#L353"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LinearRegression.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.LinearRegression.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/linear_regression.pyx#L307"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LinearRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts <cite>y</cite> values for <cite>X</cite>.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="logistic-regression">
<h3>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.LogisticRegression">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">LogisticRegression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">penalty</span><span class="o">=</span><span class="default_value">'l2'</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">C</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">class_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">linesearch_max_iter</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">solver</span><span class="o">=</span><span class="default_value">'qn'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LogisticRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>LogisticRegression is a linear model that is used to model probability of
occurrence of certain events, for example probability of success or fail of
an event.</p>
<p>cuML’s LogisticRegression can take array-like objects, either in host as
NumPy arrays or in device (as Numba or <cite>__cuda_array_interface__</cite>
compliant), in addition to cuDF objects.
It provides both single-class (using sigmoid loss) and multiple-class
(using softmax loss) variants, depending on the input variables</p>
<p>Only one solver option is currently available: Quasi-Newton (QN)
algorithms. Even though it is presented as a single option, this solver
resolves to two different algorithms underneath:</p>
<ul class="simple">
<li><p>Orthant-Wise Limited Memory Quasi-Newton (OWL-QN) if there is l1
regularization</p></li>
<li><p>Limited Memory BFGS (L-BFGS) otherwise.</p></li>
</ul>
<p>Note that, just like in Scikit-learn, the bias will not be regularized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>penalty: ‘none’, ‘l1’, ‘l2’, ‘elasticnet’ (default = ‘l2’)</strong></dt><dd><p>Used to specify the norm used in the penalization.
If ‘none’ or ‘l2’ are selected, then L-BFGS solver will be used.
If ‘l1’ is selected, solver OWL-QN will be used.
If ‘elasticnet’ is selected, OWL-QN will be used if l1_ratio &gt; 0,
otherwise L-BFGS will be used.</p>
</dd>
<dt><strong>tol: float (default = 1e-4)</strong></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>C: float (default = 1.0)</strong></dt><dd><p>Inverse of regularization strength; must be a positive float.</p>
</dd>
<dt><strong>fit_intercept: boolean (default = True)</strong></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>class_weight: None</strong></dt><dd><p>Custom class weighs are currently not supported.</p>
</dd>
<dt><strong>max_iter: int (default = 1000)</strong></dt><dd><p>Maximum number of iterations taken for the solvers to converge.</p>
</dd>
<dt><strong>linesearch_max_iter: int (default = 50)</strong></dt><dd><p>Max number of linesearch iterations per outer iteration used in the
lbfgs and owl QN solvers.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>l1_ratio: float or None, optional (default=None)</strong></dt><dd><p>The Elastic-Net mixing parameter, with <cite>0 &lt;= l1_ratio &lt;= 1</cite></p>
</dd>
<dt><strong>solver: ‘qn’, ‘lbfgs’, ‘owl’ (default=’qn’).</strong></dt><dd><p>Algorithm to use in the optimization problem. Currently only <cite>qn</cite> is
supported, which automatically selects either L-BFGS or OWL-QN
depending on the conditions of the l1 regularization described
above. Options ‘lbfgs’ and ‘owl’ are just convenience values that
end up using the same solver following the same rules.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>cuML’s LogisticRegression uses a different solver that the equivalent
Scikit-learn, except when there is no penalty and <cite>solver=lbfgs</cite> is
used in Scikit-learn. This can cause (smaller) differences in the
coefficients and predictions of the model, similar to
using different solvers in Scikit-learn.</p>
<p>For additional information, see Scikit-learn’s LogistRegression
&lt;<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>&gt;`_.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Both import methods supported</span>
<span class="c1"># from cuml import LogisticRegression</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Predictions:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>
            <span class="mf">0.22309814</span>
            <span class="mf">0.21012752</span>
<span class="n">Intercept</span><span class="p">:</span>
            <span class="o">-</span><span class="mf">0.7548761</span>
<span class="n">Predictions</span><span class="p">:</span>
            <span class="mi">0</span>    <span class="mf">0.0</span>
            <span class="mi">1</span>    <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_: dev array, dim (n_classes, n_features) or (n_classes, n_features+1)</strong></dt><dd><p>The estimated coefficients for the linear regression model.
Note: this includes the intercept as the last column if fit_intercept
is True</p>
</dd>
<dt><strong>intercept_: device array (n_classes, 1)</strong></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LogisticRegression.decision_function" title="cuml.LogisticRegression.decision_function"><code class="xref py py-obj docutils literal notranslate"><span class="pre">decision_function</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Gives confidence score for X</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LogisticRegression.fit" title="cuml.LogisticRegression.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LogisticRegression.get_param_names" title="cuml.LogisticRegression.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LogisticRegression.predict" title="cuml.LogisticRegression.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LogisticRegression.predict_log_proba" title="cuml.LogisticRegression.predict_log_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_log_proba</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the log class probabilities for each class in X</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LogisticRegression.predict_proba" title="cuml.LogisticRegression.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the class probabilities for each class in X</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.LogisticRegression.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/logistic_regression.pyx#L314"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LogisticRegression.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Gives confidence score for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the decision_function method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>score</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_classes)</span></dt><dd><p>Confidence score</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.LogisticRegression.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/logistic_regression.pyx#L261"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LogisticRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.LogisticRegression.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/logistic_regression.pyx#L403"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LogisticRegression.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.LogisticRegression.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/logistic_regression.pyx#L328"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LogisticRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.LogisticRegression.predict_log_proba">
<code class="sig-name descname">predict_log_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/logistic_regression.pyx#L357"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LogisticRegression.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the log class probabilities for each class in X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict_log_proba method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_classes)</span></dt><dd><p>Logaright of predicted                                                        class probabilities</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.LogisticRegression.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/logistic_regression.pyx#L341"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.LogisticRegression.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the class probabilities for each class in X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict_proba method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_classes)</span></dt><dd><p>Predicted class                                                        probabilities</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="ridge-regression">
<h3>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.Ridge">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">Ridge</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">solver</span><span class="o">=</span><span class="default_value">'eig'</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge" title="Permalink to this definition">¶</a></dt>
<dd><p>Ridge extends LinearRegression by providing L2 regularization on the
coefficients when predicting response y with a linear combination of the
predictors in X. It can reduce the variance of the predictors, and improves
the conditioning of the problem.</p>
<p>cuML’s Ridge can take array-like objects, either in host as
NumPy arrays or in device (as Numba or <cite>__cuda_array_interface__</cite>
compliant), in addition to cuDF objects. It provides 3
algorithms: SVD, Eig and CD to fit a linear model. In general SVD uses
significantly more memory and is slower than Eig. If using CUDA 10.1,
the memory difference is even bigger than in the other supported CUDA
versions. However, SVD is more stable than Eig (default). CD uses
Coordinate Descent and can be faster when data is large.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Regularization strength - must be a positive float. Larger values
specify stronger regularization. Array input will be supported later.</p>
</dd>
<dt><strong>solver</strong><span class="classifier">{‘eig’, ‘svd’, ‘cd’} (default = ‘eig’)</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much
faster.
SVD is slower, but guaranteed to be stable.
CD or Coordinate Descent is very fast and is suitable for large
problems.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Ridge tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Ridge provides L2 regularization. This means that the coefficients can
shrink to become very small, but not zero. This can cause issues of
interpretability on the coefficients.
Consider using Lasso, or thresholding small coefficients to zero.</p>
<p><strong>Applications of Ridge</strong></p>
<blockquote>
<div><p>Ridge Regression is used in the same way as LinearRegression, but does
not suffer from multicollinearity issues.  Ridge is used in insurance
premium prediction, stock market analysis and much more.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Scikit-learn’s Ridge Regression</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-5</span><span class="p">])</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
              <span class="n">solver</span> <span class="o">=</span> <span class="s2">"eig"</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_ridge</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_ridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Predictions:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">1.0000001</span>
            <span class="mi">1</span> <span class="mf">1.9999998</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">15.999999</span>
            <span class="mi">1</span> <span class="mf">14.999999</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Ridge.fit" title="cuml.Ridge.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Ridge.get_param_names" title="cuml.Ridge.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Ridge.predict" title="cuml.Ridge.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.Ridge.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/ridge.pyx#L260"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.Ridge.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.Ridge.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/ridge.pyx#L393"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.Ridge.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.Ridge.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/ridge.pyx#L348"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.Ridge.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="lasso-regression">
<h3>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.Lasso">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">Lasso</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">selection</span><span class="o">=</span><span class="default_value">'cyclic'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Lasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Lasso extends LinearRegression by providing L1 regularization on the
coefficients when predicting response y with a linear combination of the
predictors in X. It can zero some of the coefficients for feature
selection and improves the conditioning of the problem.</p>
<p>cuML’s Lasso can take array-like objects, either in host as
NumPy arrays or in device (as Numba or <cite>__cuda_array_interface__</cite>
compliant), in addition to cuDF objects. It uses coordinate descent to fit
a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Constant that multiplies the L1 term.
alpha = 0 is equivalent to an ordinary least square, solved by the
LinearRegression class.
For numerical reasons, using alpha = 0 with the Lasso class is not
advised.
Given this, you should use the LinearRegression class.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Lasso tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than
tol, the optimization code checks the dual gap for optimality and
continues until it is smaller than tol.</p>
</dd>
<dt><strong>selection</strong><span class="classifier">{‘cyclic’, ‘random’} (default=’cyclic’)</span></dt><dd><p>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default.
This (setting to ‘random’) often leads to significantly faster
convergence especially when tol is higher than 1e-4.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">scikitlearn’s Lasso</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">ls</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_lasso</span> <span class="o">=</span> <span class="n">ls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_lasso</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">0.85</span>
            <span class="mi">1</span> <span class="mf">0.0</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">0.149999</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">2.7</span>
            <span class="mi">1</span> <span class="mf">1.85</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Lasso.fit" title="cuml.Lasso.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Lasso.get_param_names" title="cuml.Lasso.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Lasso.predict" title="cuml.Lasso.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.Lasso.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/lasso.pyx#L178"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.Lasso.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.Lasso.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/lasso.pyx#L200"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.Lasso.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.Lasso.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/lasso.pyx#L192"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.Lasso.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="elasticnet-regression">
<h3>ElasticNet Regression<a class="headerlink" href="#elasticnet-regression" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.ElasticNet">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">ElasticNet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">selection</span><span class="o">=</span><span class="default_value">'cyclic'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ElasticNet" title="Permalink to this definition">¶</a></dt>
<dd><p>ElasticNet extends LinearRegression with combined L1 and L2 regularizations
on the coefficients when predicting response y with a linear combination of
the predictors in X. It can reduce the variance of the predictors, force
some coefficients to be small, and improves the conditioning of the
problem.</p>
<p>cuML’s ElasticNet an array-like object or cuDF DataFrame, uses coordinate
descent to fit a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Constant that multiplies the L1 term.
alpha = 0 is equivalent to an ordinary least square, solved by the
LinearRegression object.
For numerical reasons, using alpha = 0 with the Lasso object is not
advised.
Given this, you should use the LinearRegression object.</p>
</dd>
<dt><strong>l1_ratio: float (default = 0.5)</strong></dt><dd><p>The ElasticNet mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is
an L1 penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Lasso tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than
tol, the optimization code checks the dual gap for optimality and
continues until it is smaller than tol.</p>
</dd>
<dt><strong>selection</strong><span class="classifier">{‘cyclic’, ‘random’} (default=’cyclic’)</span></dt><dd><p>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default.
This (setting to ‘random’) often leads to significantly faster
convergence especially when tol is higher than 1e-4.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">scikitlearn’s ElasticNet</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">enet</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_enet</span> <span class="o">=</span> <span class="n">enet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_enet</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_enet</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_enet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">0.448408</span>
            <span class="mi">1</span> <span class="mf">0.443341</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">0.1082506</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">3.67018</span>
            <span class="mi">1</span> <span class="mf">3.22177</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ElasticNet.fit" title="cuml.ElasticNet.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ElasticNet.get_param_names" title="cuml.ElasticNet.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ElasticNet.predict" title="cuml.ElasticNet.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts <cite>y</cite> values for <cite>X</cite>.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.ElasticNet.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/elastic_net.pyx#L209"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ElasticNet.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ElasticNet.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/elastic_net.pyx#L231"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ElasticNet.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.ElasticNet.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/elastic_net.pyx#L223"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ElasticNet.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts <cite>y</cite> values for <cite>X</cite>.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="mini-batch-sgd-classifier">
<h3>Mini Batch SGD Classifier<a class="headerlink" href="#mini-batch-sgd-classifier" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.MBSGDClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">MBSGDClassifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">'hinge'</span></em>, <em class="sig-param"><span class="n">penalty</span><span class="o">=</span><span class="default_value">'l2'</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.15</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">epochs</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">'constant'</span></em>, <em class="sig-param"><span class="n">eta0</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">power_t</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.MBSGDClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear models (linear SVM, logistic regression, or linear regression)
fitted by minimizing a regularized empirical loss with mini-batch SGD.
The MBSGD Classifier implementation is experimental and and it uses a
different algorithm than sklearn’s SGDClassifier. In order to improve
the results obtained from cuML’s MBSGDClassifier:
* Reduce the batch size
* Increase the eta0
* Increase the number of iterations
Since cuML is analyzing the data in batches using a small eta0 might
not let the model learn as much as scikit learn does. Furthermore,
decreasing the batch size might seen an increase in the time required
to fit the model.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>loss</strong><span class="classifier">{‘hinge’, ‘log’, ‘squared_loss’} (default = ‘squared_loss’)</span></dt><dd><p>‘hinge’ uses linear SVM</p>
<p>‘log’ uses logistic regression</p>
<p>‘squared_loss’ uses linear regression</p>
</dd>
<dt><strong>penalty: {‘none’, ‘l1’, ‘l2’, ‘elasticnet’} (default = ‘none’)</strong></dt><dd><p>‘none’ does not perform any regularization</p>
<p>‘l1’ performs L1 norm (Lasso) which minimizes the sum of the abs value
of coefficients</p>
<p>‘l2’ performs L2 norm (Ridge) which minimizes the sum of the square of
the coefficients</p>
<p>‘elasticnet’ performs Elastic Net regularization which is a weighted
average of L1 and L2 norms</p>
</dd>
<dt><strong>alpha: float (default = 0.0001)</strong></dt><dd><p>The constant value which decides the degree of regularization</p>
</dd>
<dt><strong>l1_ratio: float (default=0.15)</strong></dt><dd><p>The l1_ratio is used only when <cite>penalty = elasticnet</cite>. The value for
l1_ratio should be <cite>0 &lt;= l1_ratio &lt;= 1</cite>. When <cite>l1_ratio = 0</cite> then the
<cite>penalty = ‘l2’</cite> and if <cite>l1_ratio = 1</cite> then <cite>penalty = ‘l1’</cite></p>
</dd>
<dt><strong>batch_size: int (default = 32)</strong></dt><dd><p>It sets the number of samples that will be included in each batch.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The number of times the model should iterate through the entire dataset
during training (default = 1000)</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>True, shuffles the training data after each epoch
False, does not shuffle the training data after each epoch</p>
</dd>
<dt><strong>eta0</strong><span class="classifier">float (default = 0.001)</span></dt><dd><p>Initial learning rate</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The exponent used for calculating the invscaling learning rate</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">{‘optimal’, ‘constant’, ‘invscaling’, ‘adaptive’}</span></dt><dd><p>(default = ‘constant’)</p>
<p><cite>optimal</cite> option will be supported in a future version</p>
<p><cite>constant</cite> keeps the learning rate constant</p>
<p><cite>adaptive</cite> changes the learning rate if the training loss or the
validation accuracy does not improve for <cite>n_iter_no_change</cite> epochs.
The old learning rate is generally divided by 5</p>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int (default = 5)</span></dt><dd><p>the number of epochs to train without any imporvement in the model</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html">scikitlearn’s SGDClassifier</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">MBSGDClassifier</span> <span class="k">as</span> <span class="n">cumlMBSGDClassifier</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">pred_data</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">cu_mbsgd_classifier</span> <span class="o">=</span> <span class="n">cumlMBSGClassifier</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="s1">'constant'</span><span class="p">,</span>
                                         <span class="n">eta0</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                                         <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                         <span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span>
                                         <span class="n">loss</span><span class="o">=</span><span class="s1">'squared_loss'</span><span class="p">,</span>
                                         <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">cu_mbsgd_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">cu_pred</span> <span class="o">=</span> <span class="n">cu_mbsgd_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_data</span><span class="p">)</span><span class="o">.</span><span class="n">to_array</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" cuML intercept : "</span><span class="p">,</span> <span class="n">cu_mbsgd_classifier</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" cuML coef : "</span><span class="p">,</span> <span class="n">cu_mbsgd_classifier</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"cuML predictions : "</span><span class="p">,</span> <span class="n">cu_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cuML</span> <span class="n">intercept</span> <span class="p">:</span>  <span class="mf">0.7150013446807861</span>
<span class="n">cuML</span> <span class="n">coef</span> <span class="p">:</span>  <span class="mi">0</span>    <span class="mf">0.27320495</span>
            <span class="mi">1</span>     <span class="mf">0.1875956</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">float32</span>
<span class="n">cuML</span> <span class="n">predictions</span> <span class="p">:</span>  <span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MBSGDClassifier.fit" title="cuml.MBSGDClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.MBSGDClassifier.get_param_names" title="cuml.MBSGDClassifier.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MBSGDClassifier.predict" title="cuml.MBSGDClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.MBSGDClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/mbsgd_classifier.pyx#L178"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MBSGDClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MBSGDClassifier.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/mbsgd_classifier.pyx#L203"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MBSGDClassifier.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.MBSGDClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/mbsgd_classifier.pyx#L192"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MBSGDClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="mini-batch-sgd-regressor">
<h3>Mini Batch SGD Regressor<a class="headerlink" href="#mini-batch-sgd-regressor" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.MBSGDRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">MBSGDRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">'squared_loss'</span></em>, <em class="sig-param"><span class="n">penalty</span><span class="o">=</span><span class="default_value">'l2'</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.15</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">epochs</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">'constant'</span></em>, <em class="sig-param"><span class="n">eta0</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">power_t</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.MBSGDRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear regression model fitted by minimizing a
regularized empirical loss with mini-batch SGD.
The MBSGD Regressor implementation is experimental and and it uses a
different algorithm than sklearn’s SGDClassifier. In order to improve
the results obtained from cuML’s MBSGD Regressor:
* Reduce the batch size
* Increase the eta0
* Increase the number of iterations
Since cuML is analyzing the data in batches using a small eta0 might
not let the model learn as much as scikit learn does. Furthermore,
decreasing the batch size might seen an increase in the time required
to fit the model.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>loss</strong><span class="classifier">‘squared_loss’ (default = ‘squared_loss’)</span></dt><dd><p>‘squared_loss’ uses linear regression</p>
</dd>
<dt><strong>penalty: ‘none’, ‘l1’, ‘l2’, ‘elasticnet’ (default = ‘none’)</strong></dt><dd><p>‘none’ does not perform any regularization
‘l1’ performs L1 norm (Lasso) which minimizes the sum of the abs value
of coefficients
‘l2’ performs L2 norm (Ridge) which minimizes the sum of the square of
the coefficients
‘elasticnet’ performs Elastic Net regularization which is a weighted
average of L1 and L2 norms</p>
</dd>
<dt><strong>alpha: float (default = 0.0001)</strong></dt><dd><p>The constant value which decides the degree of regularization</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>l1_ratio: float (default=0.15)</strong></dt><dd><p>The l1_ratio is used only when <cite>penalty = elasticnet</cite>. The value for
l1_ratio should be <cite>0 &lt;= l1_ratio &lt;= 1</cite>. When <cite>l1_ratio = 0</cite> then the
<cite>penalty = ‘l2’</cite> and if <cite>l1_ratio = 1</cite> then <cite>penalty = ‘l1’</cite></p>
</dd>
<dt><strong>batch_size: int (default = 32)</strong></dt><dd><p>It sets the number of samples that will be included in each batch.</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The number of times the model should iterate through the entire dataset
during training (default = 1000)</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>True, shuffles the training data after each epoch
False, does not shuffle the training data after each epoch</p>
</dd>
<dt><strong>eta0</strong><span class="classifier">float (default = 0.001)</span></dt><dd><p>Initial learning rate</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The exponent used for calculating the invscaling learning rate</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">{‘optimal’, ‘constant’, ‘invscaling’, ‘adaptive’}</span></dt><dd><p>(default = ‘constant’)</p>
<p><cite>optimal</cite> option will be supported in a future version</p>
<p><cite>constant</cite> keeps the learning rate constant</p>
<p><cite>adaptive</cite> changes the learning rate if the training loss or the
validation accuracy does not improve for <cite>n_iter_no_change</cite> epochs.
The old learning rate is generally divided by 5</p>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int (default = 5)</span></dt><dd><p>the number of epochs to train without any imporvement in the model</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html">scikitlearn’s SGDRegressor</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">MBSGDRegressor</span> <span class="k">as</span> <span class="n">cumlMBSGDRegressor</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">pred_data</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">cu_mbsgd_regressor</span> <span class="o">=</span> <span class="n">cumlMBSGDRegressor</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="s1">'constant'</span><span class="p">,</span>
                                        <span class="n">eta0</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                                        <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                        <span class="n">penalty</span><span class="o">=</span><span class="s1">'l2'</span><span class="p">,</span>
                                        <span class="n">loss</span><span class="o">=</span><span class="s1">'squared_loss'</span><span class="p">,</span>
                                        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">cu_mbsgd_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">cu_pred</span> <span class="o">=</span> <span class="n">cu_mbsgd_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_data</span><span class="p">)</span><span class="o">.</span><span class="n">to_array</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" cuML intercept : "</span><span class="p">,</span> <span class="n">cu_mbsgd_regressor</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" cuML coef : "</span><span class="p">,</span> <span class="n">cu_mbsgd_regressor</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"cuML predictions : "</span><span class="p">,</span> <span class="n">cu_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cuML</span> <span class="n">intercept</span> <span class="p">:</span>  <span class="mf">0.7150013446807861</span>
<span class="n">cuML</span> <span class="n">coef</span> <span class="p">:</span>  <span class="mi">0</span>    <span class="mf">0.27320495</span>
            <span class="mi">1</span>     <span class="mf">0.1875956</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">float32</span>
<span class="n">cuML</span> <span class="n">predictions</span> <span class="p">:</span>  <span class="p">[</span><span class="mf">2.4725943</span> <span class="mf">2.1993892</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MBSGDRegressor.fit" title="cuml.MBSGDRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.MBSGDRegressor.get_param_names" title="cuml.MBSGDRegressor.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MBSGDRegressor.predict" title="cuml.MBSGDRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.MBSGDRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/mbsgd_regressor.pyx#L174"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MBSGDRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MBSGDRegressor.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/mbsgd_regressor.pyx#L197"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MBSGDRegressor.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.MBSGDRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/linear_model/mbsgd_regressor.pyx#L187"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MBSGDRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="mutinomial-naive-bayes">
<h3>Mutinomial Naive Bayes<a class="headerlink" href="#mutinomial-naive-bayes" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.MultinomialNB">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">MultinomialNB</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L118"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB" title="Permalink to this definition">¶</a></dt>
<dd><p>Naive Bayes classifier for multinomial models</p>
<p>The multinomial Naive Bayes classifier is suitable for classification
with discrete features (e.g., word counts for text classification).</p>
<p>The multinomial distribution normally requires integer feature counts.
However, in practice, fractional counts such as tf-idf may also work.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float</span></dt><dd><p>Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</p>
</dd>
<dt><strong>fit_prior</strong><span class="classifier">boolean</span></dt><dd><p>Whether to learn class prior probabilities or no. If false, a uniform
prior will be used.</p>
</dd>
<dt><strong>class_prior</strong><span class="classifier">array-like, size (n_classes)</span></dt><dd><p>Prior probabilities of the classes. If specified, the priors are not
adjusted according to the data.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>While cuML only provides the multinomial version currently, the other
variants are planned to be included soon. Refer to the corresponding Github
<a class="reference external" href="https://github.com/rapidsai/cuml/issues/1666">issue</a> for updates.</p>
<p class="rubric">Examples</p>
<p>Load the 20 newsgroups dataset from Scikit-learn and train a
Naive Bayes classifier.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="kn">import</span> <span class="nn">cupyx</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="kn">from</span> <span class="nn">cuml.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>

<span class="c1"># Load corpus</span>

<span class="n">twenty_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'train'</span><span class="p">,</span>
                        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Turn documents into term frequency vectors</span>

<span class="n">count_vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Put feature vectors and labels on the GPU</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cupyx</span><span class="o">.</span><span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">tocsr</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># Train model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Compute accuracy on training set</span>

<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mf">0.9244298934936523</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MultinomialNB.fit" title="cuml.MultinomialNB.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y[, sample_weight])</p></td>
<td><p>Fit Naive Bayes classifier according to X, y</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.MultinomialNB.get_param_names" title="cuml.MultinomialNB.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p>Returns a list of hyperparameter names owned by this class.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MultinomialNB.partial_fit" title="cuml.MultinomialNB.partial_fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">partial_fit</span></code></a>(X, y[, classes, sample_weight])</p></td>
<td><p>Incremental fit on a batch of samples.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.MultinomialNB.predict" title="cuml.MultinomialNB.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X)</p></td>
<td><p>Perform classification on an array of test vectors X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MultinomialNB.predict_log_proba" title="cuml.MultinomialNB.predict_log_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_log_proba</span></code></a>(X)</p></td>
<td><p>Return log-probability estimates for the test vector X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.MultinomialNB.predict_proba" title="cuml.MultinomialNB.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(X)</p></td>
<td><p>Return probability estimates for the test vector X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.MultinomialNB.score" title="cuml.MultinomialNB.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X, y[, sample_weight])</p></td>
<td><p>Return the mean accuracy on the given test data and labels.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.MultinomialNB.update_log_probs" title="cuml.MultinomialNB.update_log_probs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update_log_probs</span></code></a>()</p></td>
<td><p>Updates the log probabilities.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.MultinomialNB.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L235"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit Naive Bayes classifier according to X, y</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L599"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of hyperparameter names owned by this class. It is
expected that every child class overrides this method and appends its
extra set of parameters that it in-turn owns. This is to simplify the
implementation of <cite>get_params</cite> and <cite>set_params</cite> methods.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L307"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Incremental fit on a batch of samples.</p>
<p>This method is expected to be called several times consecutively on
different chunks of a dataset so as to implement out-of-core or online
learning.</p>
<p>This is especially useful when the whole dataset is too big to fit in
memory at once.</p>
<p>This method has some performance overhead hence it is better to call
partial_fit on chunks of data that are as large as possible (as long
as fitting in the memory budget) to hide the overhead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, cupy sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>Training vectors, where n_samples is the number of samples and
n_features is the number of features</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples) Target values.</span></dt><dd></dd>
<dt><strong>classes</strong><span class="classifier">array-like of shape (n_classes)</span></dt><dd><p>List of all the classes that can possibly appear in the y
vector. Must be provided at the first call to partial_fit,
can be omitted in subsequent calls.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples)</span></dt><dd><p>Weights applied to individual samples (1. for
unweighted). Currently sample weight is ignored</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L348"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Perform classification on an array of test vectors X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>y_hat</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_rows, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.predict_log_proba">
<code class="sig-name descname">predict_log_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L386"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Return log-probability estimates for the test vector X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>C</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_rows, 1)</span></dt><dd><p>Returns the log-probability of the samples for each class in the             model. The columns correspond to the classes in sorted order, as             they appear in the attribute <cite>classes_</cite>.</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L440"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Return probability estimates for the test vector X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>C</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_rows, 1)</span></dt><dd><p>Returns the probability of the samples for each class in the             model. The columns correspond to the classes in sorted order, as             they appear in the attribute <cite>classes_</cite>.</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L457"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.score" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Return the mean accuracy on the given test data and labels.</p>
<p>In multi-label classification, this is the subset accuracy which is a
harsh metric since you require for each sample that each label set be
correctly predicted.</p>
<p>Currently, sample weight is ignored</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>Mean accuracy of                                        self.predict(X) with respect to y.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.MultinomialNB.update_log_probs">
<code class="sig-name descname">update_log_probs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/naive_bayes/naive_bayes.py#L296"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.MultinomialNB.update_log_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the log probabilities. This enables lazy update for
applications like distributed Naive Bayes, so that the model
can be updated incrementally without incurring this cost each
time.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="stochastic-gradient-descent">
<h3>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.SGD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">SGD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">'squared_loss'</span></em>, <em class="sig-param"><span class="n">penalty</span><span class="o">=</span><span class="default_value">'none'</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.15</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">epochs</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">'constant'</span></em>, <em class="sig-param"><span class="n">eta0</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">power_t</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Stochastic Gradient Descent is a very common machine learning algorithm
where one optimizes some cost function via gradient steps. This makes SGD
very attractive for large problems when the exact solution is hard or even
impossible to find.</p>
<p>cuML’s SGD algorithm accepts a numpy matrix or a cuDF DataFrame as the
input dataset. The SGD algorithm currently works with linear regression,
ridge regression and SVM models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">‘hinge’, ‘log’, ‘squared_loss’ (default = ‘squared_loss’)</span></dt><dd><p>‘hinge’ uses linear SVM
‘log’ uses logistic regression
‘squared_loss’ uses linear regression</p>
</dd>
<dt><strong>penalty: ‘none’, ‘l1’, ‘l2’, ‘elasticnet’ (default = ‘none’)</strong></dt><dd><p>‘none’ does not perform any regularization
‘l1’ performs L1 norm (Lasso) which minimizes the sum of the abs value
of coefficients
‘l2’ performs L2 norm (Ridge) which minimizes the sum of the square of
the coefficients
‘elasticnet’ performs Elastic Net regularization which is a weighted
average of L1 and L2 norms</p>
</dd>
<dt><strong>alpha: float (default = 0.0001)</strong></dt><dd><p>The constant value which decides the degree of regularization</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The number of times the model should iterate through the entire dataset
during training (default = 1000)</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>True, shuffles the training data after each epoch
False, does not shuffle the training data after each epoch</p>
</dd>
<dt><strong>eta0</strong><span class="classifier">float (default = 0.001)</span></dt><dd><p>Initial learning rate</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The exponent used for calculating the invscaling learning rate</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">‘optimal’, ‘constant’, ‘invscaling’,                     ‘adaptive’ (default = ‘constant’)</span></dt><dd><p>optimal option supported in the next version
constant keeps the learning rate constant
adaptive changes the learning rate if the training loss or the
validation accuracy does not improve for n_iter_no_change epochs.
The old learning rate is generally divide by 5</p>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int (default = 5)</span></dt><dd><p>the number of epochs to train without any imporvement in the model</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.solvers</span> <span class="kn">import</span> <span class="n">SGD</span> <span class="k">as</span> <span class="n">cumlSGD</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">pred_data</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">cu_sgd</span> <span class="o">=</span> <span class="n">cumlSGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lrate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
<span class="n">cu_sgd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">cu_pred</span> <span class="o">=</span> <span class="n">cu_sgd</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_data</span><span class="p">)</span><span class="o">.</span><span class="n">to_array</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" cuML intercept : "</span><span class="p">,</span> <span class="n">cu_sgd</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" cuML coef : "</span><span class="p">,</span> <span class="n">cu_sgd</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"cuML predictions : "</span><span class="p">,</span> <span class="n">cu_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cuML</span> <span class="n">intercept</span> <span class="p">:</span>  <span class="mf">0.004561662673950195</span>
<span class="n">cuML</span> <span class="n">coef</span> <span class="p">:</span>  <span class="mi">0</span>      <span class="mf">0.9834546</span>
            <span class="mi">1</span>    <span class="mf">0.010128272</span>
           <span class="n">dtype</span><span class="p">:</span> <span class="n">float32</span>
<span class="n">cuML</span> <span class="n">predictions</span> <span class="p">:</span>  <span class="p">[</span><span class="mf">3.0055666</span> <span class="mf">2.0221121</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.SGD.fit" title="cuml.SGD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.SGD.get_param_names" title="cuml.SGD.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.SGD.predict" title="cuml.SGD.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.SGD.predictClass" title="cuml.SGD.predictClass"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predictClass</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.SGD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/sgd.pyx#L307"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.SGD.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.SGD.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/sgd.pyx#L493"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.SGD.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.SGD.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/sgd.pyx#L398"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.SGD.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.SGD.predictClass">
<code class="sig-name descname">predictClass</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/sgd.pyx#L448"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.SGD.predictClass" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predictClass method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.ensemble.RandomForestClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.ensemble.</code><code class="sig-name descname">RandomForestClassifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">split_criterion</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Random Forest classifier model which fits multiple decision
tree classifiers in an ensemble.</p>
<p>Note that the underlying algorithm for tree node splits differs from that
used in scikit-learn. By default, the cuML Random Forest uses a
histogram-based algorithms to determine splits, rather than an exact
count. You can tune the size of the histograms with the n_bins parameter.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>This is an early release of the cuML</dt><dd><p>Random Forest code. It contains a few known limitations:</p>
</dd>
</dl>
<ul class="simple">
<li><p>GPU-based inference is only supported if the model was trained
with 32-bit (float32) datatypes. CPU-based inference may be used
in this case as a slower fallback.</p></li>
<li><p>Very deep / very wide models may exhaust available GPU memory.
Future versions of cuML will provide an alternative algorithm to
reduce memory consumption.</p></li>
<li><p>While training the model for multi class classification problems,
using deep trees or <cite>max_features=1.0</cite> provides better performance.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 100)</span></dt><dd><p>Number of trees in the forest. (Default changed to 100 in cuML 0.11)</p>
</dd>
<dt><strong>split_criterion</strong><span class="classifier">The criterion used to split nodes.</span></dt><dd><p>0 for GINI, 1 for ENTROPY
2 and 3 not valid for classification
(default = 0)</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The algorithm to determine how nodes are split in the tree.
0 for HIST and 1 for GLOBAL_QUANTILE. HIST curently uses a slower
tree-building algorithm so GLOBAL_QUANTILE is recommended for most
cases.</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If True, each tree in the forest is built
on a bootstrapped sample with replacement.
If False, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = 16)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure),
if -1. Unlimited depth is not supported.
<em>Note that this default differs from scikit-learn’s
random forest, which defaults to unlimited depth.</em></p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited,
if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int, float, or string (default = ‘auto’)</span></dt><dd><p>Ratio of number of features (columns) to consider per node split.
If int then max_features/n_features.
If float then max_features is used as a fraction.
If ‘auto’ then max_features=1/sqrt(n_features).
If ‘sqrt’ then max_features=1/sqrt(n_features).
If ‘log2’ then max_features=log2(n_features)/n_features.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int or float (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed to split a node.
If int then number of sample rows.
If float the min_rows_per_sample*n_rows</p>
</dd>
<dt><strong>min_impurity_decrease</strong><span class="classifier">float (default = 0.0)</span></dt><dd><p>Minimum decrease in impurity requried for
node to be spilt.</p>
</dd>
<dt><strong>quantile_per_tree</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Whether quantile is computed for individal trees in RF.
Only relevant for GLOBAL_QUANTILE split_algo.</p>
</dd>
<dt><strong>use_experimental_backend</strong><span class="classifier">boolean (default = False)</span></dt><dd><dl class="simple">
<dt>If set to true and  following conditions are also met, experimental</dt><dd><dl class="simple">
<dt>decision tree training implementation would be used:</dt><dd><p>split_algo = 1 (GLOBAL_QUANTILE)
0 &lt; max_depth &lt; 14
max_features = 1.0 (Feature sub-sampling disabled)
quantile_per_tree = false (No per tree quantile computation)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt><strong>max_batch_size: int (default = 128)</strong></dt><dd><p>Maximum number of nodes that can be processed in a given batch. This is
used only when ‘use_experimental_backend’ is true.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed for the random number generator. Unseeded by default.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (default = None)</span></dt><dd><p>Deprecated in favor of <cite>random_state</cite>.
Seed for the random number generator. Unseeded by default.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span> <span class="k">as</span> <span class="n">cuRFC</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="n">cuml_model</span> <span class="o">=</span> <span class="n">cuRFC</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                   <span class="n">n_bins</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                   <span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">cuml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">cuml_predict</span> <span class="o">=</span> <span class="n">cuml_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Predicted labels : "</span><span class="p">,</span> <span class="n">cuml_predict</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Predicted labels :  [0 1 0 1 0 1 0 1 0 1]
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.convert_to_fil_model" title="cuml.ensemble.RandomForestClassifier.convert_to_fil_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_to_fil_model</span></code></a>(self[, output_class, …])</p></td>
<td><p>Create a Forest Inference (FIL) model from the trained cuML Random Forest model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.convert_to_treelite_model" title="cuml.ensemble.RandomForestClassifier.convert_to_treelite_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_to_treelite_model</span></code></a>(self)</p></td>
<td><p>Converts the cuML RF model to a Treelite model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.dump_as_json" title="cuml.ensemble.RandomForestClassifier.dump_as_json"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dump_as_json</span></code></a>(self)</p></td>
<td><p>Dump (export) the Random Forest model as a JSON string</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.fit" title="cuml.ensemble.RandomForestClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Perform Random Forest Classification on the input data</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.predict" title="cuml.ensemble.RandomForestClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, predict_model, …])</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.predict_proba" title="cuml.ensemble.RandomForestClassifier.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(self, X[, output_class, …])</p></td>
<td><p>Predicts class probabilites for X. This function uses the GPU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.print_detailed" title="cuml.ensemble.RandomForestClassifier.print_detailed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_detailed</span></code></a>(self)</p></td>
<td><p>Prints the detailed information about the forest used to train and test the Random Forest model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.print_summary" title="cuml.ensemble.RandomForestClassifier.print_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_summary</span></code></a>(self)</p></td>
<td><p>Prints the summary of the forest used to train and test the model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestClassifier.score" title="cuml.ensemble.RandomForestClassifier.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X, y[, threshold, algo, …])</p></td>
<td><p>Calculates the accuracy metric score of the model for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.convert_to_fil_model">
<code class="sig-name descname">convert_to_fil_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L356"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.convert_to_fil_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a Forest Inference (FIL) model from the trained cuML
Random Forest model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>output_class</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
If true, return a 1 or 0 depending on whether the raw
prediction exceeds the threshold. If False, just return
the raw prediction.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>Threshold used for classification. Optional and required only
while performing the predict operation on the GPU.
It is applied if output_class == True, else it is ignored</p>
</dd>
<dt><strong>fil_sparse_format</strong><span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>fil_model</dt><dd><p>A Forest Inference model which can be used to perform
inferencing on the random forest model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.convert_to_treelite_model">
<code class="sig-name descname">convert_to_treelite_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L345"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.convert_to_treelite_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the cuML RF model to a Treelite model</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>tl_to_fil_model</strong><span class="classifier">Treelite version of this model</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.dump_as_json">
<code class="sig-name descname">dump_as_json</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L926"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.dump_as_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Dump (export) the Random Forest model as a JSON string</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L416"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Perform Random Forest Classification on the input data</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix of type np.int32.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be of dtype int32. This will increase memory used for
the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">predict_model</span><span class="o">=</span><span class="default_value">'GPU'</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L554"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the labels for X.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>predict_model<span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The ‘GPU’ can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True. Also the ‘GPU’ should only be
used for binary classification problems.</p>
</dd>
<dt>output_class<span class="classifier">boolean (default = True)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
If true, return a 1 or 0 depending on whether the raw
prediction exceeds the threshold. If False, just return
the raw prediction.</p>
</dd>
<dt>algo<span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt>threshold<span class="classifier">float (default = 0.5)</span></dt><dd><p>Threshold used for classification. Optional and required only
while performing the predict operation on the GPU.
It is applied if output_class == True, else it is ignored</p>
</dd>
<dt>num_classes<span class="classifier">int (default = None)</span></dt><dd><p>number of different classes present in the dataset. This variable
will be deprecated in 0.16. The number of classes passed
must match the number of classes the model was trained on</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt>fil_sparse_format<span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output typeconfiguration, shape =(n_samples, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L704"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts class probabilites for X. This function uses the GPU
implementation of predict. Therefore, data with ‘dtype = np.float32’
should be used with this function.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>output_class: boolean (default = True)</dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
If true, return a 1 or 0 depending on whether the raw
prediction exceeds the threshold. If False, just return
the raw prediction.</p>
</dd>
<dt>algo<span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt>threshold<span class="classifier">float (default = 0.5)</span></dt><dd><p>Threshold used for classification. Optional and required only
while performing the predict operation on the GPU.
It is applied if output_class == True, else it is ignored</p>
</dd>
<dt>num_classes<span class="classifier">int (default = None)</span></dt><dd><p>number of different classes present in the dataset. This variable
will be deprecated in 0.16. The number of classes passed
must match the number of classes the model was trained on</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt>fil_sparse_format<span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output typeconfiguration, shape =(n_samples, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.print_detailed">
<code class="sig-name descname">print_detailed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L910"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.print_detailed" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the detailed information about the forest used to
train and test the Random Forest model</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.print_summary">
<code class="sig-name descname">print_summary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L895"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.print_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the summary of the forest used to train and test the model</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestClassifier.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">predict_model</span><span class="o">=</span><span class="default_value">'GPU'</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestclassifier.pyx#L788"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestClassifier.score" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Calculates the accuracy metric score of the model for X.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><p>y : array-like (device or host) shape = (n_samples, 1)</p>
</dd>
<dt><strong>Dense matrix of type np.int32.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>algo<span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt>threshold<span class="classifier">float</span></dt><dd><p>threshold is used to for classification
This is optional and required only while performing the
predict operation on the GPU.</p>
</dd>
<dt>num_classes<span class="classifier">int (default = None)</span></dt><dd><p>number of different classes present in the dataset. This variable
will be deprecated in 0.16. The number of classes passed
must match the number of classes the model was trained on</p>
</dd>
<dt>convert_dtype<span class="classifier">boolean, default=True</span></dt><dd><p>whether to convert input data to correct dtype automatically</p>
</dd>
<dt>predict_model<span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The ‘GPU’ can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True. Also the ‘GPU’ should only be
used for binary classification problems.</p>
</dd>
<dt>fil_sparse_format<span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>accuracy</strong><span class="classifier">float</span></dt><dd><p>Accuracy of the model [0.0 - 1.0]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.ensemble.RandomForestRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.ensemble.</code><code class="sig-name descname">RandomForestRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">split_criterion</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">accuracy_metric</span><span class="o">=</span><span class="default_value">'r2'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Random Forest regressor model which fits multiple decision
trees in an ensemble.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>that the underlying algorithm for tree node splits differs from
that used in scikit-learn. By default, the cuML Random Forest uses a
histogram-based algorithm to determine splits, rather than an exact
count. You can tune the size of the histograms with the n_bins
parameter.</p>
</div>
<p><strong>Known Limitations</strong>: This is an early release of the cuML
Random Forest code. It contains a few known limitations:</p>
<blockquote>
<div><ul class="simple">
<li><p>GPU-based inference is only supported if the model was trained
with 32-bit (float32) datatypes. CPU-based inference may be used
in this case as a slower fallback.</p></li>
<li><p>Very deep / very wide models may exhaust available GPU memory.
Future versions of cuML will provide an alternative algorithm to
reduce memory consumption.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 100)</span></dt><dd><p>Number of trees in the forest. (Default changed to 100 in cuML 0.11)</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The algorithm to determine how nodes are split in the tree.
0 for HIST and 1 for GLOBAL_QUANTILE. HIST curently uses a slower
tree-building algorithm so GLOBAL_QUANTILE is recommended for most
cases.</p>
</dd>
<dt><strong>split_criterion</strong><span class="classifier">int (default = 2)</span></dt><dd><p>The criterion used to split nodes.
0 for GINI, 1 for ENTROPY,
2 for MSE, or 3 for MAE
0 and 1 not valid for regression</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If True, each tree in the forest is built
on a bootstrapped sample with replacement.
If False, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = 16)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure),
if -1. Unlimited depth is not supported with split_algo=1.
<em>Note that this default differs from scikit-learn’s
random forest, which defaults to unlimited depth.</em></p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited,
if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int, float, or string (default = ‘auto’)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.
If int then max_features/n_features.
If float then max_features is used as a fraction.
If ‘auto’ then max_features=1.0.
If ‘sqrt’ then max_features=1/sqrt(n_features).
If ‘log2’ then max_features=log2(n_features)/n_features.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int or float (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed to split a node.
If int then number of sample rows
If float the min_rows_per_sample*n_rows</p>
</dd>
<dt><strong>min_impurity_decrease</strong><span class="classifier">float (default = 0.0)</span></dt><dd><p>The minimum decrease in impurity required for node to be split</p>
</dd>
<dt><strong>accuracy_metric</strong><span class="classifier">string (default = ‘r2’)</span></dt><dd><p>Decides the metric used to evaluate the performance of the model.
In the 0.16 release, the default scoring metric was changed
from mean squared error to r-squared.
for r-squared : ‘r2’
for median of abs error : ‘median_ae’
for mean of abs error : ‘mean_ae’
for mean square error’ : ‘mse’</p>
</dd>
<dt><strong>quantile_per_tree</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Whether quantile is computed for individal trees in RF.
Only relevant for GLOBAL_QUANTILE split_algo.</p>
</dd>
<dt><strong>use_experimental_backend</strong><span class="classifier">boolean (default = False)</span></dt><dd><dl class="simple">
<dt>If set to true and  following conditions are also met, experimental</dt><dd><dl class="simple">
<dt>decision tree training implementation would be used:</dt><dd><p>split_algo = 1 (GLOBAL_QUANTILE)
0 &lt; max_depth &lt; 14
max_features = 1.0 (Feature sub-sampling disabled)
quantile_per_tree = false (No per tree quantile computation)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt><strong>max_batch_size: int (default = 128)</strong></dt><dd><p>Maximum number of nodes that can be processed in a given batch. This is
used only when ‘use_experimental_backend’ is true.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed for the random number generator. Unseeded by default. Does not
currently fully guarantee the exact same results.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (default = None)</span></dt><dd><p>Deprecated in favor of <cite>random_state</cite>.
Seed for the random number generator. Unseeded by default. Does not
currently fully guarantee the exact same results.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.test.utils</span> <span class="kn">import</span> <span class="n">get_handle</span>
<span class="kn">from</span> <span class="nn">cuml.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span> <span class="k">as</span> <span class="n">curfc</span>
<span class="kn">from</span> <span class="nn">cuml.test.utils</span> <span class="kn">import</span> <span class="n">get_handle</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">30</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">40</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">cuml_model</span> <span class="o">=</span> <span class="n">curfc</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                    <span class="n">split_algo</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">min_rows_per_node</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">accuracy_metric</span><span class="o">=</span><span class="s1">'r2'</span><span class="p">)</span>
<span class="n">cuml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">cuml_score</span> <span class="o">=</span> <span class="n">cuml_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"MSE score of cuml : "</span><span class="p">,</span> <span class="n">cuml_score</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MSE</span> <span class="n">score</span> <span class="n">of</span> <span class="n">cuml</span> <span class="p">:</span>  <span class="mf">0.1123437201231765</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.convert_to_fil_model" title="cuml.ensemble.RandomForestRegressor.convert_to_fil_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_to_fil_model</span></code></a>(self[, output_class, …])</p></td>
<td><p>Create a Forest Inference (FIL) model from the trained cuML Random Forest model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.convert_to_treelite_model" title="cuml.ensemble.RandomForestRegressor.convert_to_treelite_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_to_treelite_model</span></code></a>(self)</p></td>
<td><p>Converts the cuML RF model to a Treelite model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.dump_as_json" title="cuml.ensemble.RandomForestRegressor.dump_as_json"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dump_as_json</span></code></a>(self)</p></td>
<td><p>Dump (export) the Random Forest model as a JSON string</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.fit" title="cuml.ensemble.RandomForestRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Perform Random Forest Regression on the input data</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.predict" title="cuml.ensemble.RandomForestRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, predict_model, algo, …])</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.print_detailed" title="cuml.ensemble.RandomForestRegressor.print_detailed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_detailed</span></code></a>(self)</p></td>
<td><p>Prints the detailed information about the forest used to train and test the Random Forest model</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.print_summary" title="cuml.ensemble.RandomForestRegressor.print_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_summary</span></code></a>(self)</p></td>
<td><p>Prints the summary of the forest used to train and test the model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ensemble.RandomForestRegressor.score" title="cuml.ensemble.RandomForestRegressor.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X, y[, algo, convert_dtype, …])</p></td>
<td><p>Calculates the accuracy metric score of the model for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.convert_to_fil_model">
<code class="sig-name descname">convert_to_fil_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L349"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.convert_to_fil_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a Forest Inference (FIL) model from the trained cuML
Random Forest model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>output_class</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
If true, return a 1 or 0 depending on whether the raw
prediction exceeds the threshold. If False, just return
the raw prediction.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt><strong>fil_sparse_format</strong><span class="classifier">boolean or string (default = ‘auto’)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>fil_model</dt><dd><p>A Forest Inference model which can be used to perform
inferencing on the random forest model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.convert_to_treelite_model">
<code class="sig-name descname">convert_to_treelite_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L338"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.convert_to_treelite_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the cuML RF model to a Treelite model</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>tl_to_fil_model</strong><span class="classifier">Treelite version of this model</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.dump_as_json">
<code class="sig-name descname">dump_as_json</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L732"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.dump_as_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Dump (export) the Random Forest model as a JSON string</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L403"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Perform Random Forest Regression on the input data</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">predict_model</span><span class="o">=</span><span class="default_value">'GPU'</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L525"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the labels for X.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>predict_model<span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The GPU can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True.</p>
</dd>
<dt>algo<span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt>fil_sparse_format<span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output typeconfiguration, shape =(n_samples, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.print_detailed">
<code class="sig-name descname">print_detailed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L716"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.print_detailed" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the detailed information about the forest used to
train and test the Random Forest model</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.print_summary">
<code class="sig-name descname">print_summary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L701"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.print_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the summary of the forest used to train and test the model</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ensemble.RandomForestRegressor.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">predict_model</span><span class="o">=</span><span class="default_value">'GPU'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/ensemble/randomforestregressor.pyx#L590"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ensemble.RandomForestRegressor.score" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Calculates the accuracy metric score of the model for X.
In the 0.16 release, the default scoring metric was changed
from mean squared error to r-squared.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><p>y : array-like (device or host) shape = (n_samples, 1)</p>
</dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>algo<span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>auto</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt>convert_dtype<span class="classifier">boolean, default=True</span></dt><dd><p>whether to convert input data to correct dtype automatically</p>
</dd>
<dt>predict_model<span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The GPU can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True.</p>
</dd>
<dt>fil_sparse_format<span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>mean_square_error</strong><span class="classifier">float or</span></dt><dd><p>median_abs_error : float or
mean_abs_error : float</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="forest-inferencing">
<h3>Forest Inferencing<a class="headerlink" href="#forest-inferencing" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.ForestInference">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">ForestInference</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ForestInference" title="Permalink to this definition">¶</a></dt>
<dd><p>ForestInference provides GPU-accelerated inference (prediction)
for random forest and boosted decision tree models.</p>
<p>This module does not support training models. Rather, users should
train a model in another package and save it in a
treelite-compatible format. (See <a class="reference external" href="https://github.com/dmlc/treelite">https://github.com/dmlc/treelite</a>)
Currently, LightGBM, XGBoost and SKLearn GBDT and random forest models
are supported.</p>
<p>Users typically create a ForestInference object by loading a saved model
file with ForestInference.load. It is also possible to create it from an
SKLearn model using ForestInference.load_from_sklearn. The resulting object
provides a <cite>predict</cite> method for carrying out inference.</p>
<dl class="simple">
<dt><strong>Known limitations</strong>:</dt><dd><ul class="simple">
<li><p>A single row of data should fit into the shared memory of a thread
block, which means that more than 12288 features are not supported.</p></li>
<li><p>From sklearn.ensemble, only
{RandomForest,GradientBoosting}{Classifier,Regressor} models are
supported; other sklearn.ensemble models are currently not supported.</p></li>
<li><p>Importing large SKLearn models can be slow, as it is done in Python.</p></li>
<li><p>LightGBM categorical features are not supported.</p></li>
<li><p>Inference uses a dense matrix format, which is efficient for many
problems but can be suboptimal for sparse datasets.</p></li>
<li><p>Only binary classification and regression are supported.</p></li>
<li><p>Many other random forest implementations including LightGBM, and SKLearn
GBDTs make use of 64-bit floating point parameters, but the underlying
library for ForestInference uses only 32-bit parameters. Because of the
truncation that will occur when loading such models into
ForestInference, you may observe a slight degradation in accuracy.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional usage examples, see the sample notebook at
<a class="reference external" href="https://github.com/rapidsai/cuml/blob/branch-0.15/notebooks/forest_inference_demo.ipynb">https://github.com/rapidsai/cuml/blob/branch-0.15/notebooks/forest_inference_demo.ipynb</a></p>
<p class="rubric">Examples</p>
<p>In the example below, synthetic data is copied to the host before
inference. ForestInference can also accept a numpy array directly at the
cost of a slight performance overhead.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume that the file 'xgb.model' contains a classifier model that was</span>
<span class="c1"># previously saved by XGBoost's save_model function.</span>

<span class="kn">import</span> <span class="nn">sklearn</span><span class="o">,</span> <span class="nn">sklearn.datasets</span><span class="o">,</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">ForestInference</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">'xgb.model'</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">()</span>
<span class="n">X_gpu</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="n">fm</span> <span class="o">=</span> <span class="n">ForestInference</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">output_class</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fil_preds_gpu</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_gpu</span><span class="p">)</span>
<span class="n">accuracy_score</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fil_preds_gpu</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ForestInference.load" title="cuml.ForestInference.load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load</span></code></a>(filename[, output_class, threshold, …])</p></td>
<td><p>Returns a FIL instance containing the forest saved in <cite>filename</cite> This uses Treelite to load the saved model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ForestInference.load_from_sklearn" title="cuml.ForestInference.load_from_sklearn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_sklearn</span></code></a>(skl_model[, output_class, …])</p></td>
<td><p>Creates a FIL model using the scikit-learn model passed to the function.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ForestInference.load_from_treelite_model" title="cuml.ForestInference.load_from_treelite_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_treelite_model</span></code></a>(self, model[, …])</p></td>
<td><p>Creates a FIL model using the treelite model passed to the function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ForestInference.load_using_treelite_handle" title="cuml.ForestInference.load_using_treelite_handle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_using_treelite_handle</span></code></a>(self, model_handle)</p></td>
<td><p>Returns a FIL instance by converting a treelite model to FIL model by using the treelite ModelHandle passed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ForestInference.predict" title="cuml.ForestInference.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, preds])</p></td>
<td><p>Predicts the labels for X with the loaded forest model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ForestInference.predict_proba" title="cuml.ForestInference.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(self, X[, preds])</p></td>
<td><p>Predicts the class probabilities for X with the loaded forest model.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.ForestInference.load">
<em class="property">static </em><code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filename</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">storage_type</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">model_type</span><span class="o">=</span><span class="default_value">'xgboost'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/fil/fil.pyx#L646"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ForestInference.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a FIL instance containing the forest saved in <cite>filename</cite>
This uses Treelite to load the saved model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>filename</strong><span class="classifier">string</span></dt><dd><p>Path to saved model file in a treelite-compatible format
(See <a class="reference external" href="https://treelite.readthedocs.io/en/latest/treelite-api.html">https://treelite.readthedocs.io/en/latest/treelite-api.html</a>
for more information)</p>
</dd>
<dt><strong>output_class: boolean (default=False)</strong></dt><dd><p>For a Classification model <cite>output_class</cite> must be True.
For a Regression model <cite>output_class</cite> must be False.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default=0.5)</span></dt><dd><p>Cutoff value above which a prediction is set to 1.0
Only used if the model is classification and <cite>output_class</cite> is True</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default=’auto’)</span></dt><dd><p>Which inference algorithm to use.
See documentation in <cite>FIL.load_from_treelite_model</cite></p>
</dd>
<dt><strong>storage_type</strong><span class="classifier">string (default=’auto’)</span></dt><dd><p>In-memory storage format to be used for the FIL model.
See documentation in <cite>FIL.load_from_treelite_model</cite></p>
</dd>
<dt><strong>model_type</strong><span class="classifier">string (default=”xgboost”)</span></dt><dd><p>Format of the saved treelite model to be load.
It can be ‘xgboost’, ‘lightgbm’.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>fil_model</dt><dd><p>A Forest Inference model which can be used to perform
inferencing on the model read from the file.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ForestInference.load_from_sklearn">
<em class="property">static </em><code class="sig-name descname">load_from_sklearn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">skl_model</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">storage_type</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/fil/fil.pyx#L590"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ForestInference.load_from_sklearn" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a FIL model using the scikit-learn model passed to the
function. This function requires Treelite 0.90 to be installed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>skl_model</strong></dt><dd><p>The scikit-learn model from which to build the FIL version.</p>
</dd>
<dt><strong>output_class: boolean (default=False)</strong></dt><dd><p>For a Classification model output_class must be True.
For a Regression model output_class must be False.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default=’auto’)</span></dt><dd><dl class="simple">
<dt>name of the algo from (from algo_t enum):</dt><dd><ul class="simple">
<li><p>‘AUTO’ or ‘auto’ - choose the algorithm automatically;
currently ‘BATCH_TREE_REORG’ is used for dense storage,
and ‘NAIVE’ for sparse storage</p></li>
<li><p>‘NAIVE’ or ‘naive’ - simple inference using shared memory</p></li>
<li><p>‘TREE_REORG’ or ‘tree_reorg’ - similar to naive but trees
rearranged to be more coalescing-friendly</p></li>
<li><p>‘BATCH_TREE_REORG’ or ‘batch_tree_reorg’ - similar to TREE_REORG
but predicting multiple rows per thread block</p></li>
</ul>
</dd>
</dl>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default=0.5)</span></dt><dd><p>Threshold is used to for classification. It is applied
only if <code class="docutils literal notranslate"><span class="pre">output_class</span> <span class="pre">==</span> <span class="pre">True</span></code>, else it is ignored.</p>
</dd>
<dt><strong>storage_type</strong><span class="classifier">string or boolean (default=’auto’)</span></dt><dd><dl class="simple">
<dt>In-memory storage format to be used for the FIL model:</dt><dd><ul class="simple">
<li><p>‘auto’ - choose the storage type automatically
(currently DENSE is always used)</p></li>
<li><p>False - create a dense forest</p></li>
<li><p>True - create a sparse forest;
requires algo=’NAIVE’ or algo=’AUTO’</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>fil_model</dt><dd><p>A Forest Inference model created from the scikit-learn
model passed.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ForestInference.load_from_treelite_model">
<code class="sig-name descname">load_from_treelite_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">storage_type</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/fil/fil.pyx#L530"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ForestInference.load_from_treelite_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a FIL model using the treelite model
passed to the function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model</strong></dt><dd><p>the trained model information in the treelite format
loaded from a saved model using the treelite API
<a class="reference external" href="https://treelite.readthedocs.io/en/latest/treelite-api.html">https://treelite.readthedocs.io/en/latest/treelite-api.html</a></p>
</dd>
<dt><strong>output_class: boolean (default=False)</strong></dt><dd><p>For a Classification model output_class must be True.
For a Regression model output_class must be False.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default=’auto’)</span></dt><dd><dl class="simple">
<dt>name of the algo from (from algo_t enum) :</dt><dd><ul class="simple">
<li><p>‘AUTO’ or ‘auto’ - choose the algorithm automatically;
currently ‘BATCH_TREE_REORG’ is used for dense storage,
and ‘NAIVE’ for sparse storage</p></li>
<li><p>‘NAIVE’ or ‘naive’ - simple inference using shared memory</p></li>
<li><p>‘TREE_REORG’ or ‘tree_reorg’ - similar to naive but trees
rearranged to be more coalescing-friendly</p></li>
<li><p>‘BATCH_TREE_REORG’ or ‘batch_tree_reorg’ - similar to TREE_REORG
but predicting multiple rows per thread block</p></li>
</ul>
</dd>
</dl>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default=0.5)</span></dt><dd><p>Threshold is used to for classification. It is applied
only if output_class == True, else it is ignored.</p>
</dd>
<dt><strong>storage_type</strong><span class="classifier">string or boolean (default=’auto’)</span></dt><dd><dl class="simple">
<dt>In-memory storage format to be used for the FIL model:</dt><dd><ul class="simple">
<li><p>‘auto’ - choose the storage type automatically
(currently DENSE is always used)</p></li>
<li><p>False - create a dense forest</p></li>
<li><p>True - create a sparse forest;
requires algo=’NAIVE’ or algo=’AUTO’</p></li>
<li><dl class="simple">
<dt>‘sparse8’ - (experimental) create a sparse forest</dt><dd><p>with 8-byte nodes; requires algo=’NAIVE’ or algo=’AUTO’;
can fail if 8-byte nodes are not enough
to store the forest, e.g. if there are
too many nodes in a tree or too many features</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>fil_model</dt><dd><p>A Forest Inference model which can be used to perform
inferencing on the random forest/ XGBoost model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ForestInference.load_using_treelite_handle">
<code class="sig-name descname">load_using_treelite_handle</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">model_handle</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">storage_type</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/fil/fil.pyx#L695"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ForestInference.load_using_treelite_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a FIL instance by converting a treelite model to
FIL model by using the treelite ModelHandle passed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model_handle</strong><span class="classifier">Modelhandle to the treelite forest model</span></dt><dd><p>(See <a class="reference external" href="https://treelite.readthedocs.io/en/latest/treelite-api.html">https://treelite.readthedocs.io/en/latest/treelite-api.html</a>
for more information)</p>
</dd>
<dt><strong>output_class: boolean (default=False)</strong></dt><dd><p>For a Classification model <cite>output_class</cite> must be True.
For a Regression model <cite>output_class</cite> must be False.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default=0.5)</span></dt><dd><p>Cutoff value above which a prediction is set to 1.0
Only used if the model is classification and output_class is True</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default=’auto’)</span></dt><dd><p>Which inference algorithm to use.
See documentation in <cite>FIL.load_from_treelite_model</cite></p>
</dd>
<dt><strong>storage_type</strong><span class="classifier">string (default=’auto’)</span></dt><dd><p>In-memory storage format to be used for the FIL model.
See documentation in <cite>FIL.load_from_treelite_model</cite></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>fil_model</dt><dd><p>A Forest Inference model which can be used to perform
inferencing on the random forest model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ForestInference.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">preds</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/fil/fil.pyx#L477"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ForestInference.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X with the loaded forest model.
By default, the result is the raw floating point output
from the model, unless output_class was set to True
during model loading.</p>
<p>See the documentation of ForestInference.load for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy
For optimal performance, pass a device array with C-style layout</p>
</dd>
<dt><strong>preds: gpuarray or cudf.Series, shape = (n_samples,)</strong></dt><dd><p>Optional ‘out’ location to store inference results</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>GPU array of length n_samples with inference results</dt><dd></dd>
<dt>(or ‘preds’ filled with inference results if preds was specified)</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ForestInference.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">preds</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/fil/fil.pyx#L504"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ForestInference.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the class probabilities for X with the loaded forest model.
The result is the raw floating point output
from the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix (floats) of shape (n_samples, n_features).
Acceptable formats: cuDF DataFrame, NumPy ndarray, Numba device
ndarray, cuda array interface compliant array like CuPy
For optimal performance, pass a device array with C-style layout</p>
</dd>
<dt><strong>preds: gpuarray or cudf.Series, shape = (n_samples,2)</strong></dt><dd><p>binary probability output
Optional ‘out’ location to store inference results</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>GPU array of shape (n_samples,2) with inference results</dt><dd></dd>
<dt>(or ‘preds’ filled with inference results if preds was specified)</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="coordinate-descent">
<h3>Coordinate Descent<a class="headerlink" href="#coordinate-descent" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.CD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">CD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">'squared_loss'</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.15</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.CD" title="Permalink to this definition">¶</a></dt>
<dd><p>Coordinate Descent (CD) is a very common optimization algorithm that
minimizes along coordinate directions to find the minimum of a function.</p>
<p>cuML’s CD algorithm accepts a numpy matrix or a cuDF DataFrame as the
input dataset.algorithm The CD algorithm currently works with linear
regression and ridge, lasso, and elastic-net penalties.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">‘squared_loss’ (Only ‘squared_loss’ is supported right now)</span></dt><dd><p>‘squared_loss’ uses linear regression</p>
</dd>
<dt><strong>alpha: float (default = 0.0001)</strong></dt><dd><p>The constant value which decides the degree of regularization.
‘alpha = 0’ is equivalent to an ordinary least square, solved by the
LinearRegression object.</p>
</dd>
<dt><strong>l1_ratio: float (default = 0.15)</strong></dt><dd><p>The ElasticNet mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. For
l1_ratio = 0 the penalty is an L2 penalty.
For l1_ratio = 1 it is an L1 penalty. For 0 &lt; l1_ratio &lt; 1,
the penalty is a combination of L1 and L2.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The number of times the model should iterate through the entire
dataset during training (default = 1000)</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than tol,
solver stops.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If set to ‘True’, a random coefficient is updated every iteration rather
than looping over features sequentially by default.
This (setting to ‘True’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.solvers</span> <span class="kn">import</span> <span class="n">CD</span> <span class="k">as</span> <span class="n">cumlCD</span>

<span class="n">cd</span> <span class="o">=</span> <span class="n">cumlCD</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">cd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">cd</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>
            <span class="mi">0</span> <span class="mf">1.0019531</span>
            <span class="mi">1</span> <span class="mf">1.9980469</span>
<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>
<span class="n">Preds</span><span class="p">:</span>
            <span class="mi">0</span> <span class="mf">15.997</span>
            <span class="mi">1</span> <span class="mf">14.995</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.CD.fit" title="cuml.CD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.CD.get_param_names" title="cuml.CD.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.CD.predict" title="cuml.CD.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.CD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/cd.pyx#L225"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.CD.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.CD.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/cd.pyx#L345"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.CD.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.CD.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/cd.pyx#L299"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.CD.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="quasi-newton">
<h3>Quasi-Newton<a class="headerlink" href="#quasi-newton" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.QN">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">QN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">'sigmoid'</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">l1_strength</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">l2_strength</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">linesearch_max_iter</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">lbfgs_memory</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.QN" title="Permalink to this definition">¶</a></dt>
<dd><p>Quasi-Newton methods are used to either find zeroes or local maxima
and minima of functions, and used by this class to optimize a cost
function.</p>
<p>Two algorithms are implemented underneath cuML’s QN class, and which one
is executed depends on the following rule:</p>
<ul class="simple">
<li><p>Orthant-Wise Limited Memory Quasi-Newton (OWL-QN) if there is l1
regularization</p></li>
<li><p>Limited Memory BFGS (L-BFGS) otherwise.</p></li>
</ul>
<p>cuML’s QN class can take array-like objects, either in host as
NumPy arrays or in device (as Numba or __cuda_array_interface__ compliant).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss: ‘sigmoid’, ‘softmax’, ‘squared_loss’ (default = ‘squared_loss’)</strong></dt><dd><p>‘sigmoid’ loss used for single class logistic regression
‘softmax’ loss used for multiclass logistic regression
‘normal’ used for normal/square loss</p>
</dd>
<dt><strong>fit_intercept: boolean (default = True)</strong></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>l1_strength: float (default = 0.0)</strong></dt><dd><p>l1 regularization strength (if non-zero, will run OWL-QN, else L-BFGS).
Note, that as in Scikit-learn, the bias will not be regularized.</p>
</dd>
<dt><strong>l2_strength: float (default = 0.0)</strong></dt><dd><p>l2 regularization strength. Note, that as in Scikit-learn, the bias
will not be regularized.</p>
</dd>
<dt><strong>max_iter: int (default = 1000)</strong></dt><dd><p>Maximum number of iterations taken for the solvers to converge.</p>
</dd>
<dt><strong>tol: float (default = 1e-3)</strong></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>linesearch_max_iter: int (default = 50)</strong></dt><dd><p>Max number of linesearch iterations per outer iteration of the
algorithm.</p>
</dd>
<dt><strong>lbfgs_memory: int (default = 5)</strong></dt><dd><p>Rank of the lbfgs inverse-Hessian approximation. Method will use
O(lbfgs_memory * D) memory.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This class contains implementations of two popular Quasi-Newton methods:</p>
<ul class="simple">
<li><p>Limited-memory Broyden Fletcher Goldfarb Shanno (L-BFGS) [Nocedal,
Wright - Numerical Optimization (1999)]</p></li>
<li><p>Orthant-wise limited-memory quasi-newton (OWL-QN) [Andrew, Gao - ICML
2007]
&lt;<a class="reference external" href="https://www.microsoft.com/en-us/research/publication/scalable-training-of-l1-regularized-log-linear-models/">https://www.microsoft.com/en-us/research/publication/scalable-training-of-l1-regularized-log-linear-models/</a>&gt;</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Both import methods supported</span>
<span class="c1"># from cuml import QN</span>
<span class="kn">from</span> <span class="nn">cuml.solvers</span> <span class="kn">import</span> <span class="n">QN</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">solver</span> <span class="o">=</span> <span class="n">QN</span><span class="p">()</span>
<span class="n">solver</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Note: for now, the coefficients also include the intercept in the</span>
<span class="c1"># last position if fit_intercept=True</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Predictions:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>
            <span class="mf">10.647417</span>
            <span class="mf">0.3267412</span>
            <span class="o">-</span><span class="mf">17.158297</span>
<span class="n">Intercept</span><span class="p">:</span>
            <span class="o">-</span><span class="mf">17.158297</span>
<span class="n">Predictions</span><span class="p">:</span>
            <span class="mi">0</span>    <span class="mf">0.0</span>
            <span class="mi">1</span>    <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_classes, n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.
Note: shape is (n_classes, n_features + 1) if fit_intercept = True.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array (n_classes, 1)</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.QN.fit" title="cuml.QN.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.QN.get_param_names" title="cuml.QN.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.QN.predict" title="cuml.QN.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.QN.score" title="cuml.QN.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X, y)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.QN.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/qn.pyx#L280"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.QN.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.QN.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/qn.pyx#L522"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.QN.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.QN.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/qn.pyx#L454"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.QN.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the y for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.QN.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/solvers/qn.pyx#L505"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.QN.score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="support-vector-machines">
<h3>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.svm.SVC">
<em class="property">class </em><code class="sig-prename descclassname">cuml.svm.</code><code class="sig-name descname">SVC</code><span class="sig-paren">(</span><em class="sig-param">C-Support Vector Classification</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.svm.SVC" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct an SVC classifier for training and predictions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This implementation has the following known limitations:</p>
<ul class="simple">
<li><p>Currently only binary classification is supported.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>C</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Penalty parameter C</p>
</dd>
<dt><strong>kernel</strong><span class="classifier">string (default=’rbf’)</span></dt><dd><p>Specifies the kernel function. Possible options: ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’. Currently precomputed kernels are not supported.</p>
</dd>
<dt><strong>degree</strong><span class="classifier">int (default=3)</span></dt><dd><p>Degree of polynomial kernel function.</p>
</dd>
<dt><strong>gamma</strong><span class="classifier">float or string (default = ‘scale’)</span></dt><dd><p>Coefficient for rbf, poly, and sigmoid kernels. You can specify the
numeric value, or use one of the following options:
- ‘auto’: gamma will be set to 1 / n_features
- ‘scale’: gamma will be se to 1 / (n_features * X.var())</p>
</dd>
<dt><strong>coef0</strong><span class="classifier">float (default = 0.0)</span></dt><dd><p>Independent term in kernel function, only signifficant for poly and
sigmoid</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>Tolerance for stopping criterion.</p>
</dd>
<dt><strong>cache_size</strong><span class="classifier">float (default = 200.0)</span></dt><dd><p>Size of the kernel cache during training in MiB. The default is a
conservative value, increase it to improve the training time, at
the cost of higher memory footprint. After training the kernel
cache is deallocated.
During prediction, we also need a temporary space to store kernel
matrix elements (this can be signifficant if n_support is large).
The cache_size variable sets an upper limit to the prediction
buffer as well.</p>
</dd>
<dt><strong>class_weight</strong><span class="classifier">dict or string (default=None)</span></dt><dd><p>Weights to modify the parameter C for class i to class_weight[i]*C. The
string ‘balanced’ is also accepted, in which case class_weight[i] =
n_samples / (n_classes * n_samples_of_class[i])</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 100*n_samples)</span></dt><dd><p>Limit the number of outer iterations in the solver</p>
</dd>
<dt><strong>nochange_steps</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>We monitor how much our stopping criteria changes during outer
iterations. If it does not change (changes less then 1e-3*tol)
for nochange_steps consecutive steps, then we stop training.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>probability: bool (default = False)</strong></dt><dd><p>Enable or disable probability estimates.</p>
</dd>
<dt><strong>random_state: int (default = None)</strong></dt><dd><p>Seed for random number generator (used only when probability = True).
Currently this argument is not used and a waring will be printed if the
user provides it.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The solver uses the SMO method to fit the classifier. We use the Optimized
Hierarchical Decomposition <a class="reference internal" href="#r93eb4b3677ac-1" id="id6">[1]</a> variant of the SMO algorithm, similar to
<a class="reference internal" href="#r93eb4b3677ac-2" id="id7">[2]</a>.</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">scikitlearn’s SVC</a>.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r93eb4b3677ac-1"><span class="brackets"><a class="fn-backref" href="#id6">1</a></span></dt>
<dd><p>J. Vanek et al. A GPU-Architecture Optimized Hierarchical
Decomposition Algorithm for Support VectorMachine Training, IEEE
Transactions on Parallel and Distributed Systems, vol 28, no 12, 3330,
(2017)</p>
</dd>
<dt class="label" id="r93eb4b3677ac-2"><span class="brackets"><a class="fn-backref" href="#id7">2</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/Xtra-Computing/thundersvm">Z. Wen et al. ThunderSVM: A Fast SVM Library on GPUs and CPUs,
Journal of Machine Learning Research, 19, 1-5 (2018)</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span>
             <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">'poly'</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Predicted labels:"</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Predicted labels: [-1. -1.  1. -1.  1.  1.]
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_support_</strong><span class="classifier">int</span></dt><dd><p>The total number of support vectors. Note: this will change in the
future to represent number support vectors for each class (like
in Sklearn, see <a class="reference external" href="https://github.com/rapidsai/cuml/issues/956">https://github.com/rapidsai/cuml/issues/956</a> )</p>
</dd>
<dt><strong>support_</strong><span class="classifier">int, shape = (n_support)</span></dt><dd><p>Device array of support vector indices</p>
</dd>
<dt><strong>support_vectors_</strong><span class="classifier">float, shape (n_support, n_cols)</span></dt><dd><p>Device array of support vectors</p>
</dd>
<dt><strong>dual_coef_</strong><span class="classifier">float, shape = (1, n_support)</span></dt><dd><p>Device array of coefficients for support vectors</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">int</span></dt><dd><p>The constant in the decision function</p>
</dd>
<dt><strong>fit_status_</strong><span class="classifier">int</span></dt><dd><p>0 if SVM is correctly fitted</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">coef_</span></code><span class="classifier">float, shape (1, n_cols)</span></dt><dd><p>SVMBase.coef_(self)</p>
</dd>
<dt><strong>classes_: shape (n_classes_,)</strong></dt><dd><p>Array of class labels.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.svm.SVC.decision_function" title="cuml.svm.SVC.decision_function"><code class="xref py py-obj docutils literal notranslate"><span class="pre">decision_function</span></code></a>(self, X)</p></td>
<td><p>Calculates the decision function values for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.svm.SVC.fit" title="cuml.svm.SVC.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, sample_weight, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.svm.SVC.get_param_names" title="cuml.svm.SVC.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.svm.SVC.predict" title="cuml.svm.SVC.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the class labels for X. The returned y values are the class</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.svm.SVC.predict_log_proba" title="cuml.svm.SVC.predict_log_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_log_proba</span></code></a>(self, X)</p></td>
<td><p>Predicts the log probabilities for X (returns log(predict_proba(x)).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.svm.SVC.predict_proba" title="cuml.svm.SVC.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(self, X[, log])</p></td>
<td><p>Predicts the class probabilities for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.svm.SVC.classes_">
<em class="property">property </em><code class="sig-name descname">classes_</code><a class="headerlink" href="#cuml.svm.SVC.classes_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVC.decision_function">
<code class="sig-name descname">decision_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svc.pyx#L486"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVC.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Calculates the decision function values for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>results</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Decision function                                        values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVC.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svc.pyx#L335"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVC.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix of any dtype.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVC.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svc.pyx#L510"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVC.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVC.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svc.pyx#L418"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVC.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the class labels for X. The returned y values are the class
labels associated to sign(decision_function(X)).</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVC.predict_log_proba">
<code class="sig-name descname">predict_log_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svc.pyx#L472"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVC.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the log probabilities for X (returns log(predict_proba(x)).</p>
<p>The model has to be trained with probability=True to use this method.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_classes)</span></dt><dd><p>Log of predicted                                        probabilities</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVC.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">log</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svc.pyx#L440"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVC.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the class probabilities for X.</p>
<p>The model has to be trained with probability=True to use this method.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>log: boolean (default = False)</strong></dt><dd><p>Whether to return log probabilities.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_classes)</span></dt><dd><p>Predicted                                        probabilities</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.svm.SVR">
<em class="property">class </em><code class="sig-prename descclassname">cuml.svm.</code><code class="sig-name descname">SVR</code><span class="sig-paren">(</span><em class="sig-param">Epsilon Support Vector Regression</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.svm.SVR" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct an SVC classifier for training and predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>C</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Penalty parameter C</p>
</dd>
<dt><strong>kernel</strong><span class="classifier">string (default=’rbf’)</span></dt><dd><p>Specifies the kernel function. Possible options: ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’. Currently precomputed kernels are not supported.</p>
</dd>
<dt><strong>degree</strong><span class="classifier">int (default=3)</span></dt><dd><p>Degree of polynomial kernel function.</p>
</dd>
<dt><strong>gamma</strong><span class="classifier">float or string (default = ‘scale’)</span></dt><dd><p>Coefficient for rbf, poly, and sigmoid kernels. You can specify the
numeric value, or use one of the following options:</p>
<ul class="simple">
<li><p>‘auto’: gamma will be set to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">n_features</span></code></p></li>
<li><p>‘scale’: gamma will be se to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(n_features</span> <span class="pre">*</span> <span class="pre">X.var())</span></code></p></li>
</ul>
</dd>
<dt><strong>coef0</strong><span class="classifier">float (default = 0.0)</span></dt><dd><p>Independent term in kernel function, only signifficant for poly and
sigmoid</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>Tolerance for stopping criterion.</p>
</dd>
<dt><strong>epsilon: float (default = 0.1)</strong></dt><dd><p>epsilon parameter of the epsiron-SVR model. There is no penalty
associated to points that are predicted within the epsilon-tube
around the target values.</p>
</dd>
<dt><strong>cache_size</strong><span class="classifier">float (default = 200 MiB)</span></dt><dd><p>Size of the kernel cache during training in MiB. The default is a
conservative value, increase it to improve the training time, at
the cost of higher memory footprint. After training the kernel
cache is deallocated.
During prediction, we also need a temporary space to store kernel
matrix elements (this can be signifficant if n_support is large).
The cache_size variable sets an upper limit to the prediction
buffer as well.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 100*n_samples)</span></dt><dd><p>Limit the number of outer iterations in the solver</p>
</dd>
<dt><strong>nochange_steps</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>We monitor how much our stopping criteria changes during outer
iterations. If it does not change (changes less then 1e-3*tol)
for nochange_steps consecutive steps, then we stop training.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html">Scikit-learn’s SVR</a>.</p>
<p>The solver uses the SMO method to fit the regressor. We use the Optimized
Hierarchical Decomposition <a class="reference internal" href="#rb8bccbcfd254-1" id="id10">[1]</a> variant of the SMO algorithm, similar to
<a class="reference internal" href="#rb8bccbcfd254-2" id="id11">[2]</a></p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="rb8bccbcfd254-1"><span class="brackets"><a class="fn-backref" href="#id10">1</a></span></dt>
<dd><p>J. Vanek et al. A GPU-Architecture Optimized Hierarchical
Decomposition Algorithm for Support VectorMachine Training, IEEE
Transactions on Parallel and Distributed Systems, vol 28, no 12,
3330, (2017)</p>
</dd>
<dt class="label" id="rb8bccbcfd254-2"><span class="brackets"><a class="fn-backref" href="#id11">2</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/Xtra-Computing/thundersvm">Z. Wen et al. ThunderSVM: A Fast SVM Library on GPUs and CPUs,
Journal of Machine Learning Research, 19, 1-5 (2018)</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">'rbf'</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">'scale'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Predicted values:"</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Predicted</span> <span class="n">values</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.200474</span> <span class="mf">3.8999617</span> <span class="mf">5.100488</span> <span class="mf">3.7995374</span> <span class="mf">1.0995375</span><span class="p">]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_support_</strong><span class="classifier">int</span></dt><dd><p>The total number of support vectors. Note: this will change in the
future to represent number support vectors for each class (like
in Sklearn, see Issue #956)</p>
</dd>
<dt><strong>support_</strong><span class="classifier">int, shape = [n_support]</span></dt><dd><p>Device array of suppurt vector indices</p>
</dd>
<dt><strong>support_vectors_</strong><span class="classifier">float, shape [n_support, n_cols]</span></dt><dd><p>Device array of support vectors</p>
</dd>
<dt><strong>dual_coef_</strong><span class="classifier">float, shape = [1, n_support]</span></dt><dd><p>Device array of coefficients for support vectors</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">int</span></dt><dd><p>The constant in the decision function</p>
</dd>
<dt><strong>fit_status_</strong><span class="classifier">int</span></dt><dd><p>0 if SVM is correctly fitted</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">coef_</span></code><span class="classifier">float, shape [1, n_cols]</span></dt><dd><p>SVMBase.coef_(self)</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.svm.SVR.fit" title="cuml.svm.SVR.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, sample_weight, convert_dtype])</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.svm.SVR.predict" title="cuml.svm.SVR.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Predicts the values for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.svm.SVR.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svr.pyx#L237"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVR.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and y.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.svm.SVR.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/svm/svr.pyx#L302"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.svm.SVR.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predicts the values for X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="nearest-neighbors-classification">
<h3>Nearest Neighbors Classification<a class="headerlink" href="#nearest-neighbors-classification" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt>
<em class="property">class </em><code class="sig-prename descclassname">cuml.neighbors.</code><code class="sig-name descname">KNeighborsClassifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">'uniform'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span></dt>
<dd><p>K-Nearest Neighbors Classifier is an instance-based learning technique,
that keeps training samples around for prediction, rather than trying
to learn a generalizable set of model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors</strong><span class="classifier">int (default=5)</span></dt><dd><p>Default number of neighbors to query</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">string (default=’brute’)</span></dt><dd><p>The query algorithm to use. Currently, only ‘brute’ is supported.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">string (default=’euclidean’).</span></dt><dd><p>Distance metric to use.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">string (default=’uniform’)</span></dt><dd><p>Sample weights to use. Currently, only the uniform strategy is
supported.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">scikitlearn’s KNeighborsClassifier</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>
  <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
       <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.fit" title="cuml.neighbors.KNeighborsClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit a GPU index for k-nearest neighbors classifier model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.get_param_names" title="cuml.neighbors.KNeighborsClassifier.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.predict" title="cuml.neighbors.KNeighborsClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Use the trained k-nearest neighbors classifier to</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.predict_proba" title="cuml.neighbors.KNeighborsClassifier.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Use the trained k-nearest neighbors classifier to</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt>
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L163"><span class="viewcode-link">[source]</span></a></dt>
<dd><blockquote>
<div><p>Fit a GPU index for k-nearest neighbors classifier model.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt>
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L306"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>
<dl class="py method">
<dt>
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L184"><span class="viewcode-link">[source]</span></a></dt>
<dd><blockquote>
<div><p>Use the trained k-nearest neighbors classifier to
predict the labels for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Labels predicted</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt>
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L244"><span class="viewcode-link">[source]</span></a></dt>
<dd><blockquote>
<div><p>Use the trained k-nearest neighbors classifier to
predict the label probabilities for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Labels probabilities</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="nearest-neighbors-regression">
<h3>Nearest Neighbors Regression<a class="headerlink" href="#nearest-neighbors-regression" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt>
<em class="property">class </em><code class="sig-prename descclassname">cuml.neighbors.</code><code class="sig-name descname">KNeighborsRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">'uniform'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span></dt>
<dd><p>K-Nearest Neighbors Regressor is an instance-based learning technique,
that keeps training samples around for prediction, rather than trying
to learn a generalizable set of model parameters.</p>
<p>The K-Nearest Neighbors Regressor will compute the average of the
labels for the k closest neighbors and use it as the label.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors</strong><span class="classifier">int (default=5)</span></dt><dd><p>Default number of neighbors to query</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">string (default=’brute’)</span></dt><dd><p>The query algorithm to use. Currently, only ‘brute’ is supported.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">string (default=’euclidean’).</span></dt><dd><p>Distance metric to use.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">string (default=’uniform’)</span></dt><dd><p>Sample weights to use. Currently, only the uniform strategy is
supported.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">scikitlearn’s KNeighborsClassifier</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>
  <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span>
       <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span>
       <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span>
       <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span>
       <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span>
       <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span>
       <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">])</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsRegressor.fit" title="cuml.neighbors.KNeighborsRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit a GPU index for k-nearest neighbors regression model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsRegressor.get_param_names" title="cuml.neighbors.KNeighborsRegressor.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsRegressor.predict" title="cuml.neighbors.KNeighborsRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Use the trained k-nearest neighbors regression model to</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt>
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_regressor.pyx#L159"><span class="viewcode-link">[source]</span></a></dt>
<dd><blockquote>
<div><p>Fit a GPU index for k-nearest neighbors regression model.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt>
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_regressor.pyx#L228"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>
<dl class="py method">
<dt>
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_regressor.pyx#L178"><span class="viewcode-link">[source]</span></a></dt>
<dd><blockquote>
<div><p>Use the trained k-nearest neighbors regression model to
predict the labels for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_features)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-means-clustering">
<h3>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.KMeans">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">KMeans</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_clusters</span><span class="o">=</span><span class="default_value">8</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">300</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">'scalable-k-means++'</span></em>, <em class="sig-param"><span class="n">n_init</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">oversampling_factor</span><span class="o">=</span><span class="default_value">2.0</span></em>, <em class="sig-param"><span class="n">max_samples_per_batch</span><span class="o">=</span><span class="default_value">32768</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans" title="Permalink to this definition">¶</a></dt>
<dd><p>KMeans is a basic but powerful clustering method which is optimized via
Expectation Maximization. It randomly selects K data points in X, and
computes which samples are close to these points.
For every cluster of points, a mean is computed (hence the name), and this
becomes the new centroid.</p>
<p>cuML’s KMeans expects an array-like object or cuDF DataFrame, and supports
the scalable KMeans++ initialization method. This method is more stable
than randomly selecting K points.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_clusters</strong><span class="classifier">int (default = 8)</span></dt><dd><p>The number of centroids or clusters you want.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 300)</span></dt><dd><p>The more iterations of EM, the more accurate, but slower.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float64 (default = 1e-4)</span></dt><dd><p>Stopping criterion when centroid means do not change much.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = 1)</span></dt><dd><p>If you want results to be the same when you restart Python, select a
state.</p>
</dd>
<dt><strong>init</strong><span class="classifier">‘scalable-kmeans++’, ‘k-means||’ , ‘random’ or an ndarray (default = ‘scalable-k-means++’)  # noqa</span></dt><dd><p>‘scalable-k-means++’ or ‘k-means||’: Uses fast and stable scalable
kmeans++ initialization.
‘random’: Choose ‘n_cluster’ observations (rows) at random from data
for the initial centroids. If an ndarray is passed, it should be of
shape (n_clusters, n_features) and gives the initial centers.</p>
</dd>
<dt><strong>n_init: int (default = 1)</strong></dt><dd><p>Number of instances the k-means algorithm will be called with different seeds.
The final results will be from the instance that produces lowest inertia out
of n_init instances.</p>
</dd>
<dt><strong>oversampling_factor</strong><span class="classifier">float64</span></dt><dd><p>scalable k-means|| oversampling factor</p>
</dd>
<dt><strong>max_samples_per_batch</strong><span class="classifier">int (default=1&lt;&lt;15)</span></dt><dd><p>maximum number of samples to use for each batch
of the pairwise distance computation.</p>
</dd>
<dt><strong>oversampling_factor</strong><span class="classifier">int (default = 2)</span></dt><dd><p>The amount of points to sample
in scalable k-means++ initialization for potential centroids.
Increasing this value can lead to better initial centroids at the
cost of memory. The total number of centroids sampled in scalable
k-means++ is oversampling_factor * n_clusters * 8.</p>
</dd>
<dt><strong>max_samples_per_batch</strong><span class="classifier">int (default = 32768)</span></dt><dd><p>The number of data
samples to use for batches of the pairwise distance computation.
This computation is done throughout both fit predict. The default
should suit most cases. The total number of elements in the batched
pairwise distance computation is max_samples_per_batch * n_clusters.
It might become necessary to lower this number when n_clusters
becomes prohibitively large.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>KMeans requires n_clusters to be specified. This means one needs to
approximately guess or know how many clusters a dataset has. If one is not
sure, one can start with a small number of clusters, and visualize the
resulting clusters with PCA, UMAP or T-SNE, and verify that they look
appropriate.</p>
<p><strong>Applications of KMeans</strong></p>
<blockquote>
<div><p>The biggest advantage of KMeans is its speed and simplicity. That is
why KMeans is many practitioner’s first choice of a clustering
algorithm. KMeans has been extensively used when the number of clusters
is approximately known, such as in big data clustering tasks,
image segmentation and medical clustering.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">scikitlearn’s Kmeans</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">cuml.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">np2cudf</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># convert numpy array to cuDF dataframe</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">'fea</span><span class="si">%d</span><span class="s1">'</span><span class="o">%</span><span class="n">i</span><span class="p">:</span><span class="n">df</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])})</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">column</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
      <span class="n">pdf</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pdf</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np2cudf</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"input:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Calling fit"</span><span class="p">)</span>
<span class="n">kmeans_float</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">kmeans_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"labels:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kmeans_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"cluster_centers:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kmeans_float</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span><span class="p">:</span>

     <span class="mi">0</span>    <span class="mi">1</span>
 <span class="mi">0</span>  <span class="mf">1.0</span>  <span class="mf">1.0</span>
 <span class="mi">1</span>  <span class="mf">1.0</span>  <span class="mf">2.0</span>
 <span class="mi">2</span>  <span class="mf">3.0</span>  <span class="mf">2.0</span>
 <span class="mi">3</span>  <span class="mf">4.0</span>  <span class="mf">3.0</span>

<span class="n">Calling</span> <span class="n">fit</span>

<span class="n">labels</span><span class="p">:</span>

   <span class="mi">0</span>    <span class="mi">0</span>
   <span class="mi">1</span>    <span class="mi">0</span>
   <span class="mi">2</span>    <span class="mi">1</span>
   <span class="mi">3</span>    <span class="mi">1</span>

<span class="n">cluster_centers</span><span class="p">:</span>

   <span class="mi">0</span>    <span class="mi">1</span>
<span class="mi">0</span>  <span class="mf">1.0</span>  <span class="mf">1.5</span>
<span class="mi">1</span>  <span class="mf">3.5</span>  <span class="mf">2.5</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>cluster_centers_</strong><span class="classifier">array</span></dt><dd><p>The coordinates of the final clusters. This represents of “mean” of
each data cluster.</p>
</dd>
<dt><strong>labels_</strong><span class="classifier">array</span></dt><dd><p>Which cluster each datapoint belongs to.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.fit" title="cuml.KMeans.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, sample_weight])</p></td>
<td><p>Compute k-means clustering with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.fit_predict" title="cuml.KMeans.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(self, X[, sample_weight])</p></td>
<td><p>Compute cluster centers and predict cluster index for each sample.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.fit_transform" title="cuml.KMeans.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Compute clustering and transform X to cluster-distance space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.get_param_names" title="cuml.KMeans.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.predict" title="cuml.KMeans.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype, sample_weight])</p></td>
<td><p>Predict the closest cluster each sample in X belongs to.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.score" title="cuml.KMeans.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self, X[, y, sample_weight, convert_dtype])</p></td>
<td><p>Opposite of the value of X on the K-means objective.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.transform" title="cuml.KMeans.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Transform X to a cluster-distance space.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.KMeans.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L319"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Compute k-means clustering with X.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.KMeans.fit_predict">
<code class="sig-name descname">fit_predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L412"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Compute cluster centers and predict cluster index for each sample.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Cluster indexes</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.KMeans.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L613"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Compute clustering and transform X to cluster-distance space.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit_transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_clusters)</span></dt><dd><p>Transformed data</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.KMeans.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L620"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.KMeans.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L520"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Predict the closest cluster each sample in X belongs to.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the predict method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Cluster indexes</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.KMeans.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L599"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.score" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Opposite of the value of X on the K-means objective.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like (device or host) shape = (n_samples,), default=None</span></dt><dd><p>The weights for each observation in X. If None, all observations
are assigned equal weight.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the score method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>Opposite of the value                                                         of X on the K-means                                                         objective.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.KMeans.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/kmeans.pyx#L535"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.KMeans.transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Transform X to a cluster-distance space.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_clusters)</span></dt><dd><p>Transformed data</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dbscan">
<h3>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.DBSCAN">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">DBSCAN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">min_samples</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_mbytes_per_batch</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">calc_core_sample_indices</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN" title="Permalink to this definition">¶</a></dt>
<dd><p>DBSCAN is a very powerful yet fast clustering technique that finds clusters
where data is concentrated. This allows DBSCAN to generalize to many
problems if the datapoints tend to congregate in larger groups.</p>
<p>cuML’s DBSCAN expects an array-like object or cuDF DataFrame, and
constructs an adjacency graph to compute the distances between close
neighbours.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>eps</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The maximum distance between 2 points such they reside in the same
neighborhood.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>min_samples</strong><span class="classifier">int (default = 5)</span></dt><dd><p>The number of samples in a neighborhood such that this group can be
considered as an important core point (including the point itself).</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>max_mbytes_per_batch</strong><span class="classifier">(optional) int64</span></dt><dd><p>Calculate batch size using no more than this number of megabytes for
the pairwise distance computation. This enables the trade-off between
runtime and memory usage for making the N^2 pairwise distance
computations more tractable for large numbers of samples.
If you are experiencing out of memory errors when running DBSCAN, you
can set this value based on the memory size of your device.
Note: this option does not set the maximum total memory used in the
DBSCAN computation and so this value will not be able to be set to
the total memory available on the device.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
<dt><strong>calc_core_sample_indices</strong><span class="classifier">(optional) boolean (default = True)</span></dt><dd><p>Indicates whether the indices of the core samples should be calculated.
The the attribute <cite>core_sample_indices_</cite> will not be used, setting this
to False will avoid unnecessary kernel launches</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>DBSCAN is very sensitive to the distance metric it is used with, and a
large assumption is that datapoints need to be concentrated in groups for
clusters to be constructed.</p>
<p><strong>Applications of DBSCAN</strong></p>
<blockquote>
<div><p>DBSCAN’s main benefit is that the number of clusters is not a
hyperparameter, and that it can find non-linearly shaped clusters.
This also allows DBSCAN to be robust to noise.
DBSCAN has been applied to analyzing particle collisions in the
Large Hadron Collider, customer segmentation in marketing analyses,
and much more.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">scikitlearn’s DBSCAN</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">DBSCAN</span>
<span class="kn">from</span> <span class="nn">cuml.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'0'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">dbscan_float</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">min_samples</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>    <span class="mi">0</span>
<span class="mi">1</span>    <span class="mi">1</span>
<span class="mi">2</span>    <span class="mi">2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>labels_</strong><span class="classifier">array-like or cuDF series</span></dt><dd><p>Which cluster each datapoint belongs to. Noisy samples are labeled as
-1. Format depends on cuml global output type and estimator
output_type.</p>
</dd>
<dt><strong>core_sample_indices_</strong><span class="classifier">array-like or cuDF series</span></dt><dd><p>The indices of the core samples. Only calculated if
calc_core_sample_indices==True</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.DBSCAN.fit" title="cuml.DBSCAN.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, out_dtype])</p></td>
<td><p>Perform DBSCAN clustering from features.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.DBSCAN.fit_predict" title="cuml.DBSCAN.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(self, X[, out_dtype])</p></td>
<td><p>Performs clustering on X and returns cluster labels.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.DBSCAN.get_param_names" title="cuml.DBSCAN.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.DBSCAN.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">out_dtype</span><span class="o">=</span><span class="default_value">'int32'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/dbscan.pyx#L210"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.DBSCAN.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Perform DBSCAN clustering from features.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>out_dtype: dtype Determines the precision of the output labels array.</strong></dt><dd><p>default: “int32”. Valid values are { “int32”, np.int32,
“int64”, np.int64}.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.DBSCAN.fit_predict">
<code class="sig-name descname">fit_predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">out_dtype</span><span class="o">=</span><span class="default_value">'int32'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/dbscan.pyx#L328"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.DBSCAN.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Performs clustering on X and returns cluster labels.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>out_dtype: dtype Determines the precision of the output labels array.</strong></dt><dd><p>default: “int32”. Valid values are { “int32”, np.int32,
“int64”, np.int64}.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>preds</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Cluster labels</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.DBSCAN.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/cluster/dbscan.pyx#L342"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.DBSCAN.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="dimensionality-reduction-and-manifold-learning">
<h2>Dimensionality Reduction and Manifold Learning<a class="headerlink" href="#dimensionality-reduction-and-manifold-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="principal-component-analysis">
<h3>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.PCA">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">PCA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">iterated_power</span><span class="o">=</span><span class="default_value">15</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">svd_solver</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">whiten</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA" title="Permalink to this definition">¶</a></dt>
<dd><p>PCA (Principal Component Analysis) is a fundamental dimensionality
reduction technique used to combine features in X in linear combinations
such that each new component captures the most information or variance of
the data. N_components is usually small, say at 3, where it can be used for
data visualization, data compression and exploratory analysis.</p>
<p>cuML’s PCA expects an array-like object or cuDF DataFrame, and provides 2
algorithms Full and Jacobi. Full (default) uses a full eigendecomposition
then selects the top K eigenvectors. The Jacobi algorithm is much faster
as it iteratively tries to correct the top K eigenvectors, but might be
less accurate.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>copy</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, then copies data then removes mean from data. False might
cause data to be overwritten with its mean centered version.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>iterated_power</strong><span class="classifier">int (default = 15)</span></dt><dd><p>Used in Jacobi solver. The more iterations, the more accurate, but
slower.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = None)</span></dt><dd><p>The number of top K singular vectors / values you want.
Must be &lt;= number(columns). If n_components is not set, then all
components are kept:</p>
<blockquote>
<div><p>n_components = min(n_samples, n_features)</p>
</div></blockquote>
</dd>
<dt><strong>random_state</strong><span class="classifier">int / None (default = None)</span></dt><dd><p>If you want results to be the same when you restart Python, select a
state.</p>
</dd>
<dt><strong>svd_solver</strong><span class="classifier">‘full’ or ‘jacobi’ or ‘auto’ (default = ‘full’)</span></dt><dd><p>Full uses a eigendecomposition of the covariance matrix then discards
components.
Jacobi is much faster as it iteratively corrects, but is less accurate.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-7)</span></dt><dd><p>Used if algorithm = “jacobi”. Smaller tolerance can increase accuracy,
but but will slow down the algorithm’s convergence.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>whiten</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, de-correlates the components. This is done by dividing them by
the corresponding singular values then multiplying by sqrt(n_samples).
Whitening allows each component to have unit variance and removes
multi-collinearity. It might be beneficial for downstream
tasks like LinearRegression where correlated features cause problems.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>PCA considers linear combinations of features, specifically those that
maximize global variance structure. This means PCA is fantastic for global
structure analyses, but weak for local relationships. Consider UMAP or
T-SNE for a locally important embedding.</p>
<p><strong>Applications of PCA</strong></p>
<blockquote>
<div><p>PCA is used extensively in practice for data visualization and data
compression. It has been used to visualize extremely large word
embeddings like Word2Vec and GloVe in 2 or 3 dimensions, large
datasets of everyday objects and images, and used to distinguish
between cancerous cells from healthy cells.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">scikitlearn’s PCA</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">cuml.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'0'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">pca_float</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pca_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'components: </span><span class="si">{</span><span class="n">pca_float</span><span class="o">.</span><span class="n">components_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'explained variance: </span><span class="si">{</span><span class="n">pca_float</span><span class="o">.</span><span class="n">_explained_variance_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="n">exp_var</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">_explained_variance_ratio_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'explained variance ratio: </span><span class="si">{</span><span class="n">exp_var</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'singular values: </span><span class="si">{</span><span class="n">pca_float</span><span class="o">.</span><span class="n">_singular_values_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'mean: </span><span class="si">{</span><span class="n">pca_float</span><span class="o">.</span><span class="n">_mean_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'noise variance: </span><span class="si">{</span><span class="n">pca_float</span><span class="o">.</span><span class="n">_noise_variance_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="n">trans_gdf_float</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Inverse: </span><span class="si">{</span><span class="n">trans_gdf_float</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="n">input_gdf_float</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">trans_gdf_float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Input: </span><span class="si">{</span><span class="n">input_gdf_float</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">components</span><span class="p">:</span>
            <span class="mi">0</span>           <span class="mi">1</span>           <span class="mi">2</span>
            <span class="mi">0</span>  <span class="mf">0.69225764</span>  <span class="o">-</span><span class="mf">0.5102837</span> <span class="o">-</span><span class="mf">0.51028395</span>
            <span class="mi">1</span> <span class="o">-</span><span class="mf">0.72165036</span> <span class="o">-</span><span class="mf">0.48949987</span>  <span class="o">-</span><span class="mf">0.4895003</span>

<span class="n">explained</span> <span class="n">variance</span><span class="p">:</span>

            <span class="mi">0</span>   <span class="mf">8.510402</span>
            <span class="mi">1</span> <span class="mf">0.48959687</span>

<span class="n">explained</span> <span class="n">variance</span> <span class="n">ratio</span><span class="p">:</span>

             <span class="mi">0</span>   <span class="mf">0.9456003</span>
             <span class="mi">1</span> <span class="mf">0.054399658</span>

<span class="n">singular</span> <span class="n">values</span><span class="p">:</span>

           <span class="mi">0</span> <span class="mf">4.1256275</span>
           <span class="mi">1</span> <span class="mf">0.9895422</span>

<span class="n">mean</span><span class="p">:</span>

          <span class="mi">0</span> <span class="mf">2.6666667</span>
          <span class="mi">1</span> <span class="mf">2.3333333</span>
          <span class="mi">2</span> <span class="mf">2.3333333</span>

<span class="n">noise</span> <span class="n">variance</span><span class="p">:</span>

      <span class="mi">0</span>  <span class="mf">0.0</span>

<span class="n">transformed</span> <span class="n">matrix</span><span class="p">:</span>
             <span class="mi">0</span>           <span class="mi">1</span>
             <span class="mi">0</span>   <span class="o">-</span><span class="mf">2.8547091</span> <span class="o">-</span><span class="mf">0.42891636</span>
             <span class="mi">1</span> <span class="o">-</span><span class="mf">0.121316016</span>  <span class="mf">0.80743366</span>
             <span class="mi">2</span>    <span class="mf">2.9760244</span> <span class="o">-</span><span class="mf">0.37851727</span>

<span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
          <span class="mi">0</span> <span class="mf">1.0000001</span> <span class="mf">3.9999993</span>       <span class="mf">4.0</span>
          <span class="mi">1</span>       <span class="mf">2.0</span> <span class="mf">2.0000002</span> <span class="mf">1.9999999</span>
          <span class="mi">2</span> <span class="mf">4.9999995</span> <span class="mf">1.0000006</span>       <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
<dt><strong>mean_</strong><span class="classifier">array</span></dt><dd><p>The column wise mean of X. Used to mean - center the data first.</p>
</dd>
<dt><strong>noise_variance_</strong><span class="classifier">float</span></dt><dd><p>From Bishop 1999’s Textbook. Used in later tasks like calculating the
estimated covariance of X.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.fit" title="cuml.PCA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, y])</p></td>
<td><p>Fit the model with X. y is currently ignored.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.fit_transform" title="cuml.PCA.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, y])</p></td>
<td><p>Fit the model with X and apply the dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.get_param_names" title="cuml.PCA.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.inverse_transform" title="cuml.PCA.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(self, X[, convert_dtype, …])</p></td>
<td><p>Transform data back to its original space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.transform" title="cuml.PCA.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Apply dimensionality reduction to X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.PCA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/pca.pyx#L439"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.PCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X. y is currently ignored.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.PCA.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/pca.pyx#L515"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.PCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit the model with X and apply the dimensionality reduction on X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>trans</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, cupyx.scipy.sparse for sparse output, shape = (n_samples, n_components)</span></dt><dd><p>Transformed values</p>
<p>For more information on how to configure cuML’s dense output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.PCA.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/pca.pyx#L751"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.PCA.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.PCA.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_sparse</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">sparse_tol</span><span class="o">=</span><span class="default_value">1e-10</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/pca.pyx#L565"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.PCA.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Transform data back to its original space.</p>
<p>In other words, return an input X_original whose transform would be X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the inverse_transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
<dt><strong>return_sparse</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>Ignored when the model is not fit on a sparse matrix
If True, the method will convert the result to a
cupyx.scipy.sparse.csr_matrix object.
NOTE: Currently, there is a loss of information when converting
to csr matrix (cusolver bug). Default will be switched to True
once this is solved.</p>
</dd>
<dt><strong>sparse_tol</strong><span class="classifier">float, optional (default = 1e-10)</span></dt><dd><p>Ignored when return_sparse=False.
If True, values in the inverse transform below this parameter
are clipped to 0.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_inv</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, cupyx.scipy.sparse for sparse output, shape = (n_samples, n_features)</span></dt><dd><p>Transformed values</p>
<p>For more information on how to configure cuML’s dense output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.PCA.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/pca.pyx#L679"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.PCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Apply dimensionality reduction to X.</p>
<p>X is projected on the first principal components previously extracted
from a training set.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense or sparse matrix containing floats or doubles.
Acceptable dense formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>trans</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, cupyx.scipy.sparse for sparse output, shape = (n_samples, n_components)</span></dt><dd><p>Transformed values</p>
<p>For more information on how to configure cuML’s dense output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="truncated-svd">
<h3>Truncated SVD<a class="headerlink" href="#truncated-svd" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.TruncatedSVD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">TruncatedSVD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">algorithm</span><span class="o">=</span><span class="default_value">'full'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">n_iter</span><span class="o">=</span><span class="default_value">15</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>TruncatedSVD is used to compute the top K singular values and vectors of a
large matrix X. It is much faster when n_components is small, such as in
the use of PCA when 3 components is used for 3D visualization.</p>
<p>cuML’s TruncatedSVD an array-like object or cuDF DataFrame, and provides 2
algorithms Full and Jacobi. Full (default) uses a full eigendecomposition
then selects the top K singular vectors. The Jacobi algorithm is much
faster as it iteratively tries to correct the top K singular vectors, but
might be less accurate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algorithm</strong><span class="classifier">‘full’ or ‘jacobi’ or ‘auto’ (default = ‘full’)</span></dt><dd><p>Full uses a eigendecomposition of the covariance matrix then discards
components.
Jacobi is much faster as it iteratively corrects, but is less accurate.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want.
Must be &lt;= number(columns).</p>
</dd>
<dt><strong>n_iter</strong><span class="classifier">int (default = 15)</span></dt><dd><p>Used in Jacobi solver. The more iterations, the more accurate, but
slower.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int / None (default = None)</span></dt><dd><p>If you want results to be the same when you restart Python, select a
state.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-7)</span></dt><dd><p>Used if algorithm = “jacobi”. Smaller tolerance can increase accuracy,
but but will slow down the algorithm’s convergence.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>TruncatedSVD (the randomized version [Jacobi]) is fantastic when the number
of components you want is much smaller than the number of features. The
approximation to the largest singular values and vectors is very robust,
however, this method loses a lot of accuracy when you want many, many
components.</p>
<p><strong>Applications of TruncatedSVD</strong></p>
<p>TruncatedSVD is also known as Latent Semantic Indexing (LSI) which
tries to find topics of a word count matrix. If X previously was
centered with mean removal, TruncatedSVD is the same as TruncatedPCA.
TruncatedSVD is also used in information retrieval tasks,
recommendation systems and data compression.</p>
<p>For additional documentation, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">scikitlearn’s TruncatedSVD docs</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">cuml.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'0'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">'2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">tsvd_float</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">"jacobi"</span><span class="p">,</span>
                          <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">)</span>
<span class="n">tsvd_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'components: </span><span class="si">{</span><span class="n">tsvd_float</span><span class="o">.</span><span class="n">components_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'explained variance: </span><span class="si">{</span><span class="n">tsvd_float</span><span class="o">.</span><span class="n">_explained_variance_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="n">exp_var</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">_explained_variance_ratio_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'explained variance ratio: </span><span class="si">{</span><span class="n">exp_var</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'singular values: </span><span class="si">{</span><span class="n">tsvd_float</span><span class="o">.</span><span class="n">_singular_values_</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="n">trans_gdf_float</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Transformed matrix: </span><span class="si">{</span><span class="n">trans_gdf_float</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="n">input_gdf_float</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">trans_gdf_float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Input matrix: </span><span class="si">{</span><span class="n">input_gdf_float</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">components</span><span class="p">:</span>            <span class="mi">0</span>           <span class="mi">1</span>          <span class="mi">2</span>
<span class="mi">0</span> <span class="mf">0.58725953</span>  <span class="mf">0.57233137</span>  <span class="mf">0.5723314</span>
<span class="mi">1</span> <span class="mf">0.80939883</span> <span class="o">-</span><span class="mf">0.41525528</span> <span class="o">-</span><span class="mf">0.4152552</span>
<span class="n">explained</span> <span class="n">variance</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">55.33908</span>
<span class="mi">1</span> <span class="mf">16.660923</span>

<span class="n">explained</span> <span class="n">variance</span> <span class="n">ratio</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">0.7685983</span>
<span class="mi">1</span> <span class="mf">0.23140171</span>

<span class="n">singular</span> <span class="n">values</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">7.439024</span>
<span class="mi">1</span> <span class="mf">4.0817795</span>

<span class="n">Transformed</span> <span class="n">Matrix</span><span class="p">:</span>
<span class="mi">0</span>           <span class="mi">1</span>         <span class="mi">2</span>
<span class="mi">0</span>   <span class="mf">5.1659107</span>    <span class="o">-</span><span class="mf">2.512643</span>
<span class="mi">1</span>   <span class="mf">3.4638448</span>    <span class="o">-</span><span class="mf">0.042223275</span>
<span class="mi">2</span>    <span class="mf">4.0809603</span>   <span class="mf">3.2164836</span>

<span class="n">Input</span> <span class="n">matrix</span><span class="p">:</span>           <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
<span class="mi">0</span>       <span class="mf">1.0</span>  <span class="mf">4.000001</span>  <span class="mf">4.000001</span>
<span class="mi">1</span> <span class="mf">2.0000005</span> <span class="mf">2.0000005</span> <span class="mf">2.0000007</span>
<span class="mi">2</span>  <span class="mf">5.000001</span> <span class="mf">0.9999999</span> <span class="mf">1.0000004</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.fit" title="cuml.TruncatedSVD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, y])</p></td>
<td><p>Fit LSI model on training cudf DataFrame X. y is currently ignored.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.fit_transform" title="cuml.TruncatedSVD.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, y])</p></td>
<td><p>Fit LSI model to X and perform dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.get_param_names" title="cuml.TruncatedSVD.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.inverse_transform" title="cuml.TruncatedSVD.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Transform X back to its original space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.transform" title="cuml.TruncatedSVD.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Perform dimensionality reduction on X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.TruncatedSVD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/tsvd.pyx#L305"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TruncatedSVD.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit LSI model on training cudf DataFrame X. y is currently ignored.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.TruncatedSVD.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/tsvd.pyx#L319"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TruncatedSVD.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit LSI model to X and perform dimensionality reduction on X.
y is currently ignored.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>trans</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_components)</span></dt><dd><p>Reduced version of X</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.TruncatedSVD.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/tsvd.pyx#L481"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TruncatedSVD.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.TruncatedSVD.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/tsvd.pyx#L385"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TruncatedSVD.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Transform X back to its original space.
Returns X_original whose transform would be X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the inverse_transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_original</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_features)</span></dt><dd><p>X in original space</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.TruncatedSVD.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/decomposition/tsvd.pyx#L435"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TruncatedSVD.transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Perform dimensionality reduction on X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will, when necessary,
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_components)</span></dt><dd><p>Reduced version of X</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="umap">
<h3>UMAP<a class="headerlink" href="#umap" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.UMAP">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">UMAP</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">15</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">n_epochs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">min_dist</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">spread</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">set_op_mix_ratio</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">local_connectivity</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">repulsion_strength</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">negative_sample_rate</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">transform_queue_size</span><span class="o">=</span><span class="default_value">4.0</span></em>, <em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">'spectral'</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">a</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">b</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">target_n_neighbors</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">target_weights</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">target_metric</span><span class="o">=</span><span class="default_value">'categorical'</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hash_input</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">optim_batch_size</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">callback</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniform Manifold Approximation and Projection</p>
<p>Finds a low dimensional embedding of the data that approximates
an underlying manifold.</p>
<p>Adapted from <a class="reference external" href="https://github.com/lmcinnes/umap/blob/master/umap/">https://github.com/lmcinnes/umap/blob/master/umap/</a><a class="reference internal" href="#umap">umap</a>.py</p>
<p>The UMAP algorithm is outlined in [1]. This implementation follows the
GPU-accelerated version as described in [2].</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>n_neighbors: float (optional, default 15)</strong></dt><dd><p>The size of local neighborhood (in terms of number of neighboring
sample points) used for manifold approximation. Larger values
result in more global views of the manifold, while smaller
values result in more local data being preserved. In general
values should be in the range 2 to 100.</p>
</dd>
<dt><strong>n_components: int (optional, default 2)</strong></dt><dd><p>The dimension of the space to embed into. This defaults to 2 to
provide easy visualization, but can reasonably be set to any</p>
</dd>
<dt><strong>n_epochs: int (optional, default None)</strong></dt><dd><p>The number of training epochs to be used in optimizing the
low dimensional embedding. Larger values result in more accurate
embeddings. If None is specified a value will be selected based on
the size of the input dataset (200 for large datasets, 500 for small).</p>
</dd>
<dt><strong>learning_rate: float (optional, default 1.0)</strong></dt><dd><p>The initial learning rate for the embedding optimization.</p>
</dd>
<dt><strong>init: string (optional, default ‘spectral’)</strong></dt><dd><p>How to initialize the low dimensional embedding. Options are:</p>
<ul class="simple">
<li><p>‘spectral’: use a spectral embedding of the fuzzy 1-skeleton</p></li>
<li><p>‘random’: assign initial embedding positions at random.</p></li>
</ul>
</dd>
<dt><strong>min_dist: float (optional, default 0.1)</strong></dt><dd><p>The effective minimum distance between embedded points. Smaller values
will result in a more clustered/clumped embedding where nearby points
on the manifold are drawn closer together, while larger values will
result on a more even dispersal of points. The value should be set
relative to the <code class="docutils literal notranslate"><span class="pre">spread</span></code> value, which determines the scale at which
embedded points will be spread out.</p>
</dd>
<dt><strong>spread: float (optional, default 1.0)</strong></dt><dd><p>The effective scale of embedded points. In combination with
<code class="docutils literal notranslate"><span class="pre">min_dist</span></code> this determines how clustered/clumped the embedded
points are.</p>
</dd>
<dt><strong>set_op_mix_ratio: float (optional, default 1.0)</strong></dt><dd><p>Interpolate between (fuzzy) union and intersection as the set operation
used to combine local fuzzy simplicial sets to obtain a global fuzzy
simplicial sets. Both fuzzy set operations use the product t-norm.
The value of this parameter should be between 0.0 and 1.0; a value of
1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
intersection.</p>
</dd>
<dt><strong>local_connectivity: int (optional, default 1)</strong></dt><dd><p>The local connectivity required – i.e. the number of nearest
neighbors that should be assumed to be connected at a local level.
The higher this value the more connected the manifold becomes
locally. In practice this should be not more than the local intrinsic
dimension of the manifold.</p>
</dd>
<dt><strong>repulsion_strength: float (optional, default 1.0)</strong></dt><dd><p>Weighting applied to negative samples in low dimensional embedding
optimization. Values higher than one will result in greater weight
being given to negative samples.</p>
</dd>
<dt><strong>negative_sample_rate: int (optional, default 5)</strong></dt><dd><p>The number of negative samples to select per positive sample
in the optimization process. Increasing this value will result
in greater repulsive force being applied, greater optimization
cost, but slightly more accuracy.</p>
</dd>
<dt><strong>transform_queue_size: float (optional, default 4.0)</strong></dt><dd><p>For transform operations (embedding new points using a trained model
this will control how aggressively to search for nearest neighbors.
Larger values will result in slower performance but more accurate
nearest neighbor evaluation.</p>
</dd>
<dt><strong>a: float (optional, default None)</strong></dt><dd><p>More specific parameters controlling the embedding. If None these
values are set automatically as determined by <code class="docutils literal notranslate"><span class="pre">min_dist</span></code> and
<code class="docutils literal notranslate"><span class="pre">spread</span></code>.</p>
</dd>
<dt><strong>b: float (optional, default None)</strong></dt><dd><p>More specific parameters controlling the embedding. If None these
values are set automatically as determined by <code class="docutils literal notranslate"><span class="pre">min_dist</span></code> and
<code class="docutils literal notranslate"><span class="pre">spread</span></code>.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>hash_input: bool, optional (default = False)</strong></dt><dd><p>UMAP can hash the training input so that exact embeddings
are returned when transform is called on the same data upon
which the model was trained. This enables consistent
behavior between calling <code class="docutils literal notranslate"><span class="pre">model.fit_transform(X)</span></code> and
calling <code class="docutils literal notranslate"><span class="pre">model.fit(X).transform(X)</span></code>. Not that the CPU-based
UMAP reference implementation does this by default. This
feature is made optional in the GPU version due to the
significant overhead in copying memory to the host for
computing the hash.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>random_state is the seed used by the random number generator during
embedding initialization and during sampling used by the optimizer.
Note: Unfortunately, achieving a high amount of parallelism during
the optimization stage often comes at the expense of determinism,
since many floating-point additions are being made in parallel
without a deterministic ordering. This causes slightly different
results across training sessions, even when the same seed is used
for random number generation. Setting a random_state will enable
consistency of trained embeddings, allowing for reproducible results
to 3 digits of precision, but will do so at the expense of potentially
slower training and increased memory usage.</p>
</dd>
<dt><strong>optim_batch_size: int (optional, default 100000 / n_components)</strong></dt><dd><p>Used to maintain the consistency of embeddings for large datasets.
The optimization step will be processed with at most optim_batch_size
edges at once preventing inconsistencies. A lower batch size will yield
more consistently repeatable embeddings at the cost of speed.</p>
</dd>
<dt><strong>callback: An instance of GraphBasedDimRedCallback class</strong></dt><dd><p>Used to intercept the internal state of embeddings while they are being
trained. Example of callback usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.internals</span> <span class="kn">import</span> <span class="n">GraphBasedDimRedCallback</span>

<span class="k">class</span> <span class="nc">CustomCallback</span><span class="p">(</span><span class="n">GraphBasedDimRedCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_preprocess_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">())</span>
</pre></div>
</div>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This module is heavily based on Leland McInnes’ reference UMAP package.
However, there are a number of differences and features that are not yet
implemented in <cite>cuml.umap</cite>:</p>
<ul class="simple">
<li><p>Using a pre-computed pairwise distance matrix (under consideration
for future releases)</p></li>
<li><p>Manual initialization of initial embedding positions</p></li>
</ul>
<p>In addition to these missing features, you should expect to see
the final embeddings differing between cuml.umap and the reference
UMAP. In particular, the reference UMAP uses an approximate kNN
algorithm for large data sizes while cuml.umap always uses exact
kNN.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r37dd27ff64dd-1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/abs/1802.03426">Leland McInnes, John Healy, James Melville
UMAP: Uniform Manifold Approximation and Projection for Dimension
Reduction</a></p>
</dd>
<dt class="label" id="r37dd27ff64dd-2"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/abs/2008.00325">Corey Nolet, Victor Lafargue, Edward Raff, Thejaswi Nanditale,
Tim Oates, John Zedlewski, Joshua Patterson
Bringing UMAP Closer to the Speed of Light with GPU Acceleration</a></p>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.find_ab_params" title="cuml.UMAP.find_ab_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">find_ab_params</span></code></a>(spread, min_dist)</p></td>
<td><p>Function taken from UMAP-learn : <a class="reference external" href="https://github.com/lmcinnes/umap">https://github.com/lmcinnes/umap</a> Fit a, b params for the differentiable curve used in lower dimensional fuzzy simplicial complex construction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.UMAP.fit" title="cuml.UMAP.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, y, convert_dtype, knn_graph])</p></td>
<td><p>Fit X into an embedded space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.fit_transform" title="cuml.UMAP.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, y, convert_dtype, …])</p></td>
<td><p>Fit X into an embedded space and return that transformed</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.UMAP.get_param_names" title="cuml.UMAP.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.transform" title="cuml.UMAP.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X[, convert_dtype, knn_graph])</p></td>
<td><p>Transform X into the existing embedded space and return that</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.UMAP.validate_hyperparams" title="cuml.UMAP.validate_hyperparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validate_hyperparams</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.UMAP.find_ab_params">
<em class="property">static </em><code class="sig-name descname">find_ab_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">spread</span></em>, <em class="sig-param"><span class="n">min_dist</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/umap.pyx#L437"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.UMAP.find_ab_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Function taken from UMAP-learn : <a class="reference external" href="https://github.com/lmcinnes/umap">https://github.com/lmcinnes/umap</a>
Fit a, b params for the differentiable curve used in lower
dimensional fuzzy simplicial complex construction. We want the
smooth curve (from a pre-defined family with simple gradient) that
best matches an offset exponential decay.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.UMAP.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">knn_graph</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/umap.pyx#L513"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.UMAP.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit X into an embedded space.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
<dt><strong>knn_graph</strong><span class="classifier">sparse array-like (device or host)</span></dt><dd><p>shape=(n_samples, n_samples)
A sparse array containing the k-nearest neighbors of X,
where the columns are the nearest neighbor indices
for each row and the values are their distances.
It’s important that <cite>k&gt;=n_neighbors</cite>,
so that UMAP can model the neighbors from this graph,
instead of building its own internally.
Users using the knn_graph parameter provide UMAP
with their own run of the KNN algorithm. This allows the user
to pick a custom distance function (sometimes useful
on certain datasets) whereas UMAP uses euclidean by default.
The custom distance function should match the metric used
to train UMAP embeedings. Storing and reusing a knn_graph
will also provide a speedup to the UMAP algorithm
when performing a grid search.
Acceptable formats: sparse SciPy ndarray, CuPy device ndarray,
CSR/COO preferred other formats will go through conversion to CSR</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.UMAP.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">knn_graph</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/umap.pyx#L625"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.UMAP.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit X into an embedded space and return that transformed
output.</p>
<p>There is a subtle difference between calling fit_transform(X)
and calling fit().transform(). Calling fit_transform(X) will
train the embeddings on X and return the embeddings. Calling
fit(X).transform(X) will train the embeddings on X and then
run a second optimization.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
<dt><strong>knn_graph</strong><span class="classifier">sparse array-like (device or host)</span></dt><dd><p>shape=(n_samples, n_samples)
A sparse array containing the k-nearest neighbors of X,
where the columns are the nearest neighbor indices
for each row and the values are their distances.
It’s important that <cite>k&gt;=n_neighbors</cite>,
so that UMAP can model the neighbors from this graph,
instead of building its own internally.
Users using the knn_graph parameter provide UMAP
with their own run of the KNN algorithm. This allows the user
to pick a custom distance function (sometimes useful
on certain datasets) whereas UMAP uses euclidean by default.
The custom distance function should match the metric used
to train UMAP embeedings. Storing and reusing a knn_graph
will also provide a speedup to the UMAP algorithm
when performing a grid search.
Acceptable formats: sparse SciPy ndarray, CuPy device ndarray,
CSR/COO preferred other formats will go through conversion to CSR</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_components)</span></dt><dd><p>Embedding of the                                                        data in                                                        low-dimensional space.</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.UMAP.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/umap.pyx#L774"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.UMAP.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.UMAP.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">knn_graph</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/umap.pyx#L673"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.UMAP.transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Transform X into the existing embedded space and return that
transformed output.</p>
<p>Please refer to the reference UMAP implementation for information
on the differences between fit_transform() and running fit()
transform().</p>
<p>Specifically, the transform() function is stochastic:
<a class="reference external" href="https://github.com/lmcinnes/umap/issues/158">https://github.com/lmcinnes/umap/issues/158</a></p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
<dt><strong>knn_graph</strong><span class="classifier">sparse array-like (device or host)</span></dt><dd><p>shape=(n_samples, n_samples)
A sparse array containing the k-nearest neighbors of X,
where the columns are the nearest neighbor indices
for each row and the values are their distances.
It’s important that <cite>k&gt;=n_neighbors</cite>,
so that UMAP can model the neighbors from this graph,
instead of building its own internally.
Users using the knn_graph parameter provide UMAP
with their own run of the KNN algorithm. This allows the user
to pick a custom distance function (sometimes useful
on certain datasets) whereas UMAP uses euclidean by default.
The custom distance function should match the metric used
to train UMAP embeedings. Storing and reusing a knn_graph
will also provide a speedup to the UMAP algorithm
when performing a grid search.
Acceptable formats: sparse SciPy ndarray, CuPy device ndarray,
CSR/COO preferred other formats will go through conversion to CSR</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_components)</span></dt><dd><p>Embedding of the                                                        data in                                                        low-dimensional space.</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.UMAP.validate_hyperparams">
<code class="sig-name descname">validate_hyperparams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/umap.pyx#L388"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.UMAP.validate_hyperparams" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="random-projections">
<h3>Random Projections<a class="headerlink" href="#random-projections" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.random_projection.GaussianRandomProjection">
<em class="property">class </em><code class="sig-prename descclassname">cuml.random_projection.</code><code class="sig-name descname">GaussianRandomProjection</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.random_projection.GaussianRandomProjection" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian Random Projection method derivated from BaseRandomProjection
class.</p>
<p>Random projection is a dimensionality reduction technique. Random
projection methods are powerful methods known for their simplicity,
computational efficiency and restricted model size.
This algorithm also has the advantage to preserve distances well between
any two samples and is thus suitable for methods having this requirement.</p>
<p>The components of the random matrix are drawn from N(0, 1 / n_components).</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = ‘auto’)</span></dt><dd><p>Dimensionality of the target projection space. If set to ‘auto’,
the parameter is deducted thanks to Johnson–Lindenstrauss lemma.
The automatic deduction make use of the number of samples and
the eps parameter.</p>
<p>The Johnson–Lindenstrauss lemma can produce very conservative
n_components parameter as it makes no assumption on dataset structure.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float (default = 0.1)</span></dt><dd><p>Error tolerance during projection. Used by Johnson–Lindenstrauss
automatic deduction when n_components is set to ‘auto’.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed used to initilize random generator</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This class is unable to be used with <code class="docutils literal notranslate"><span class="pre">sklearn.base.clone()</span></code> and will
raise an exception when called.</p>
<p>Inspired by Scikit-learn’s implementation :
<a class="reference external" href="https://scikit-learn.org/stable/modules/random_projection.html">https://scikit-learn.org/stable/modules/random_projection.html</a></p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.random_projection</span> <span class="kn">import</span> <span class="n">GaussianRandomProjection</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># dataset generation</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span>
                          <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># model fitting</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianRandomProjection</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                 <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># dataset transformation</span>
<span class="n">transformed_data</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># classifier training</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># classifier scoring</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># measure information preservation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Score: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Score</span><span class="p">:</span> <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>gaussian_method</strong><span class="classifier">boolean</span></dt><dd><p>To be passed to base class in order to determine
random matrix generation method</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.random_projection.GaussianRandomProjection.get_param_names" title="cuml.random_projection.GaussianRandomProjection.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.random_projection.GaussianRandomProjection.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/random_projection/random_projection.pyx#L444"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.random_projection.GaussianRandomProjection.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.random_projection.SparseRandomProjection">
<em class="property">class </em><code class="sig-prename descclassname">cuml.random_projection.</code><code class="sig-name descname">SparseRandomProjection</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">density</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">dense_output</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.random_projection.SparseRandomProjection" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse Random Projection method derivated from BaseRandomProjection class.</p>
<p>Random projection is a dimensionality reduction technique. Random
projection methods are powerful methods known for their simplicity,
computational efficiency and restricted model size.
This algorithm also has the advantage to preserve distances well between
any two samples and is thus suitable for methods having this requirement.</p>
<p>Sparse random matrix is an alternative to dense random projection matrix
(e.g. Gaussian) that guarantees similar embedding quality while being much
more memory efficient and allowing faster computation of the projected data
(with sparse enough matrices).
If we note <code class="docutils literal notranslate"><span class="pre">s</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">density</span></code> the components of the random matrix are
drawn from:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-sqrt(s)</span> <span class="pre">/</span> <span class="pre">sqrt(n_components)</span></code> - with probability <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">2s</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">0</span></code> - with probability <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">s</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">+sqrt(s)</span> <span class="pre">/</span> <span class="pre">sqrt(n_components)</span></code> - with probability <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">2s</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = ‘auto’)</span></dt><dd><p>Dimensionality of the target projection space. If set to ‘auto’,
the parameter is deducted thanks to Johnson–Lindenstrauss lemma.
The automatic deduction make use of the number of samples and
the eps parameter.
The Johnson–Lindenstrauss lemma can produce very conservative
n_components parameter as it makes no assumption on dataset structure.</p>
</dd>
<dt><strong>density</strong><span class="classifier">float in range (0, 1] (default = ‘auto’)</span></dt><dd><p>Ratio of non-zero component in the random projection matrix.
If density = ‘auto’, the value is set to the minimum density
as recommended by Ping Li et al.: 1 / sqrt(n_features).</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float (default = 0.1)</span></dt><dd><p>Error tolerance during projection. Used by Johnson–Lindenstrauss
automatic deduction when n_components is set to ‘auto’.</p>
</dd>
<dt><strong>dense_output</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If set to True transformed matrix will be dense otherwise sparse.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed used to initilize random generator</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This class is unable to be used with <code class="docutils literal notranslate"><span class="pre">sklearn.base.clone()</span></code> and will
raise an exception when called.</p>
<p>Inspired by Scikit-learn’s <a class="reference external" href="https://scikit-learn.org/stable/modules/random_projection.html">implementation</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.random_projection</span> <span class="kn">import</span> <span class="n">SparseRandomProjection</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># dataset generation</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span>
                          <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># model fitting</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SparseRandomProjection</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># dataset transformation</span>
<span class="n">transformed_data</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># classifier training</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># classifier scoring</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># measure information preservation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Score: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Score</span><span class="p">:</span> <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>gaussian_method</strong><span class="classifier">boolean</span></dt><dd><p>To be passed to base class in order to determine
random matrix generation method</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.random_projection.SparseRandomProjection.get_param_names" title="cuml.random_projection.SparseRandomProjection.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.random_projection.SparseRandomProjection.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/random_projection/random_projection.pyx#L585"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.random_projection.SparseRandomProjection.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.random_projection.johnson_lindenstrauss_min_dim">
<code class="sig-prename descclassname">random_projection.</code><code class="sig-name descname">johnson_lindenstrauss_min_dim</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_samples</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/random_projection/random_projection.pyx#L71"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.random_projection.johnson_lindenstrauss_min_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>In mathematics, the Johnson–Lindenstrauss lemma states that
high-dimensional data can be embedded into lower dimension while preserving
the distances.</p>
<p>With p the random projection :
(1 - eps) ||u - v||^2 &lt; ||p(u) - p(v)||^2 &lt; (1 + eps) ||u - v||^2</p>
<p>This function finds the minimum number of components to guarantee that
the embedding is inside the eps error tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int</span></dt><dd><p>Number of samples.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float in (0,1) (default = 0.1)</span></dt><dd><p>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>n_components</strong><span class="classifier">int</span></dt><dd><p>The minimal number of components to guarantee with good probability
an eps-embedding with n_samples.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="tsne">
<h3>TSNE<a class="headerlink" href="#tsne" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.TSNE">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">TSNE</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">perplexity</span><span class="o">=</span><span class="default_value">30.0</span></em>, <em class="sig-param"><span class="n">early_exaggeration</span><span class="o">=</span><span class="default_value">12.0</span></em>, <em class="sig-param"><span class="n">learning_rate</span><span class="o">=</span><span class="default_value">200.0</span></em>, <em class="sig-param"><span class="n">n_iter</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">n_iter_without_progress</span><span class="o">=</span><span class="default_value">300</span></em>, <em class="sig-param"><span class="n">min_grad_norm</span><span class="o">=</span><span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">'random'</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">'barnes_hut'</span></em>, <em class="sig-param"><span class="n">angle</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">learning_rate_method</span><span class="o">=</span><span class="default_value">'adaptive'</span></em>, <em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">90</span></em>, <em class="sig-param"><span class="n">perplexity_max_iter</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">exaggeration_iter</span><span class="o">=</span><span class="default_value">250</span></em>, <em class="sig-param"><span class="n">pre_momentum</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">post_momentum</span><span class="o">=</span><span class="default_value">0.8</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TSNE" title="Permalink to this definition">¶</a></dt>
<dd><p>TSNE (T-Distributed Stochastic Neighbor Embedding) is an extremely
powerful dimensionality reduction technique that aims to maintain
local distances between data points. It is extremely robust to whatever
dataset you give it, and is used in many areas including cancer research,
music analysis and neural network weight visualizations.</p>
<p>Currently, cuML’s TSNE supports the fast Barnes Hut O(NlogN) TSNE
approximation (derived from CannyLabs’ BH open source CUDA code). This
allows TSNE to produce extremely fast embeddings when n_components = 2.
cuML defaults to this algorithm. A slower but more accurate Exact
algorithm is also provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_components</strong><span class="classifier">int (default 2)</span></dt><dd><p>The output dimensionality size. Currently only size=2 is tested and
supported, but the ‘exact’ algorithm will support greater
dimensionality in future.</p>
</dd>
<dt><strong>perplexity</strong><span class="classifier">float (default 30.0)</span></dt><dd><p>Larger datasets require a larger value. Consider choosing different
perplexity values from 5 to 50 and see the output differences.</p>
</dd>
<dt><strong>early_exaggeration</strong><span class="classifier">float (default 12.0)</span></dt><dd><p>Controls the space between clusters. Not critical to tune this.</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">float (default 200.0)</span></dt><dd><p>The learning rate usually between (10, 1000). If this is too high,
TSNE could look like a cloud / ball of points.</p>
</dd>
<dt><strong>n_iter</strong><span class="classifier">int (default 1000)</span></dt><dd><p>The more epochs, the more stable/accurate the final embedding.</p>
</dd>
<dt><strong>n_iter_without_progress</strong><span class="classifier">int (default 300)</span></dt><dd><p>Currently unused. When the KL Divergence becomes too small after some
iterations, terminate TSNE early.</p>
</dd>
<dt><strong>min_grad_norm</strong><span class="classifier">float (default 1e-07)</span></dt><dd><p>The minimum gradient norm for when TSNE will terminate early.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">str ‘euclidean’ only (default ‘euclidean’)</span></dt><dd><p>Currently only supports euclidean distance. Will support cosine in
a future release.</p>
</dd>
<dt><strong>init</strong><span class="classifier">str ‘random’ (default ‘random’)</span></dt><dd><p>Currently supports random intialization.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default None)</span></dt><dd><p>Setting this can allow future runs of TSNE to look mostly the same.
It is known that TSNE tends to have vastly different outputs on
many runs. Try using PCA intialization (upcoming with change #1098)
to possibly counteract this problem.
It is known that small perturbations can directly
change the result of the embedding for parallel TSNE implementations.</p>
</dd>
<dt><strong>method</strong><span class="classifier">str ‘barnes_hut’ or ‘exact’ (default ‘barnes_hut’)</span></dt><dd><p>Options are either barnes_hut or exact. It is recommended that you use
the barnes hut approximation for superior O(nlogn) complexity.</p>
</dd>
<dt><strong>angle</strong><span class="classifier">float (default 0.5)</span></dt><dd><p>Tradeoff between accuracy and speed. Choose between (0,2 0.8) where
closer to one indicates full accuracy but slower speeds.</p>
</dd>
<dt><strong>learning_rate_method</strong><span class="classifier">str ‘adaptive’, ‘none’ or None (default ‘adaptive’)</span></dt><dd><p>Either adaptive or None. Uses a special adpative method that tunes
the learning rate, early exaggeration and perplexity automatically
based on input size.</p>
</dd>
<dt><strong>n_neighbors</strong><span class="classifier">int (default 90)</span></dt><dd><p>The number of datapoints you want to use in the
attractive forces. Smaller values are better for preserving
local structure, whilst larger values can improve global structure
preservation. Default is 3 * 30 (perplexity)</p>
</dd>
<dt><strong>perplexity_max_iter</strong><span class="classifier">int (default 100)</span></dt><dd><p>The number of epochs the best gaussian bands are found for.</p>
</dd>
<dt><strong>exaggeration_iter</strong><span class="classifier">int (default 250)</span></dt><dd><p>To promote the growth of clusters, set this higher.</p>
</dd>
<dt><strong>pre_momentum</strong><span class="classifier">float (default 0.5)</span></dt><dd><p>During the exaggeration iteration, more forcefully apply gradients.</p>
</dd>
<dt><strong>post_momentum</strong><span class="classifier">float (default 0.8)</span></dt><dd><p>During the late phases, less forcefully apply gradients.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r567d6098fcf6-1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://lvdmaaten.github.io/tsne/">van der Maaten, L.J.P.
t-Distributed Stochastic Neighbor Embedding</a></p>
</dd>
<dt class="label" id="r567d6098fcf6-2"><span class="brackets">2</span></dt>
<dd><p>van der Maaten, L.J.P.; Hinton, G.E.
Visualizing High-Dimensional Data
Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</p>
</dd>
<dt class="label" id="r567d6098fcf6-3"><span class="brackets">3</span></dt>
<dd><p>George C. Linderman, Manas Rachh, Jeremy G. Hoskins,
Stefan Steinerberger, Yuval Kluger Efficient Algorithms for
t-distributed Stochastic Neighborhood Embedding</p>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Maaten and Linderman showcased how TSNE can be very sensitive to both
the starting conditions (ie random initialization), and how parallel
versions of TSNE can generate vastly different results. It has been
suggested that you run TSNE a few times to settle on the best
configuration. Notice specifying random_state and fixing it across runs
can help, but TSNE does not guarantee similar results each time.</p>
<p>As suggested, PCA (upcoming with change #1098) can also help to
alleviate this issue.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The CUDA implementation is derived from the excellent CannyLabs open
source implementation here: <a class="reference external" href="https://github.com/CannyLab/tsne-cuda/">https://github.com/CannyLab/tsne-cuda/</a>. The
CannyLabs code is licensed according to the conditions in
cuml/cpp/src/tsne/ cannylabs_tsne_license.txt. A full description of
their approach is available in their article t-SNE-CUDA:
GPU-Accelerated t-SNE and its Applications to Modern Data
(<a class="reference external" href="https://arxiv.org/abs/1807.11824">https://arxiv.org/abs/1807.11824</a>).</p>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TSNE.fit" title="cuml.TSNE.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Fit X into an embedded space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TSNE.fit_transform" title="cuml.TSNE.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Fit X into an embedded space and return that transformed output.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TSNE.get_param_names" title="cuml.TSNE.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.TSNE.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/t_sne.pyx#L325"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TSNE.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit X into an embedded space.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.TSNE.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/t_sne.pyx#L433"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TSNE.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit X into an embedded space and return that transformed output.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_components)</span></dt><dd><p>Embedding of the                                                        training data in                                                        low-dimensional space.</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.TSNE.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/manifold/t_sne.pyx#L459"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.TSNE.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="neighbors">
<h2>Neighbors<a class="headerlink" href="#neighbors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nearest-neighbors">
<h3>Nearest Neighbors<a class="headerlink" href="#nearest-neighbors" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.neighbors.NearestNeighbors">
<em class="property">class </em><code class="sig-prename descclassname">cuml.neighbors.</code><code class="sig-name descname">NearestNeighbors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">algorithm</span><span class="o">=</span><span class="default_value">'brute'</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">p</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">metric_params</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.neighbors.NearestNeighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>NearestNeighbors is an queries neighborhoods from a given set of
datapoints. Currently, cuML supports k-NN queries, which define
the neighborhood as the closest <cite>k</cite> neighbors to each query point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors</strong><span class="classifier">int (default=5)</span></dt><dd><p>Default number of neighbors to query</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">string (default=’brute’)</span></dt><dd><p>The query algorithm to use. Currently, only ‘brute’ is supported.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">string (default=’euclidean’).</span></dt><dd><p>Distance metric to use. Supported distances are [‘l1, ‘cityblock’,
‘taxicab’, ‘manhattan’, ‘euclidean’, ‘l2’, ‘braycurtis’, ‘canberra’,
‘minkowski’, ‘chebyshev’, ‘jensenshannon’, ‘cosine’, ‘correlation’]</p>
</dd>
<dt><strong>p</strong><span class="classifier">float (default=2) Parameter for the Minkowski metric. When p = 1, this</span></dt><dd><p>is equivalent to manhattan distance (l1), and euclidean distance (l2)
for p = 2. For arbitrary p, minkowski distance (lp) is used.</p>
</dd>
<dt><strong>metric_expanded</strong><span class="classifier">bool</span></dt><dd><p>Can increase performance in Minkowski-based (Lp) metrics (for p &gt; 1)
by using the expanded form and not computing the n-th roots.</p>
</dd>
<dt><strong>metric_params</strong><span class="classifier">dict, optional (default = None) This is currently ignored.</span></dt><dd></dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/cuml/blob/branch-0.15/notebooks/nearest_neighbors_demo.ipynb">the NearestNeighbors notebook</a>.</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors">scikit-learn’s NearestNeighbors</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">cuml.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># build a cudf Dataframe</span>
<span class="n">X_cudf</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># fit model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># get 3 nearest neighbors</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">)</span>

<span class="c1"># print results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">indices</span><span class="p">:</span>

     <span class="mi">0</span>   <span class="mi">1</span>   <span class="mi">2</span>
<span class="mi">0</span>    <span class="mi">0</span>  <span class="mi">14</span>  <span class="mi">21</span>
<span class="mi">1</span>    <span class="mi">1</span>  <span class="mi">19</span>   <span class="mi">8</span>
<span class="mi">2</span>    <span class="mi">2</span>   <span class="mi">9</span>  <span class="mi">23</span>
<span class="mi">3</span>    <span class="mi">3</span>  <span class="mi">14</span>  <span class="mi">21</span>
<span class="o">...</span>

<span class="mi">22</span>  <span class="mi">22</span>  <span class="mi">18</span>  <span class="mi">11</span>
<span class="mi">23</span>  <span class="mi">23</span>  <span class="mi">16</span>   <span class="mi">9</span>
<span class="mi">24</span>  <span class="mi">24</span>  <span class="mi">17</span>  <span class="mi">10</span>

<span class="n">distances</span><span class="p">:</span>

      <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
<span class="mi">0</span>   <span class="mf">0.0</span>  <span class="mf">4.883116</span>  <span class="mf">5.570006</span>
<span class="mi">1</span>   <span class="mf">0.0</span>  <span class="mf">3.047896</span>  <span class="mf">4.105496</span>
<span class="mi">2</span>   <span class="mf">0.0</span>  <span class="mf">3.558557</span>  <span class="mf">3.567704</span>
<span class="mi">3</span>   <span class="mf">0.0</span>  <span class="mf">3.806127</span>  <span class="mf">3.880100</span>
<span class="o">...</span>

<span class="mi">22</span>  <span class="mf">0.0</span>  <span class="mf">4.210738</span>  <span class="mf">4.227068</span>
<span class="mi">23</span>  <span class="mf">0.0</span>  <span class="mf">3.357889</span>  <span class="mf">3.404269</span>
<span class="mi">24</span>  <span class="mf">0.0</span>  <span class="mf">3.428183</span>  <span class="mf">3.818043</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.NearestNeighbors.fit" title="cuml.neighbors.NearestNeighbors.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Fit GPU index for performing nearest neighbor queries.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.NearestNeighbors.get_param_names" title="cuml.neighbors.NearestNeighbors.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.NearestNeighbors.kneighbors" title="cuml.neighbors.NearestNeighbors.kneighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kneighbors</span></code></a>(self[, X, n_neighbors, …])</p></td>
<td><p>Query the GPU index for the k nearest neighbors of column vectors in X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.NearestNeighbors.kneighbors_graph" title="cuml.neighbors.NearestNeighbors.kneighbors_graph"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kneighbors_graph</span></code></a>(self[, X, n_neighbors, mode])</p></td>
<td><p>Find the k nearest neighbors of column vectors in X and return as</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.neighbors.NearestNeighbors.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/nearest_neighbors.pyx#L221"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.NearestNeighbors.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit GPU index for performing nearest neighbor queries.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the train method will, when necessary, convert
y to be the same data type as X if they differ. This
will increase memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.NearestNeighbors.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/nearest_neighbors.pyx#L244"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.NearestNeighbors.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.NearestNeighbors.kneighbors">
<code class="sig-name descname">kneighbors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_distance</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/nearest_neighbors.pyx#L285"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.NearestNeighbors.kneighbors" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Query the GPU index for the k nearest neighbors of column vectors in X.</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>n_neighbors<span class="classifier">Integer</span></dt><dd><p>Number of neighbors to search. If not provided, the n_neighbors
from the model instance is used (default=10)</p>
</dd>
<dt>return_distance: Boolean</dt><dd><p>If False, distances will not be returned</p>
</dd>
<dt>convert_dtype<span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the kneighbors method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>distances</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output typeconfiguration, shape =(n_samples, n_features)</span></dt><dd><blockquote>
<div><p>The distances of the k-nearest neighbors for each column vector
in X</p>
</div></blockquote>
<dl class="simple">
<dt>indices<span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output typeconfiguration, shape =(n_samples, n_features)</span></dt><dd><p>The indices of the k-nearest neighbors for each column vector in X</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.NearestNeighbors.kneighbors_graph">
<code class="sig-name descname">kneighbors_graph</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mode</span><span class="o">=</span><span class="default_value">'connectivity'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/nearest_neighbors.pyx#L446"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.NearestNeighbors.kneighbors_graph" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Find the k nearest neighbors of column vectors in X and return as
a sparse matrix in CSR format.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd></dd>
<dt><strong>Dense matrix containing floats or doubles.</strong></dt><dd></dd>
<dt><strong>Acceptable formats: CUDA array interface compliant objects like</strong></dt><dd></dd>
<dt><strong>CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas</strong></dt><dd></dd>
<dt><strong>DataFrame/Series.</strong></dt><dd><dl class="simple">
<dt>n_neighbors<span class="classifier">Integer</span></dt><dd><p>Number of neighbors to search. If not provided, the n_neighbors
from the model instance is used</p>
</dd>
<dt>mode<span class="classifier">string (default=’connectivity’)</span></dt><dd><p>Values in connectivity matrix: ‘connectivity’ returns the
connectivity matrix with ones and zeros, ‘distance’ returns the
edges as the distances between points with the requested metric.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>A</strong><span class="classifier">sparse graph in CSR format, shape = (n_samples, n_samples_fit)</span></dt><dd><p>n_samples_fit is the number of samples in the fitted data where
A[i, j] is assigned the weight of the edge that connects i to j.
Values will either be ones/zeros or the selected distance metric.
Return types are either cupy’s CSR sparse graph (device) or
numpy’s CSR sparse graph (host)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id21">
<h3>Nearest Neighbors Classification<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.neighbors.KNeighborsClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.neighbors.</code><code class="sig-name descname">KNeighborsClassifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">'uniform'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.neighbors.KNeighborsClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>K-Nearest Neighbors Classifier is an instance-based learning technique,
that keeps training samples around for prediction, rather than trying
to learn a generalizable set of model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors</strong><span class="classifier">int (default=5)</span></dt><dd><p>Default number of neighbors to query</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">string (default=’brute’)</span></dt><dd><p>The query algorithm to use. Currently, only ‘brute’ is supported.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">string (default=’euclidean’).</span></dt><dd><p>Distance metric to use.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">string (default=’uniform’)</span></dt><dd><p>Sample weights to use. Currently, only the uniform strategy is
supported.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">scikitlearn’s KNeighborsClassifier</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>
  <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
       <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.fit" title="cuml.neighbors.KNeighborsClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit a GPU index for k-nearest neighbors classifier model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.get_param_names" title="cuml.neighbors.KNeighborsClassifier.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.predict" title="cuml.neighbors.KNeighborsClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Use the trained k-nearest neighbors classifier to</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsClassifier.predict_proba" title="cuml.neighbors.KNeighborsClassifier.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Use the trained k-nearest neighbors classifier to</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L163"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit a GPU index for k-nearest neighbors classifier model.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsClassifier.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L306"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsClassifier.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L184"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Use the trained k-nearest neighbors classifier to
predict the labels for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Labels predicted</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_classifier.pyx#L244"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Use the trained k-nearest neighbors classifier to
predict the label probabilities for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, 1)</span></dt><dd><p>Labels probabilities</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id23">
<h3>Nearest Neighbors Regression<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.neighbors.KNeighborsRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.neighbors.</code><code class="sig-name descname">KNeighborsRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">'uniform'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.neighbors.KNeighborsRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>K-Nearest Neighbors Regressor is an instance-based learning technique,
that keeps training samples around for prediction, rather than trying
to learn a generalizable set of model parameters.</p>
<p>The K-Nearest Neighbors Regressor will compute the average of the
labels for the k closest neighbors and use it as the label.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors</strong><span class="classifier">int (default=5)</span></dt><dd><p>Default number of neighbors to query</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">string (default=’brute’)</span></dt><dd><p>The query algorithm to use. Currently, only ‘brute’ is supported.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">string (default=’euclidean’).</span></dt><dd><p>Distance metric to use.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">string (default=’uniform’)</span></dt><dd><p>Sample weights to use. Currently, only the uniform strategy is
supported.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">scikitlearn’s KNeighborsClassifier</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>
  <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span>
       <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span>
       <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span>
       <span class="mf">1.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span>
       <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span>
       <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.</span>        <span class="p">,</span> <span class="mf">2.</span>        <span class="p">,</span>
       <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">3.79999995</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">])</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsRegressor.fit" title="cuml.neighbors.KNeighborsRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y[, convert_dtype])</p></td>
<td><p>Fit a GPU index for k-nearest neighbors regression model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsRegressor.get_param_names" title="cuml.neighbors.KNeighborsRegressor.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.neighbors.KNeighborsRegressor.predict" title="cuml.neighbors.KNeighborsRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X[, convert_dtype])</p></td>
<td><p>Use the trained k-nearest neighbors regression model to</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_regressor.pyx#L159"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Fit a GPU index for k-nearest neighbors regression model.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, 1)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsRegressor.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_regressor.pyx#L228"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsRegressor.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.neighbors.KNeighborsRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/neighbors/kneighbors_regressor.pyx#L178"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.neighbors.KNeighborsRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Use the trained k-nearest neighbors regression model to
predict the labels for X</p>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Dense matrix containing floats or doubles.
Acceptable formats: CUDA array interface compliant objects like
CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas
DataFrame/Series.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the method will automatically
convert the inputs to np.float32.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>X_new</strong><span class="classifier">cuDF, CuPy or NumPy object depending on cuML’s output type configuration, shape = (n_samples, n_features)</span></dt><dd><p>Predicted values</p>
<p>For more information on how to configure cuML’s output type,
refer to: <a class="reference internal" href="#id1">Output Data Type Configuration</a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="time-series">
<h2>Time Series<a class="headerlink" href="#time-series" title="Permalink to this headline">¶</a></h2>
<div class="section" id="holtwinters">
<h3>HoltWinters<a class="headerlink" href="#holtwinters" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.ExponentialSmoothing">
<em class="property">class </em><code class="sig-prename descclassname">cuml.</code><code class="sig-name descname">ExponentialSmoothing</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">endog</span></em>, <em class="sig-param"><span class="n">seasonal</span><span class="o">=</span><span class="default_value">'additive'</span></em>, <em class="sig-param"><span class="n">seasonal_periods</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">start_periods</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">ts_num</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.00224</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.ExponentialSmoothing" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a HoltWinters time series analysis model which is used in
both forecasting future entries in a time series as well as in providing
exponential smoothing, where weights are assigned against historical
data with exponentially decreasing impact. This is done by analyzing
three components of the data: level, trend, and seasonality.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>endog</strong><span class="classifier">array-like (device or host)</span></dt><dd><p>Acceptable formats: cuDF DataFrame, cuDF Series,
NumPy ndarray, Numba device ndarray, cuda array interface
compliant array like CuPy.
Note: cuDF.DataFrame types assumes data is in columns,
while all other datatypes assume data is in rows.
The endogenous dataset to be operated on.</p>
</dd>
<dt><strong>seasonal</strong><span class="classifier">‘additive’, ‘add’, ‘multiplicative’, ‘mul’         (default = ‘additive’)</span></dt><dd><p>Whether the seasonal trend should be calculated
additively or multiplicatively.</p>
</dd>
<dt><strong>seasonal_periods</strong><span class="classifier">int (default=2)</span></dt><dd><p>The seasonality of the data (how often it
repeats). For monthly data this should be 12,
for weekly data, this should be 7.</p>
</dd>
<dt><strong>start_periods</strong><span class="classifier">int (default=2)</span></dt><dd><p>Number of seasons to be used for seasonal seed values</p>
</dd>
<dt><strong>ts_num</strong><span class="classifier">int (default=1)</span></dt><dd><p>The number of different time series that were passed
in the endog param.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">np.number &gt; 0 (default=2.24e-3)</span></dt><dd><p>The accuracy to which gradient descent should achieve.
Note that changing this value may affect the forecasted results.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p><em>Known Limitations:</em> This version of ExponentialSmoothing currently
provides only a limited number of features when compared to the
<cite>statsmodels.holtwinters.ExponentialSmoothing</cite> model. Noticeably, it lacks:</p>
<ul class="simple">
<li><dl class="simple">
<dt>predict<span class="classifier">no support for in-sample prediction.</span></dt><dd><ul>
<li><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/875">https://github.com/rapidsai/cuml/issues/875</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>hessian<span class="classifier">no support for returning Hessian matrix.</span></dt><dd><ul>
<li><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/880">https://github.com/rapidsai/cuml/issues/880</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>information<span class="classifier">no support for returning Fisher matrix.</span></dt><dd><ul>
<li><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/880">https://github.com/rapidsai/cuml/issues/880</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>loglike<span class="classifier">no support for returning Log-likelihood.</span></dt><dd><ul>
<li><p><a class="reference external" href="https://github.com/rapidsai/cuml/issues/880">https://github.com/rapidsai/cuml/issues/880</a></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Additionally, be warned that there may exist floating point instability
issues in this model. Small values in endog may lead to faulty results.
See <a class="reference external" href="https://github.com/rapidsai/cuml/issues/888">https://github.com/rapidsai/cuml/issues/888</a> for more information.</p>
<p><em>Known Differences:</em> This version of ExponentialSmoothing differs from
statsmodels in some other minor ways:</p>
<ul class="simple">
<li><p>Cannot pass trend component or damped trend component</p></li>
<li><p>this version can take additional parameters <cite>eps</cite>,
<cite>start_periods</cite>, <cite>ts_num</cite>, and <cite>handle</cite></p></li>
<li><p>Score returns SSE rather than gradient logL
<a class="reference external" href="https://github.com/rapidsai/cuml/issues/876">https://github.com/rapidsai/cuml/issues/876</a></p></li>
<li><p>This version provides get_level(), get_trend(), get_season()</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">ExponentialSmoothing</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span>
                    <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span>
                    <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span>
                    <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span>
                    <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span>
                    <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">cu_hw</span> <span class="o">=</span> <span class="n">ExponentialSmoothing</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">seasonal_periods</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">cu_hw</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">cu_pred</span> <span class="o">=</span> <span class="n">cu_hw</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Forecasted points:'</span><span class="p">,</span> <span class="n">cu_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Forecasted</span> <span class="n">points</span> <span class="p">:</span>
<span class="mi">0</span>    <span class="mf">4.000143766093652</span>
<span class="mi">1</span>    <span class="mf">5.000000163513641</span>
<span class="mi">2</span>    <span class="mf">6.000000000174092</span>
<span class="mi">3</span>    <span class="mf">7.000000000000178</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.fit" title="cuml.ExponentialSmoothing.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self)</p></td>
<td><p>Perform fitting on the given <cite>endog</cite> dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.forecast" title="cuml.ExponentialSmoothing.forecast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forecast</span></code></a>(self[, h, index])</p></td>
<td><p>Forecasts future points based on the fitted model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_level" title="cuml.ExponentialSmoothing.get_level"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_level</span></code></a>(self[, index])</p></td>
<td><p>Returns the level component of the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_param_names" title="cuml.ExponentialSmoothing.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_season" title="cuml.ExponentialSmoothing.get_season"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_season</span></code></a>(self[, index])</p></td>
<td><p>Returns the season component of the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.get_trend" title="cuml.ExponentialSmoothing.get_trend"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_trend</span></code></a>(self[, index])</p></td>
<td><p>Returns the trend component of the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.ExponentialSmoothing.score" title="cuml.ExponentialSmoothing.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(self[, index])</p></td>
<td><p>Returns the score of the model.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L274"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform fitting on the given <cite>endog</cite> dataset.
Calculates the level, trend, season, and SSE components.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.forecast">
<code class="sig-name descname">forecast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">h</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">index</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L364"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.forecast" title="Permalink to this definition">¶</a></dt>
<dd><p>Forecasts future points based on the fitted model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h</strong><span class="classifier">int (default=1)</span></dt><dd><p>The number of points for each series to be forecasted.</p>
</dd>
<dt><strong>index</strong><span class="classifier">int (default=None)</span></dt><dd><p>The index of the time series from which you want
forecasted points. if None, then a cudf.DataFrame of
the forecasted points from all time series is returned.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>preds</strong><span class="classifier">cudf.DataFrame or cudf.Series</span></dt><dd><p>Series of forecasted points if index is provided.
DataFrame of all forecasted points if index=None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.get_level">
<code class="sig-name descname">get_level</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">index</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L477"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.get_level" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the level component of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>index</strong><span class="classifier">int (default=None)</span></dt><dd><p>The index of the time series from which the level will be
returned. if None, then all level components are returned
in a cudf.Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>level</strong><span class="classifier">cudf.Series or cudf.DataFrame</span></dt><dd><p>The level component of the fitted model</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L570"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.get_season">
<code class="sig-name descname">get_season</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">index</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L539"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.get_season" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the season component of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>index</strong><span class="classifier">int (default=None)</span></dt><dd><p>The index of the time series from which the season will be
returned. if None, then all season components are returned
in a cudf.Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>season: cudf.Series or cudf.DataFrame</dt><dd><p>The season component of the fitted model</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.get_trend">
<code class="sig-name descname">get_trend</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">index</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L508"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.get_trend" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the trend component of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>index</strong><span class="classifier">int (default=None)</span></dt><dd><p>The index of the time series from which the trend will be
returned. if None, then all trend components are returned
in a cudf.Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>trend</strong><span class="classifier">cudf.Series or cudf.DataFrame</span></dt><dd><p>The trend component of the fitted model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.ExponentialSmoothing.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">index</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/holtwinters.pyx#L446"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.ExponentialSmoothing.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the score of the model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently returns the SSE, rather than the gradient of the
LogLikelihood. <a class="reference external" href="https://github.com/rapidsai/cuml/issues/876">https://github.com/rapidsai/cuml/issues/876</a></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>index</strong><span class="classifier">int (default=None)</span></dt><dd><p>The index of the time series from which the SSE will be
returned. if None, then all SSEs are returned in a cudf
Series.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">np.float32, np.float64, or cudf.Series</span></dt><dd><p>The SSE of the fitted model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="arima">
<h3>ARIMA<a class="headerlink" href="#arima" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.tsa.ARIMA">
<em class="property">class </em><code class="sig-prename descclassname">cuml.tsa.</code><code class="sig-name descname">ARIMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">endog</span></em>, <em class="sig-param"><span class="n">order</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">1, 1, 1</span></em>, <em class="sig-param"><span class="n">seasonal_order</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0, 0, 0, 0</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">simple_differencing</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.tsa.ARIMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a batched ARIMA model for in- and out-of-sample
time-series prediction, with support for seasonality (SARIMA)</p>
<p>ARIMA stands for Auto-Regressive Integrated Moving Average.
See <a class="reference external" href="https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average">https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average</a></p>
<p>This class can fit an ARIMA(p,d,q) or ARIMA(p,d,q)(P,D,Q)_s model to a
batch of time series of the same length with no missing values.
The implementation is designed to give the best performance when using
large batches of time series.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>endog</strong><span class="classifier">dataframe or array-like (device or host)</span></dt><dd><p>The time series data, assumed to have each time series in columns.
Acceptable formats: cuDF DataFrame, cuDF Series, NumPy ndarray,
Numba device ndarray, cuda array interface compliant array like CuPy.</p>
</dd>
<dt><strong>order</strong><span class="classifier">Tuple[int, int, int]</span></dt><dd><p>The ARIMA order (p, d, q) of the model</p>
</dd>
<dt><strong>seasonal_order: Tuple[int, int, int, int]</strong></dt><dd><p>The seasonal ARIMA order (P, D, Q, s) of the model</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool or int (default = True)</span></dt><dd><p>Whether to include a constant trend mu in the model</p>
</dd>
<dt><strong>simple_differencing: bool or int (default = True)</strong></dt><dd><p>If True, the data is differenced before being passed to the Kalman
filter. If False, differencing is part of the state-space model.
In some cases this setting can be ignored: computing forecasts with
confidence intervals will force it to False ; fitting with the CSS
method will force it to True.
Note: that forecasts are always for the original series, whereas
statsmodels computes forecasts for the differenced series when
simple_differencing is True.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p><em>Performance:</em> Let <span class="math notranslate nohighlight">\(r=max(p+s*P, q+s*Q+1)\)</span>. The device memory used
for most operations is
<span class="math notranslate nohighlight">\(O(\mathtt{batch\_size}*\mathtt{n\_obs} + \mathtt{batch\_size}*r^2)\)</span>.
The execution time is a linear function of <cite>n_obs</cite> and <cite>batch_size</cite>
(if <cite>batch_size</cite> is large), but grows very fast with <cite>r</cite>.</p>
<p>The performance is optimized for very large batch sizes (e.g thousands of
series).</p>
<p class="rubric">References</p>
<p>This class is heavily influenced by the Python library <cite>statsmodels</cite>,
particularly <cite>statsmodels.tsa.statespace.sarimax.SARIMAX</cite>.
See <a class="reference external" href="https://www.statsmodels.org/stable/statespace.html">https://www.statsmodels.org/stable/statespace.html</a>.</p>
<p>Additionally the following book is a useful reference:
“Time Series Analysis by State Space Methods”,
J. Durbin, S.J. Koopman, 2nd Edition (2012).</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.tsa.arima</span> <span class="kn">import</span> <span class="n">ARIMA</span>

<span class="c1"># Create seasonal data with a trend, a seasonal pattern and noise</span>
<span class="n">n_obs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">)</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">],</span>
                    <span class="p">[</span><span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">]])</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="mf">0.5</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="o">*</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Fit a seasonal ARIMA model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ARIMA</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Forecast</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fc</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span> <span class="mf">0.55204599</span> <span class="o">-</span><span class="mf">0.25681163</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.57430705</span> <span class="o">-</span><span class="mf">0.2262438</span> <span class="p">]</span>
<span class="p">[</span> <span class="mf">0.48120315</span> <span class="o">-</span><span class="mf">0.20583011</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.535594</span>   <span class="o">-</span><span class="mf">0.24060046</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.57207541</span> <span class="o">-</span><span class="mf">0.26695497</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.59433647</span> <span class="o">-</span><span class="mf">0.23638713</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.50123257</span> <span class="o">-</span><span class="mf">0.21597344</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.55562342</span> <span class="o">-</span><span class="mf">0.25074379</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.59210483</span> <span class="o">-</span><span class="mf">0.27709831</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.61436589</span> <span class="o">-</span><span class="mf">0.24653047</span><span class="p">]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>order</strong><span class="classifier">ARIMAOrder</span></dt><dd><p>The ARIMA order of the model (p, d, q, P, D, Q, s, k)</p>
</dd>
<dt><strong>seasonal_order: Tuple[int, int, int, int]</strong></dt><dd><p>The seasonal ARIMA order (P, D, Q, s) of the model</p>
</dd>
<dt><strong>intercept</strong><span class="classifier">bool or int</span></dt><dd><p>Whether the model includes a constant trend mu</p>
</dd>
<dt><strong>d_y: device array</strong></dt><dd><p>Time series data on device</p>
</dd>
<dt><strong>n_obs: int</strong></dt><dd><p>Number of observations</p>
</dd>
<dt><strong>batch_size: int</strong></dt><dd><p>Number of time series in the batch</p>
</dd>
<dt><strong>dtype: numpy.dtype</strong></dt><dd><p>Floating-point type of the data and parameters</p>
</dd>
<dt><strong>niter: numpy.ndarray</strong></dt><dd><p>After fitting, contains the number of iterations before convergence
for each time series.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.fit" title="cuml.tsa.ARIMA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, start_params, object]] = None, …)</p></td>
<td><p>Fit the ARIMA model to each time series.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.forecast" title="cuml.tsa.ARIMA.forecast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forecast</span></code></a>(self, nsteps[, level])</p></td>
<td><p>Forecast the given model <cite>nsteps</cite> into the future.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.get_fit_params" title="cuml.tsa.ARIMA.get_fit_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_fit_params</span></code></a>(self)</p></td>
<td><p>Get all the fit parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.get_param_names" title="cuml.tsa.ARIMA.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>ARIMA is unable to be cloned at this time. The methods:</p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.get_params" title="cuml.tsa.ARIMA.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>ARIMA is unable to be cloned at this time. The methods:</p>
</div>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.pack" title="cuml.tsa.ARIMA.pack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pack</span></code></a>(self)</p></td>
<td><p>Pack parameters of the model into a linearized vector <cite>x</cite></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.predict" title="cuml.tsa.ARIMA.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self[, start, end, level])</p></td>
<td><p>Compute in-sample and/or out-of-sample prediction for each series</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.set_fit_params" title="cuml.tsa.ARIMA.set_fit_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_fit_params</span></code></a>(self, params, object])</p></td>
<td><p>Set all the fit parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.set_params" title="cuml.tsa.ARIMA.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>ARIMA is unable to be cloned at this time. The methods:</p>
</div>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.ARIMA.unpack" title="cuml.tsa.ARIMA.unpack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unpack</span></code></a>(self, x, np.ndarray])</p></td>
<td><p>Unpack linearized parameter vector <cite>x</cite> into the separate parameter arrays of the model</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.aic">
<em class="property">property </em><code class="sig-name descname">aic</code><a class="headerlink" href="#cuml.tsa.ARIMA.aic" title="Permalink to this definition">¶</a></dt>
<dd><p>Akaike Information Criterion</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.aicc">
<em class="property">property </em><code class="sig-name descname">aicc</code><a class="headerlink" href="#cuml.tsa.ARIMA.aicc" title="Permalink to this definition">¶</a></dt>
<dd><p>Corrected Akaike Information Criterion</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.bic">
<em class="property">property </em><code class="sig-name descname">bic</code><a class="headerlink" href="#cuml.tsa.ARIMA.bic" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian Information Criterion</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.complexity">
<em class="property">property </em><code class="sig-name descname">complexity</code><a class="headerlink" href="#cuml.tsa.ARIMA.complexity" title="Permalink to this definition">¶</a></dt>
<dd><p>Model complexity (number of parameters)</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">start_params: Optional[Mapping[str</em>, <em class="sig-param">object]] = None</em>, <em class="sig-param">opt_disp: int = -1</em>, <em class="sig-param">double h: float = 1e-8</em>, <em class="sig-param">maxiter: int = 1000</em>, <em class="sig-param">method=u'ml'</em>, <em class="sig-param">truncate: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L688"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the ARIMA model to each time series.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>start_params</strong><span class="classifier">Mapping[str, array-like] (optional)</span></dt><dd><p>A mapping (e.g dictionary) of parameter names and associated arrays
The key names are in {“mu”, “ar”, “ma”, “sar”, “sma”, “sigma2”}
The shape of the arrays are (batch_size,) for mu and sigma2
parameters and (n, batch_size) for any other type, where n is the
corresponding number of parameters of this type.
Pass None for automatic estimation (recommended)</p>
</dd>
<dt><strong>opt_disp</strong><span class="classifier">int</span></dt><dd><p>Fit diagnostic level (for L-BFGS solver):</p>
<ul class="simple">
<li><p><cite>-1</cite> for no output (default)</p></li>
<li><p><cite>0&lt;n&lt;100</cite> for output every <cite>n</cite> steps</p></li>
<li><p><cite>n&gt;100</cite> for more detailed output</p></li>
</ul>
</dd>
<dt><strong>h</strong><span class="classifier">float</span></dt><dd><p>Finite-differencing step size. The gradient is computed using
forward finite differencing:
<span class="math notranslate nohighlight">\(g = \frac{f(x + \mathtt{h}) - f(x)}{\mathtt{h}} + O(\mathtt{h})\)</span> # noqa</p>
</dd>
<dt><strong>maxiter</strong><span class="classifier">int</span></dt><dd><p>Maximum number of iterations of L-BFGS-B</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Estimation method - “css”, “css-ml” or “ml”.
CSS uses a sum-of-squares approximation.
ML estimates the log-likelihood with statespace methods.
CSS-ML starts with CSS and refines with ML.</p>
</dd>
<dt><strong>truncate</strong><span class="classifier">int</span></dt><dd><p>When using CSS, start the sum of squares after a given number of
observations</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.forecast">
<code class="sig-name descname">forecast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">nsteps</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></span></em>, <em class="sig-param"><span class="n">level</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L588"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.forecast" title="Permalink to this definition">¶</a></dt>
<dd><p>Forecast the given model <cite>nsteps</cite> into the future.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>nsteps</strong><span class="classifier">int</span></dt><dd><p>The number of steps to forecast beyond end of the given series</p>
</dd>
<dt><strong>level: float or None (default = None)</strong></dt><dd><p>Confidence level for prediction intervals, or None to return only
the point forecasts. 0 &lt; level &lt; 1</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y_fc</strong><span class="classifier">array-like</span></dt><dd><p>Forecasts. Shape = (nsteps, batch_size)</p>
</dd>
<dt>lower: array-like (device) (optional)</dt><dd><p>Lower limit of the prediction interval if level != None
Shape = (end - start, batch_size)</p>
</dd>
<dt>upper: array-like (device) (optional)</dt><dd><p>Upper limit of the prediction interval if level != None
Shape = (end - start, batch_size)</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.tsa.arima</span> <span class="kn">import</span> <span class="n">ARIMA</span>
<span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ARIMA</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">y_fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.get_fit_params">
<code class="sig-name descname">get_fit_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span> → Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><span class="p">, </span>np.ndarray<span class="p">]</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L391"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.get_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all the fit parameters. Not to be confused with get_params
Note: pack() can be used to get a compact vector of the parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt>params: Dict[str, np.ndarray]</dt><dd><p>A dictionary of parameter names and associated arrays
The key names are in {“mu”, “ar”, “ma”, “sar”, “sma”, “sigma2”}
The shape of the arrays are (batch_size,) for mu and sigma2 and
(n, batch_size) for any other type, where n is the corresponding
number of parameters of this type.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L432"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>ARIMA is unable to be cloned at this time. The methods:
<cite>get_param_names()</cite>, <cite>get_params</cite> and <cite>set_params</cite> will raise
<code class="docutils literal notranslate"><span class="pre">NotImplementedError</span></code></p>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">deep</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L441"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.get_params" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>ARIMA is unable to be cloned at this time. The methods:
<cite>get_param_names()</cite>, <cite>get_params</cite> and <cite>set_params</cite> will raise
<code class="docutils literal notranslate"><span class="pre">NotImplementedError</span></code></p>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.llf">
<em class="property">property </em><code class="sig-name descname">llf</code><a class="headerlink" href="#cuml.tsa.ARIMA.llf" title="Permalink to this definition">¶</a></dt>
<dd><p>Log-likelihood of a fit model. Shape: (batch_size,)</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.pack">
<code class="sig-name descname">pack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span> → np.ndarray<a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L936"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.pack" title="Permalink to this definition">¶</a></dt>
<dd><p>Pack parameters of the model into a linearized vector <cite>x</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">numpy ndarray</span></dt><dd><p>Packed parameter array, grouped by series.
Shape: (n_params * batch_size,)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">end</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">level</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L460"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute in-sample and/or out-of-sample prediction for each series</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>start: int (default = 0)</strong></dt><dd><p>Index where to start the predictions (0 &lt;= start &lt;= num_samples)</p>
</dd>
<dt><strong>end: int (default = None)</strong></dt><dd><p>Index where to end the predictions, excluded (end &gt; start), or
<code class="docutils literal notranslate"><span class="pre">None</span></code> to predict until the last observation</p>
</dd>
<dt><strong>level: float or None (default = None)</strong></dt><dd><p>Confidence level for prediction intervals, or None to return only
the point forecasts. <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">level</span> <span class="pre">&lt;</span> <span class="pre">1</span></code></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y_p</strong><span class="classifier">array-like (device)</span></dt><dd><p>Predictions. Shape = (end - start, batch_size)</p>
</dd>
<dt>lower: array-like (device) (optional)</dt><dd><p>Lower limit of the prediction interval if <code class="docutils literal notranslate"><span class="pre">level</span> <span class="pre">!=</span> <span class="pre">None</span></code>
Shape = (end - start, batch_size)</p>
</dd>
<dt>upper: array-like (device) (optional)</dt><dd><p>Upper limit of the prediction interval if <code class="docutils literal notranslate"><span class="pre">level</span> <span class="pre">!=</span> <span class="pre">None</span></code>
Shape = (end - start, batch_size)</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.tsa.arima</span> <span class="kn">import</span> <span class="n">ARIMA</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ARIMA</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.set_fit_params">
<code class="sig-name descname">set_fit_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Mapping<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><span class="p">, </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)">object</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L413"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.set_fit_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set all the fit parameters. Not to be confused with <code class="docutils literal notranslate"><span class="pre">set_params</span></code>
Note: <cite>unpack()</cite> can be used to load a compact vector of the
parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params:</strong></dt><dd><p>A dictionary of parameter names and associated arrays
The key names are in {“mu”, “ar”, “ma”, “sar”, “sma”, “sigma2”}
The shape of the arrays are (batch_size,) for mu and sigma2 and
(n, batch_size) for any other type, where n is the corresponding
number of parameters of this type.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L450"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.set_params" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>ARIMA is unable to be cloned at this time. The methods:
<cite>get_param_names()</cite>, <cite>get_params</cite> and <cite>set_params</cite> will raise
<code class="docutils literal notranslate"><span class="pre">NotImplementedError</span></code></p>
</div>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.ARIMA.unpack">
<code class="sig-name descname">unpack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a><span class="p">, </span>np.ndarray<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/arima.pyx#L899"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.ARIMA.unpack" title="Permalink to this definition">¶</a></dt>
<dd><p>Unpack linearized parameter vector <cite>x</cite> into the separate
parameter arrays of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">array-like</span></dt><dd><p>Packed parameter array, grouped by series.
Shape: (n_params * batch_size,)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.tsa.auto_arima.AutoARIMA">
<em class="property">class </em><code class="sig-prename descclassname">cuml.tsa.auto_arima.</code><code class="sig-name descname">AutoARIMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">endog</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">simple_differencing</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.tsa.auto_arima.AutoARIMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a batched auto-ARIMA model for in- and out-of-sample
times-series prediction.</p>
<p>This interface offers a highly customizable search, with functionality
similar to the <cite>forecast</cite> and <cite>fable</cite> packages in R. It provides an
abstraction around the underlying ARIMA models to predict and forecast as
if using a single model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>endog</strong><span class="classifier">dataframe or array-like (device or host)</span></dt><dd><p>The time series data, assumed to have each time series in columns.
Acceptable formats: cuDF DataFrame, cuDF Series, NumPy ndarray,
Numba device ndarray, cuda array interface compliant array like CuPy.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>simple_differencing: bool or int, default=True</strong></dt><dd><p>If True, the data is differenced before being passed to the Kalman
filter. If False, differencing is part of the state-space model.
See additional notes in the ARIMA docs</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The interface was influenced by the R <cite>fable</cite> package:
See <a class="reference external" href="https://fable.tidyverts.org/reference/ARIMA.html">https://fable.tidyverts.org/reference/ARIMA.html</a></p>
<p class="rubric">References</p>
<p>A useful (though outdated) reference is the paper:</p>
<dl class="citation">
<dt class="label" id="rdaa189fe5726-1"><span class="brackets">1</span></dt>
<dd><p>Rob J. Hyndman, Yeasmin Khandakar, 2008. “Automatic Time Series
Forecasting: The ‘forecast’ Package for R”, Journal of Statistical
Software 27</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml.tsa.auto_arima</span> <span class="kn">import</span> <span class="n">AutoARIMA</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoARIMA</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">D</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
             <span class="n">P</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">Q</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s2">"css"</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">"css-ml"</span><span class="p">)</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.auto_arima.AutoARIMA.fit" title="cuml.tsa.auto_arima.AutoARIMA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, double h, maxiter[, method])</p></td>
<td><p>Fits the selected models for their respective series</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.auto_arima.AutoARIMA.forecast" title="cuml.tsa.auto_arima.AutoARIMA.forecast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forecast</span></code></a>(self, nsteps[, level])</p></td>
<td><p>Forecast <cite>nsteps</cite> into the future.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.auto_arima.AutoARIMA.predict" title="cuml.tsa.auto_arima.AutoARIMA.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self[, start, end, level])</p></td>
<td><p>Compute in-sample and/or out-of-sample prediction for each series</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.tsa.auto_arima.AutoARIMA.search" title="cuml.tsa.auto_arima.AutoARIMA.search"><code class="xref py py-obj docutils literal notranslate"><span class="pre">search</span></code></a>(self[, s, d, D, p, q, P, Q, …])</p></td>
<td><p>Searches through the specified model space and associates each series to the most appropriate model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.tsa.auto_arima.AutoARIMA.summary" title="cuml.tsa.auto_arima.AutoARIMA.summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summary</span></code></a>(self)</p></td>
<td><p>Display a quick summary of the models selected by <cite>search</cite></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.tsa.auto_arima.AutoARIMA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">double h: float = 1e-8</em>, <em class="sig-param">maxiter: int = 1000</em>, <em class="sig-param">method=u'ml'</em>, <em class="sig-param">truncate: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/auto_arima.pyx#L400"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.auto_arima.AutoARIMA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the selected models for their respective series</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h</strong><span class="classifier">float</span></dt><dd><p>Finite-differencing step size used to compute gradients in ARIMA</p>
</dd>
<dt><strong>maxiter</strong><span class="classifier">int</span></dt><dd><p>Maximum number of iterations of L-BFGS-B</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Estimation method - “css”, “css-ml” or “ml”.
CSS uses a fast sum-of-squares approximation.
ML estimates the log-likelihood with statespace methods.
CSS-ML starts with CSS and refines with ML.</p>
</dd>
<dt><strong>truncate</strong><span class="classifier">int</span></dt><dd><p>When using CSS, start the sum of squares after a given number of
observations for better performance (but often a worse fit)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.auto_arima.AutoARIMA.forecast">
<code class="sig-name descname">forecast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">nsteps</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a></span></em>, <em class="sig-param"><span class="n">level</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/auto_arima.pyx#L479"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.auto_arima.AutoARIMA.forecast" title="Permalink to this definition">¶</a></dt>
<dd><p>Forecast <cite>nsteps</cite> into the future.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>nsteps</strong><span class="classifier">int</span></dt><dd><p>The number of steps to forecast beyond end of the given series</p>
</dd>
<dt><strong>level: float or None (default = None)</strong></dt><dd><p>Confidence level for prediction intervals, or None to return only
the point forecasts. 0 &lt; level &lt; 1</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y_fc</strong><span class="classifier">array-like</span></dt><dd><p>Forecasts. Shape = (nsteps, batch_size)</p>
</dd>
<dt>lower: array-like (device) (optional)</dt><dd><p>Lower limit of the prediction interval if level != None
Shape = (end - start, batch_size)</p>
</dd>
<dt>upper: array-like (device) (optional)</dt><dd><p>Upper limit of the prediction interval if level != None
Shape = (end - start, batch_size)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.auto_arima.AutoARIMA.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">end</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">level</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/auto_arima.pyx#L426"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.auto_arima.AutoARIMA.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute in-sample and/or out-of-sample prediction for each series</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>start: int</strong></dt><dd><p>Index where to start the predictions (0 &lt;= start &lt;= num_samples)</p>
</dd>
<dt><strong>end:</strong></dt><dd><p>Index where to end the predictions, excluded (end &gt; start)</p>
</dd>
<dt><strong>level: float or None (default = None)</strong></dt><dd><p>Confidence level for prediction intervals, or None to return only
the point forecasts. 0 &lt; level &lt; 1</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y_p</strong><span class="classifier">array-like (device)</span></dt><dd><p>Predictions. Shape = (end - start, batch_size)</p>
</dd>
<dt>lower: array-like (device) (optional)</dt><dd><p>Lower limit of the prediction interval if level != None
Shape = (end - start, batch_size)</p>
</dd>
<dt>upper: array-like (device) (optional)</dt><dd><p>Upper limit of the prediction interval if level != None
Shape = (end - start, batch_size)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.auto_arima.AutoARIMA.search">
<code class="sig-name descname">search</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">s=None</em>, <em class="sig-param">d=range(3)</em>, <em class="sig-param">D=range(2)</em>, <em class="sig-param">p=range(1</em>, <em class="sig-param">4)</em>, <em class="sig-param">q=range(1</em>, <em class="sig-param">4)</em>, <em class="sig-param">P=range(3)</em>, <em class="sig-param">Q=range(3)</em>, <em class="sig-param">fit_intercept=u'auto'</em>, <em class="sig-param">ic=u'aicc'</em>, <em class="sig-param">test=u'kpss'</em>, <em class="sig-param">seasonal_test=u'seas'</em>, <em class="sig-param">double h: float = 1e-8</em>, <em class="sig-param">maxiter: int = 1000</em>, <em class="sig-param">method=u'auto'</em>, <em class="sig-param">truncate: int = 0</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/auto_arima.pyx#L190"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.auto_arima.AutoARIMA.search" title="Permalink to this definition">¶</a></dt>
<dd><p>Searches through the specified model space and associates each
series to the most appropriate model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>s</strong><span class="classifier">int</span></dt><dd><p>Seasonal period. None or 0 for non-seasonal time series</p>
</dd>
<dt><strong>d</strong><span class="classifier">int, sequence or generator</span></dt><dd><p>Possible values for d (simple difference)</p>
</dd>
<dt><strong>D</strong><span class="classifier">int, sequence or generator</span></dt><dd><p>Possible values for D (seasonal difference)</p>
</dd>
<dt><strong>p</strong><span class="classifier">int, sequence or generator</span></dt><dd><p>Possible values for p (AR order)</p>
</dd>
<dt><strong>q</strong><span class="classifier">int, sequence or generator</span></dt><dd><p>Possible values for q (MA order)</p>
</dd>
<dt><strong>P</strong><span class="classifier">int, sequence or generator</span></dt><dd><p>Possible values for P (seasonal AR order)</p>
</dd>
<dt><strong>Q</strong><span class="classifier">int, sequence or generator</span></dt><dd><p>Possible values for Q (seasonal MA order)</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">int, sequence, generator or “auto”</span></dt><dd><p>Whether to fit an intercept. “auto” chooses based on the model
parameters: it uses an incercept iff d + D &lt;= 1</p>
</dd>
<dt><strong>ic</strong><span class="classifier">str</span></dt><dd><p>Which information criterion to use for the model selection.
Currently supported: AIC, AICc, BIC</p>
</dd>
<dt><strong>test</strong><span class="classifier">str</span></dt><dd><p>Which stationarity test to use to choose d.
Currently supported: KPSS</p>
</dd>
<dt><strong>seasonal_test</strong><span class="classifier">str</span></dt><dd><p>Which seasonality test to use to choose D.
Currently supported: seas</p>
</dd>
<dt><strong>h</strong><span class="classifier">float</span></dt><dd><p>Finite-differencing step size used to compute gradients in ARIMA</p>
</dd>
<dt><strong>maxiter</strong><span class="classifier">int</span></dt><dd><p>Maximum number of iterations of L-BFGS-B</p>
</dd>
<dt><strong>method</strong><span class="classifier">str</span></dt><dd><p>Estimation method - “auto”, “css”, “css-ml” or “ml”.
CSS uses a fast sum-of-squares approximation.
ML estimates the log-likelihood with statespace methods.
CSS-ML starts with CSS and refines with ML.
“auto” will use CSS for long seasonal time series, ML otherwise.</p>
</dd>
<dt><strong>truncate</strong><span class="classifier">int</span></dt><dd><p>When using CSS, start the sum of squares after a given number of
observations for better performance. Recommended for long time
series when truncating doesn’t lose too much information.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.tsa.auto_arima.AutoARIMA.summary">
<code class="sig-name descname">summary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/tsa/auto_arima.pyx#L503"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.tsa.auto_arima.AutoARIMA.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Display a quick summary of the models selected by <cite>search</cite></p>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="multi-node-multi-gpu-algorithms">
<h2>Multi-Node, Multi-GPU Algorithms<a class="headerlink" href="#multi-node-multi-gpu-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id26">
<h3>K-Means Clustering<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.cluster.KMeans">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.cluster.</code><code class="sig-name descname">KMeans</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L34"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-Node Multi-GPU implementation of KMeans.</p>
<p>This version minimizes data transfer by sharing only
the centroids between workers in each iteration.</p>
<p>Predictions are done embarrassingly parallel, using cuML’s
single-GPU version.</p>
<p>For more information on this implementation, refer to the
documentation for single-GPU K-Means.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_clusters</strong><span class="classifier">int (default = 8)</span></dt><dd><p>The number of centroids or clusters you want.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 300)</span></dt><dd><p>The more iterations of EM, the more accurate, but slower.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-4)</span></dt><dd><p>Stopping criterion when centroid means do not change much.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = 1)</span></dt><dd><p>If you want results to be the same when you restart Python,
select a state.</p>
</dd>
<dt><strong>init</strong><span class="classifier">{‘scalable-kmeans++’, ‘k-means||’ , ‘random’ or an ndarray}            (default = ‘scalable-k-means++’)</span></dt><dd><p>‘scalable-k-means++’ or ‘k-means||’: Uses fast and stable scalable
kmeans++ intialization.
‘random’: Choose ‘n_cluster’ observations (rows) at random
from data for the initial centroids. If an ndarray is passed,
it should be of shape (n_clusters, n_features) and gives the
initial centers.</p>
</dd>
<dt><strong>oversampling_factor</strong><span class="classifier">int (default = 2) The amount of points to sample</span></dt><dd><p>in scalable k-means++ initialization for potential centroids.
Increasing this value can lead to better initial centroids at the
cost of memory. The total number of centroids sampled in scalable
k-means++ is oversampling_factor * n_clusters * 8.</p>
</dd>
<dt><strong>max_samples_per_batch</strong><span class="classifier">int (default = 32768) The number of data</span></dt><dd><p>samples to use for batches of the pairwise distance computation.
This computation is done throughout both fit predict. The default
should suit most cases. The total number of elements in the
batched pairwise distance computation is max_samples_per_batch
* n_clusters. It might become necessary to lower this number when
n_clusters becomes prohibitively large.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>cluster_centers_</strong><span class="classifier">cuDF DataFrame or CuPy ndarray</span></dt><dd><p>The coordinates of the final clusters. This represents of “mean” of
each data cluster.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.fit" title="cuml.dask.cluster.KMeans.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X)</p></td>
<td><p>Fit a multi-node multi-GPU KMeans model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.fit_predict" title="cuml.dask.cluster.KMeans.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(X[, delayed])</p></td>
<td><p>Compute cluster centers and predict cluster index for each sample.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.fit_transform" title="cuml.dask.cluster.KMeans.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(X[, delayed])</p></td>
<td><p>Calls fit followed by transform using a distributed KMeans model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.predict" title="cuml.dask.cluster.KMeans.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, delayed])</p></td>
<td><p>Predict labels for the input</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.score" title="cuml.dask.cluster.KMeans.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X)</p></td>
<td><p>Computes the inertia score for the trained KMeans centroids.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.cluster.KMeans.transform" title="cuml.dask.cluster.KMeans.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X[, delayed])</p></td>
<td><p>Transforms the input into the learned centroid space</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%"/>
<col style="width: 34%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>get_param_names</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.cluster.KMeans.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L120"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a multi-node multi-GPU KMeans model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd></dd>
<dt><strong>Training data to cluster.</strong></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.cluster.KMeans.fit_predict">
<code class="sig-name descname">fit_predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L155"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cluster centers and predict cluster index for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Data to predict</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>result: Dask cuDF DataFrame or CuPy backed Dask Array</dt><dd><p>Distributed object containing predictions</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.cluster.KMeans.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L192"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls fit followed by transform using a distributed KMeans model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Data to predict</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to execute as a delayed task or eager.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>result: Dask cuDF DataFrame or CuPy backed Dask Array</dt><dd><p>Distributed object containing the transformed data</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.cluster.KMeans.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L172"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict labels for the input</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Data to predict</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>result: Dask cuDF DataFrame or CuPy backed Dask Array</dt><dd><p>Distributed object containing predictions</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.cluster.KMeans.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L230"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the inertia score for the trained KMeans centroids.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask_cudf.Dataframe</span></dt><dd><p>Dataframe to compute score</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>Inertial score</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.cluster.KMeans.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/cluster/kmeans.py#L211"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.cluster.KMeans.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms the input into the learned centroid space</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Data to predict</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to execute as a delayed task or eager.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>result: Dask cuDF DataFrame or CuPy backed Dask Array</dt><dd><p>Distributed object containing the transformed data</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id27">
<h3>Nearest Neighbors<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.neighbors.NearestNeighbors">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.neighbors.</code><code class="sig-name descname">NearestNeighbors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">streams_per_handle</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/nearest_neighbors.py#L40"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.NearestNeighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-node Multi-GPU NearestNeighbors Model.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.neighbors.NearestNeighbors.fit" title="cuml.dask.neighbors.NearestNeighbors.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X)</p></td>
<td><p>Fit a multi-node multi-GPU Nearest Neighbors index</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.neighbors.NearestNeighbors.get_neighbors" title="cuml.dask.neighbors.NearestNeighbors.get_neighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_neighbors</span></code></a>(n_neighbors)</p></td>
<td><p>Returns the default n_neighbors, initialized from the constructor, if n_neighbors is None.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.neighbors.NearestNeighbors.kneighbors" title="cuml.dask.neighbors.NearestNeighbors.kneighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kneighbors</span></code></a>([X, n_neighbors, …])</p></td>
<td><p>Query the distributed nearest neighbors index</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.neighbors.NearestNeighbors.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/nearest_neighbors.py#L50"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.NearestNeighbors.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a multi-node multi-GPU Nearest Neighbors index</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask_cudf.Dataframe</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self: NearestNeighbors model</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.NearestNeighbors.get_neighbors">
<code class="sig-name descname">get_neighbors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_neighbors</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/nearest_neighbors.py#L109"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.NearestNeighbors.get_neighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the default n_neighbors, initialized from the constructor,
if n_neighbors is None.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors</strong><span class="classifier">int</span></dt><dd><p>Number of neighbors</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>n_neighbors: int</dt><dd><p>Default n_neighbors if parameter n_neighbors is none</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.NearestNeighbors.kneighbors">
<code class="sig-name descname">kneighbors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_neighbors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_distance</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">_return_futures</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/nearest_neighbors.py#L213"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.NearestNeighbors.kneighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Query the distributed nearest neighbors index</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask_cudf.Dataframe</span></dt><dd><p>Vectors to query. If not provided, neighbors of each indexed point
are returned.</p>
</dd>
<dt><strong>n_neighbors</strong><span class="classifier">int</span></dt><dd><p>Number of neighbors to query for each row in X. If not provided,
the n_neighbors on the model are used.</p>
</dd>
<dt><strong>return_distance</strong><span class="classifier">boolean (default=True)</span></dt><dd><p>If false, only indices are returned</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>ret</strong><span class="classifier">tuple (dask_cudf.DataFrame, dask_cudf.DataFrame)</span></dt><dd><p>First dask-cuDF DataFrame contains distances, second contains the
indices.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.neighbors.KNeighborsRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.neighbors.</code><code class="sig-name descname">KNeighborsRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">streams_per_handle</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_regressor.py#L35"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-node Multi-GPU K-Nearest Neighbors Regressor Model.</p>
<p>K-Nearest Neighbors Regressor is an instance-based learning technique,
that keeps training samples around for prediction, rather than trying
to learn a generalizable set of model parameters.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsRegressor.fit" title="cuml.dask.neighbors.KNeighborsRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit a multi-node multi-GPU K-Nearest Neighbors Regressor index</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsRegressor.predict" title="cuml.dask.neighbors.KNeighborsRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, convert_dtype])</p></td>
<td><p>Predict outputs for a query from previously stored index and outputs.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsRegressor.score" title="cuml.dask.neighbors.KNeighborsRegressor.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X, y)</p></td>
<td><p>Provide score by comparing predictions and ground truth.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_regressor.py#L50"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a multi-node multi-GPU K-Nearest Neighbors Regressor index</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Index data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Index output data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">KNeighborsRegressor model</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_regressor.py#L96"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict outputs for a query from previously stored index
and outputs.
The process is done in a multi-node multi-GPU fashion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Query data.
Acceptable formats: dask cuDF, dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will automatically
convert the data to the right formats.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>predictions</strong><span class="classifier">Dask futures or Dask CuPy Arrays</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsRegressor.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_regressor.py#L206"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsRegressor.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Provide score by comparing predictions and ground truth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Query test data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Outputs test data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>score</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.neighbors.KNeighborsClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.neighbors.</code><code class="sig-name descname">KNeighborsClassifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">streams_per_handle</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_classifier.py#L36"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-node Multi-GPU K-Nearest Neighbors Classifier Model.</p>
<p>K-Nearest Neighbors Classifier is an instance-based learning technique,
that keeps training samples around for prediction, rather than trying
to learn a generalizable set of model parameters.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsClassifier.fit" title="cuml.dask.neighbors.KNeighborsClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit a multi-node multi-GPU K-Nearest Neighbors Classifier index</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsClassifier.predict" title="cuml.dask.neighbors.KNeighborsClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, convert_dtype])</p></td>
<td><p>Predict labels for a query from previously stored index and index labels.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsClassifier.predict_proba" title="cuml.dask.neighbors.KNeighborsClassifier.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(X[, convert_dtype])</p></td>
<td><p>Provide score by comparing predictions and ground truth.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.neighbors.KNeighborsClassifier.score" title="cuml.dask.neighbors.KNeighborsClassifier.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X, y[, convert_dtype])</p></td>
<td><p>Predict labels for a query from previously stored index and index labels.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_classifier.py#L51"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a multi-node multi-GPU K-Nearest Neighbors Classifier index</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Index data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Index labels data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">KNeighborsClassifier model</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_classifier.py#L123"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict labels for a query from previously stored index
and index labels.
The process is done in a multi-node multi-GPU fashion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Query data.
Acceptable formats: dask cuDF, dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will automatically
convert the data to the right formats.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>predictions</strong><span class="classifier">Dask futures or Dask CuPy Arrays</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_classifier.py#L263"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Provide score by comparing predictions and ground truth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Query data.
Acceptable formats: dask cuDF, dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will automatically
convert the data to the right formats.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>probabilities</strong><span class="classifier">Dask futures or Dask CuPy Arrays</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.neighbors.KNeighborsClassifier.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/neighbors/kneighbors_classifier.py#L235"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.neighbors.KNeighborsClassifier.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict labels for a query from previously stored index
and index labels.
The process is done in a multi-node multi-GPU fashion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Query test data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>Labels test data.
Acceptable formats: dask CuPy/NumPy/Numba Array</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>score</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id28">
<h3>Principal Component Analysis<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.decomposition.PCA">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.decomposition.</code><code class="sig-name descname">PCA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/pca.py#L24"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.PCA" title="Permalink to this definition">¶</a></dt>
<dd><p>PCA (Principal Component Analysis) is a fundamental dimensionality
reduction technique used to combine features in X in linear combinations
such that each new component captures the most information or variance of
the data. N_components is usually small, say at 3, where it can be used for
data visualization, data compression and exploratory analysis.</p>
<p>cuML’s multi-node multi-gpu (MNMG) PCA expects a dask cuDF input, and
provides a “Full” algorithm. It uses a full eigendecomposition
then selects the top K eigenvectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want.
Must be &lt;= number(columns).</p>
</dd>
<dt><strong>svd_solver</strong><span class="classifier">‘full’, ‘jacobi’, or ‘tsqr’</span></dt><dd><p>‘full’: run exact full SVD and select the components by postprocessing
‘jacobi’: iteratively compute SVD of the covariance matrix</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>whiten</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, de-correlates the components. This is done by dividing them by
the corresponding singular values then multiplying by sqrt(n_samples).
Whitening allows each component to have unit variance and removes
multi-collinearity. It might be beneficial for downstream
tasks like LinearRegression where correlated features cause problems.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>PCA considers linear combinations of features, specifically those that
maximise global variance structure. This means PCA is fantastic for global
structure analyses, but weak for local relationships. Consider UMAP or
T-SNE for a locally important embedding.</p>
<p><strong>Applications of PCA</strong></p>
<blockquote>
<div><p>PCA is used extensively in practice for data visualization and data
compression. It has been used to visualize extremely large word
embeddings like Word2Vec and GloVe in 2 or 3 dimensions, large
datasets of everyday objects and images, and used to distinguish
between cancerous cells from healthy cells.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">scikitlearn’s PCA</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">wait</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.dask.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">cuml.dask.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">(</span><span class="n">threads_per_worker</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>

<span class="n">nrows</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">ncols</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_parts</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">X_cudf</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_parts</span><span class="p">,</span>
                <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">cuml</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">level_info</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wait</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Input Matrix"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_cudf</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>

<span class="n">cumlModel</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">XT</span> <span class="o">=</span> <span class="n">cumlModel</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Transformed Input Matrix"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">XT</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
          <span class="mi">0</span> <span class="o">-</span><span class="mf">6.520953</span>  <span class="mf">0.015584</span> <span class="o">-</span><span class="mf">8.828546</span>
          <span class="mi">1</span> <span class="o">-</span><span class="mf">6.507554</span>  <span class="mf">0.016524</span> <span class="o">-</span><span class="mf">8.836799</span>
          <span class="mi">2</span> <span class="o">-</span><span class="mf">6.518214</span>  <span class="mf">0.010457</span> <span class="o">-</span><span class="mf">8.821301</span>
          <span class="mi">0</span> <span class="o">-</span><span class="mf">6.520953</span>  <span class="mf">0.015584</span> <span class="o">-</span><span class="mf">8.828546</span>
          <span class="mi">1</span> <span class="o">-</span><span class="mf">6.507554</span>  <span class="mf">0.016524</span> <span class="o">-</span><span class="mf">8.836799</span>
          <span class="mi">2</span> <span class="o">-</span><span class="mf">6.518214</span>  <span class="mf">0.010457</span> <span class="o">-</span><span class="mf">8.821301</span>

<span class="n">Transformed</span> <span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
                    <span class="mi">0</span>
          <span class="mi">0</span> <span class="o">-</span><span class="mf">0.003271</span>
          <span class="mi">1</span>  <span class="mf">0.011454</span>
          <span class="mi">2</span> <span class="o">-</span><span class="mf">0.008182</span>
          <span class="mi">0</span> <span class="o">-</span><span class="mf">0.003271</span>
          <span class="mi">1</span>  <span class="mf">0.011454</span>
          <span class="mi">2</span> <span class="o">-</span><span class="mf">0.008182</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Everytime this code is run, the output will be different because
“make_blobs” function generates random matrices.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
<dt><strong>mean_</strong><span class="classifier">array</span></dt><dd><p>The column wise mean of X. Used to mean - center the data first.</p>
</dd>
<dt><strong>noise_variance_</strong><span class="classifier">float</span></dt><dd><p>From Bishop 1999’s Textbook. Used in later tasks like calculating the
estimated covariance of X.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.decomposition.PCA.fit" title="cuml.dask.decomposition.PCA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X)</p></td>
<td><p>Fit the model with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.decomposition.PCA.fit_transform" title="cuml.dask.decomposition.PCA.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(X)</p></td>
<td><p>Fit the model with X and apply the dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.decomposition.PCA.inverse_transform" title="cuml.dask.decomposition.PCA.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(X[, delayed])</p></td>
<td><p>Transform data back to its original space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.decomposition.PCA.transform" title="cuml.dask.decomposition.PCA.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X[, delayed])</p></td>
<td><p>Apply dimensionality reduction to X.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%"/>
<col style="width: 34%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>get_param_names</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.decomposition.PCA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/pca.py#L166"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.PCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF input</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.decomposition.PCA.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/pca.py#L178"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.PCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and apply the dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.decomposition.PCA.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/pca.py#L212"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.PCA.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform data back to its original space.</p>
<p>In other words, return an input X_original whose transform would be X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_original</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.decomposition.PCA.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/pca.py#L192"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.PCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply dimensionality reduction to X.</p>
<p>X is projected on the first principal components previously extracted
from a training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id30">
<h3>Random Forest<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.ensemble.RandomForestClassifier">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.ensemble.</code><code class="sig-name descname">RandomForestClassifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">workers</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">n_estimators</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">seed</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignore_empty_partitions</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L31"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Experimental API implementing a multi-GPU Random Forest classifier
model which fits multiple decision tree classifiers in an
ensemble. This uses Dask to partition data over multiple GPUs
(possibly on different nodes).</p>
<dl class="simple">
<dt>Currently, this API makes the following assumptions:</dt><dd><ul class="simple">
<li><p>The set of Dask workers used between instantiation, fit,         and predict are all consistent</p></li>
<li><p>Training data comes in the form of cuDF dataframes or Dask Arrays         distributed so that each worker has at least one partition.</p></li>
<li><p>The print_summary and print_detailed functions print the         information of the forest on the worker.</p></li>
</ul>
</dd>
</dl>
<p>Future versions of the API will support more flexible data
distribution and additional input types.</p>
<p>The distributed algorithm uses an embarrassingly-parallel
approach. For a forest with N trees being built on w workers, each
worker simply builds N/w trees on the data it has available
locally. In many cases, partitioning the data so that each worker
builds trees on a subset of the total dataset works well, but
it generally requires the data to be well-shuffled in advance.
Alternatively, callers can replicate all of the data across
workers so that rf.fit receives w partitions, each containing the
same data. This would produce results approximately identical to
single-GPU fitting.</p>
<p>Please check the single-GPU implementation of Random Forest
classifier for more information about the underlying algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 10)</span></dt><dd><p>total number of trees in the forest (not per-worker)</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>split_criterion</strong><span class="classifier">The criterion used to split nodes.</span></dt><dd><p>0 for GINI, 1 for ENTROPY, 4 for CRITERION_END.
2 and 3 not valid for classification
(default = 0)</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">0 for HIST and 1 for GLOBAL_QUANTILE (default = 1)</span></dt><dd><p>the algorithm to determine how nodes are split in the tree.</p>
</dd>
<dt><strong>split_criterion</strong><span class="classifier">The criterion used to split nodes.</span></dt><dd><p>0 for GINI, 1 for ENTROPY, 4 for CRITERION_END.
2 and 3 not valid for classification
(default = 0)</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If set, each tree in the forest is built
on a bootstrapped sample with replacement.
If false, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure), if -1.</p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited, if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">float (default = ‘auto’)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed to split a node.</p>
</dd>
<dt><strong>quantile_per_tree</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Whether quantile is computed for individual RF trees.
Only relevant for GLOBAL_QUANTILE split_algo.</p>
</dd>
<dt><strong>n_streams</strong><span class="classifier">int (default = 4 )</span></dt><dd><p>Number of parallel streams used for forest building</p>
</dd>
<dt><strong>workers</strong><span class="classifier">optional, list of strings</span></dt><dd><p>Dask addresses of workers to use for computation.
If None, all available Dask workers will be used.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed for the random number generator. Unseeded by default.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (default = None)</span></dt><dd><p>Deprecated in favor of <cite>random_state</cite>.
Base seed for the random number generator. Unseeded by default. Does
not currently fully guarantee the exact same results.</p>
</dd>
<dt><strong>ignore_empty_partitions: Boolean (default = False)</strong></dt><dd><p>Specify behavior when a worker does not hold any data
while splitting. When True, it returns the results from workers
with data (the number of trained estimators will be less than
n_estimators) When False, throws a RuntimeError.
This is an experiemental parameter, and may be removed
in the future.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>For usage examples, please see the RAPIDS notebooks repository:
<a class="reference external" href="https://github.com/rapidsai/cuml/blob/branch-0.15/notebooks/random_forest_mnmg_demo.ipynb">https://github.com/rapidsai/cuml/blob/branch-0.15/notebooks/random_forest_mnmg_demo.ipynb</a></p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.fit" title="cuml.dask.ensemble.RandomForestClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y[, convert_dtype])</p></td>
<td><p>Fit the input data with a Random Forest classifier</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.get_params" title="cuml.dask.ensemble.RandomForestClassifier.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>([deep])</p></td>
<td><p>Returns the value of all parameters required to configure this estimator as a dictionary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.predict" title="cuml.dask.ensemble.RandomForestClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, output_class, algo, threshold, …])</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.predict_model_on_cpu" title="cuml.dask.ensemble.RandomForestClassifier.predict_model_on_cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_model_on_cpu</span></code></a>(X[, convert_dtype])</p></td>
<td><p>Predicts the labels for X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.predict_proba" title="cuml.dask.ensemble.RandomForestClassifier.predict_proba"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_proba</span></code></a>(X[, delayed])</p></td>
<td><p>Predicts the probability of each class for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.print_detailed" title="cuml.dask.ensemble.RandomForestClassifier.print_detailed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_detailed</span></code></a>()</p></td>
<td><p>Print detailed information of the forest used to train the model on each worker.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.print_summary" title="cuml.dask.ensemble.RandomForestClassifier.print_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_summary</span></code></a>()</p></td>
<td><p>Print the summary of the forest used to train the model on each worker.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestClassifier.set_params" title="cuml.dask.ensemble.RandomForestClassifier.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(**params)</p></td>
<td><p>Sets the value of parameters required to configure this estimator, it functions similar to the sklearn set_params.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 68%"/>
<col style="width: 32%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>predict_using_fil</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L205"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the input data with a Random Forest classifier</p>
<p>IMPORTANT: X is expected to be partitioned with at least one partition
on each Dask worker being used by the forest (self.workers).</p>
<p>If a worker has multiple data partitions, they will be concatenated
before fitting, which will lead to additional memory usage. To minimize
memory consumption, ensure that each worker has exactly one partition.</p>
<p>When persisting data, you can use
<cite>cuml.dask.common.utils.persist_across_workers</cite> to simplify this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">y_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">X_dask_cudf</span><span class="p">,</span> <span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">persist_across_workers</span><span class="p">(</span><span class="n">dask_client</span><span class="p">,</span>
                                                  <span class="p">[</span><span class="n">X_dask_cudf</span><span class="p">,</span>
                                                   <span class="n">y_dask_cudf</span><span class="p">])</span>
</pre></div>
</div>
<p>This is equivalent to calling <cite>persist</cite> with the data and workers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_dask_cudf</span><span class="p">,</span> <span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">dask_client</span><span class="o">.</span><span class="n">persist</span><span class="p">([</span><span class="n">X_dask_cudf</span><span class="p">,</span>
                                                <span class="n">y_dask_cudf</span><span class="p">],</span>
                                               <span class="n">workers</span><span class="o">=</span><span class="p">{</span>
                                               <span class="n">X_dask_cudf</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span>
                                               <span class="n">y_dask_cudf</span><span class="o">=</span><span class="n">workers</span>
                                               <span class="p">})</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd><p>Labels of training examples.
<strong>y must be partitioned the same way as X</strong></p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be of dtype int32. This will increase memory used for
the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">deep</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L490"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of all parameters
required to configure this estimator as a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">output_class</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">predict_model</span><span class="o">=</span><span class="default_value">'GPU'</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L258"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X.</p>
<p>GPU-based prediction in a multi-node, multi-GPU context works
by sending the sub-forest from each worker to the client,
concatenating these into one forest with the full
<cite>n_estimators</cite> set of trees, and sending this combined forest to
the workers, which will each infer on their local set of data.
Within the worker, this uses the cuML Forest Inference Library
(cuml.fil) for high-throughput prediction.</p>
<p>This allows inference to scale to large datasets, but the forest
transmission incurs overheads for very large trees. For inference
on small datasets, this overhead may dominate prediction time.</p>
<p>The ‘CPU’ fallback method works with sub-forests in-place,
broadcasting the datasets to all workers and combining predictions
via a voting method at the end. This method is slower
on a per-row basis but may be faster for problems with many trees
and few rows.</p>
<p>In the 0.15 cuML release, inference will be updated with much
faster tree transfer. Preliminary builds with this updated approach
will be available from rapids.ai</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>output_class</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
If true, return a 1 or 0 depending on whether the raw
prediction exceeds the threshold. If False, just return
the raw prediction.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
‘algo’ - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>Threshold used for classification. Optional and required only
while performing the predict operation on the GPU, that is for,
predict_model=’GPU’.
It is applied if output_class == True, else it is ignored</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt><strong>predict_model</strong><span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The GPU can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True.</p>
</dd>
<dt><strong>fil_sparse_format</strong><span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.  It is not required  while using
predict_model=’CPU’.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe or CuPy backed Dask Array (n_rows, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.predict_model_on_cpu">
<code class="sig-name descname">predict_model_on_cpu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L366"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.predict_model_on_cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the labels for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>———-</strong></dt><dd></dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe or CuPy backed Dask Array (n_rows, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L426"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the probability of each class for X.</p>
<p>See documentation of <cite>predict</cite> for notes on performance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>predict_model</strong><span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The ‘GPU’ can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True. Also the ‘GPU’ should only be
used for binary classification problems.</p>
</dd>
<dt><strong>output_class</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
If true, return a 1 or 0 depending on whether the raw
prediction exceeds the threshold. If False, just return
the raw prediction.</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
‘auto’ - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>Threshold used for classification. Optional and required only
while performing the predict operation on the GPU.
It is applied if output_class == True, else it is ignored</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt><strong>fil_sparse_format</strong><span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">NumPy</span></dt><dd><p>Dask cuDF dataframe or CuPy backed Dask Array (n_rows, n_classes)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.print_detailed">
<code class="sig-name descname">print_detailed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L197"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.print_detailed" title="Permalink to this definition">¶</a></dt>
<dd><p>Print detailed information of the forest used to train
the model on each worker. This information is displayed on the
workers and not the client.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.print_summary">
<code class="sig-name descname">print_summary</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L189"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.print_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Print the summary of the forest used to train the model
on each worker. This information is displayed on the
individual workers and not the client.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestClassifier.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestclassifier.py#L501"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of parameters required to
configure this estimator, it functions similar to
the sklearn set_params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params.</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.ensemble.RandomForestRegressor">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.ensemble.</code><code class="sig-name descname">RandomForestRegressor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">workers</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">n_estimators</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">seed</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignore_empty_partitions</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L26"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Experimental API implementing a multi-GPU Random Forest classifier
model which fits multiple decision tree classifiers in an
ensemble. This uses Dask to partition data over multiple GPUs
(possibly on different nodes).</p>
<dl class="simple">
<dt>Currently, this API makes the following assumptions:</dt><dd><ul class="simple">
<li><p>The set of Dask workers used between instantiation, fit,
and predict are all consistent</p></li>
<li><p>Training data comes in the form of cuDF dataframes or Dask Arrays
distributed so that each worker has at least one partition.</p></li>
<li><p>The print_summary and print_detailed functions print the
information of the forest on the worker.</p></li>
</ul>
</dd>
</dl>
<p>Future versions of the API will support more flexible data
distribution and additional input types. User-facing APIs are
expected to change in upcoming versions.</p>
<p>The distributed algorithm uses an embarrassingly-parallel
approach. For a forest with N trees being built on w workers, each
worker simply builds N/w trees on the data it has available
locally. In many cases, partitioning the data so that each worker
builds trees on a subset of the total dataset works well, but
it generally requires the data to be well-shuffled in advance.
Alternatively, callers can replicate all of the data across
workers so that rf.fit receives w partitions, each containing the
same data. This would produce results approximately identical to
single-GPU fitting.</p>
<p>Please check the single-GPU implementation of Random Forest
classifier for more information about the underlying algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_estimators</strong><span class="classifier">int (default = 10)</span></dt><dd><p>total number of trees in the forest (not per-worker)</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>split_algo</strong><span class="classifier">int (default = 1)</span></dt><dd><p>0 for HIST, 1 for GLOBAL_QUANTILE
The type of algorithm to be used to create the trees.</p>
</dd>
<dt><strong>split_criterion</strong><span class="classifier">int (default = 2)</span></dt><dd><p>The criterion used to split nodes.
0 for GINI, 1 for ENTROPY,
2 for MSE, 3 for MAE and 4 for CRITERION_END.
0 and 1 not valid for regression</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>Control bootstrapping.
If set, each tree in the forest is built
on a bootstrapped sample with replacement.
If false, sampling without replacement is done.</p>
</dd>
<dt><strong>bootstrap_features</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>Control bootstrapping for features.
If features are drawn with or without replacement</p>
</dd>
<dt><strong>rows_sample</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Ratio of dataset rows used while fitting each tree.</p>
</dd>
<dt><strong>max_depth</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum tree depth. Unlimited (i.e, until leaves are pure), if -1.</p>
</dd>
<dt><strong>max_leaves</strong><span class="classifier">int (default = -1)</span></dt><dd><p>Maximum leaf nodes per tree. Soft constraint. Unlimited, if -1.</p>
</dd>
<dt><strong>max_features</strong><span class="classifier">int or float or string or None (default = ‘auto’)</span></dt><dd><p>Ratio of number of features (columns) to consider
per node split.
If int then max_features/n_features.
If float then max_features is a fraction.
If ‘auto’ then max_features=n_features which is 1.0.
If ‘sqrt’ then max_features=1/sqrt(n_features).
If ‘log2’ then max_features=log2(n_features)/n_features.
If None, then max_features=n_features which is 1.0.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int (default = 8)</span></dt><dd><p>Number of bins used by the split algorithm.</p>
</dd>
<dt><strong>min_rows_per_node</strong><span class="classifier">int or float (default = 2)</span></dt><dd><p>The minimum number of samples (rows) needed to split a node.
If int then number of sample rows
If float the min_rows_per_sample*n_rows</p>
</dd>
<dt><strong>accuracy_metric</strong><span class="classifier">string (default = ‘r2’)</span></dt><dd><p>Decides the metric used to evaluate the performance of the model.
In the 0.16 release, the default scoring metric was changed
from mean squared error to r-squared.
for r-squared : ‘r2’
for median of abs error : ‘median_ae’
for mean of abs error : ‘mean_ae’
for mean square error’ : ‘mse’</p>
</dd>
<dt><strong>n_streams</strong><span class="classifier">int (default = 4 )</span></dt><dd><p>Number of parallel streams used for forest building</p>
</dd>
<dt><strong>workers</strong><span class="classifier">optional, list of strings</span></dt><dd><p>Dask addresses of workers to use for computation.
If None, all available Dask workers will be used.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = None)</span></dt><dd><p>Seed for the random number generator. Unseeded by default.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (default = None)</span></dt><dd><p>Deprecated in favor of <cite>random_state</cite>.
Base seed for the random number generator. Unseeded by default. Does
not currently fully guarantee the exact same results.</p>
</dd>
<dt><strong>ignore_empty_partitions: Boolean (default = False)</strong></dt><dd><p>Specify behavior when a worker does not hold any data
while splitting. When True, it returns the results from workers
with data (the number of trained estimators will be less than
n_estimators) When False, throws a RuntimeError.
This is an experiemental parameter, and may be removed
in the future.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.fit" title="cuml.dask.ensemble.RandomForestRegressor.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y[, convert_dtype])</p></td>
<td><p>Fit the input data with a Random Forest regression model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.get_params" title="cuml.dask.ensemble.RandomForestRegressor.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>([deep])</p></td>
<td><p>Returns the value of all parameters required to configure this estimator as a dictionary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.predict" title="cuml.dask.ensemble.RandomForestRegressor.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, predict_model, algo, …])</p></td>
<td><p>Predicts the regressor outputs for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.print_detailed" title="cuml.dask.ensemble.RandomForestRegressor.print_detailed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_detailed</span></code></a>()</p></td>
<td><p>Print detailed information of the forest used to train the model on each worker.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.print_summary" title="cuml.dask.ensemble.RandomForestRegressor.print_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_summary</span></code></a>()</p></td>
<td><p>Print the summary of the forest used to train the model on each worker.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.ensemble.RandomForestRegressor.set_params" title="cuml.dask.ensemble.RandomForestRegressor.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(**params)</p></td>
<td><p>Sets the value of parameters required to configure this estimator, it functions similar to the sklearn set_params.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 71%"/>
<col style="width: 29%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>predict_model_on_cpu</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>predict_using_fil</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L207"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the input data with a Random Forest regression model</p>
<p>IMPORTANT: X is expected to be partitioned with at least one partition
on each Dask worker being used by the forest (self.workers).</p>
<p>When persisting data, you can use
<cite>cuml.dask.common.utils.persist_across_workers</cite> to simplify this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_cudf</span><span class="p">(</span><span class="n">y_cudf</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">n_workers</span><span class="p">)</span>
<span class="n">X_dask_cudf</span><span class="p">,</span> <span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">persist_across_workers</span><span class="p">(</span><span class="n">dask_client</span><span class="p">,</span>
                                                  <span class="p">[</span><span class="n">X_dask_cudf</span><span class="p">,</span>
                                                   <span class="n">y_dask_cudf</span><span class="p">])</span>
</pre></div>
</div>
<p>This is equivalent to calling <cite>persist</cite> with the data and workers):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_dask_cudf</span><span class="p">,</span> <span class="n">y_dask_cudf</span> <span class="o">=</span> <span class="n">dask_client</span><span class="o">.</span><span class="n">persist</span><span class="p">([</span><span class="n">X_dask_cudf</span><span class="p">,</span>
                                                <span class="n">y_dask_cudf</span><span class="p">],</span>
                                               <span class="n">workers</span><span class="o">=</span><span class="p">{</span>
                                               <span class="n">X_dask_cudf</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span>
                                               <span class="n">y_dask_cudf</span><span class="o">=</span><span class="n">workers</span>
                                               <span class="p">})</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd><p>Labels of training examples.
<strong>y must be partitioned the same way as X</strong></p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the fit method will, when necessary, convert
y to be the same data type as X if they differ. This will increase
memory used for the method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">deep</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L377"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of all parameters
required to configure this estimator as a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">predict_model</span><span class="o">=</span><span class="default_value">'GPU'</span></em>, <em class="sig-param"><span class="n">algo</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">fil_sparse_format</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L256"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the regressor outputs for X.</p>
<p>GPU-based prediction in a multi-node, multi-GPU context works
by sending the sub-forest from each worker to the client,
concatenating these into one forest with the full
<cite>n_estimators</cite> set of trees, and sending this combined forest to
the workers, which will each infer on their local set of data.
This allows inference to scale to large datasets, but the forest
transmission incurs overheads for very large trees. For inference
on small datasets, this overhead may dominate prediction time.
Within the worker, this uses the cuML Forest Inference Library
(cuml.fil) for high-throughput prediction.</p>
<p>The ‘CPU’ fallback method works with sub-forests in-place,
broadcasting the datasets to all workers and combining predictions
via an averaging method at the end. This method is slower
on a per-row basis but may be faster for problems with many trees
and few rows.</p>
<p>In the 0.15 cuML release, inference will be updated with much
faster tree transfer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string (default = ‘auto’)</span></dt><dd><p>This is optional and required only while performing the
predict operation on the GPU.
‘naive’ - simple inference using shared memory
‘tree_reorg’ - similar to naive but trees rearranged to be more
coalescing-friendly
‘batch_tree_reorg’ - similar to tree_reorg but predicting
multiple rows per thread block
<cite>algo</cite> - choose the algorithm automatically. Currently
‘batch_tree_reorg’ is used for dense storage
and ‘naive’ for sparse storage</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = True)</span></dt><dd><p>When set to True, the predict method will, when necessary, convert
the input to the data type which was used to train the model. This
will increase memory used for the method.</p>
</dd>
<dt><strong>predict_model</strong><span class="classifier">String (default = ‘GPU’)</span></dt><dd><p>‘GPU’ to predict using the GPU, ‘CPU’ otherwise. The GPU can only
be used if the model was trained on float32 data and <cite>X</cite> is float32
or convert_dtype is set to True.</p>
</dd>
<dt><strong>fil_sparse_format</strong><span class="classifier">boolean or string (default = auto)</span></dt><dd><p>This variable is used to choose the type of forest that will be
created in the Forest Inference Library. It is not required
while using predict_model=’CPU’.
‘auto’ - choose the storage type automatically
(currently True is chosen by auto)
False - create a dense forest
True - create a sparse forest, requires algo=’naive’
or algo=’auto’</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe or CuPy backed Dask Array (n_rows, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.print_detailed">
<code class="sig-name descname">print_detailed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L199"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.print_detailed" title="Permalink to this definition">¶</a></dt>
<dd><p>Print detailed information of the forest used to train
the model on each worker. This information is displayed on the
workers and not the client.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.print_summary">
<code class="sig-name descname">print_summary</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L191"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.print_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Print the summary of the forest used to train the model
on each worker. This information is displayed on the
individual workers and not the client.</p>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.ensemble.RandomForestRegressor.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/ensemble/randomforestregressor.py#L388"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.ensemble.RandomForestRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of parameters required to
configure this estimator, it functions similar to
the sklearn set_params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id31">
<h3>Truncated SVD<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.decomposition.TruncatedSVD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.decomposition.</code><code class="sig-name descname">TruncatedSVD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/tsvd.py#L24"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.TruncatedSVD" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want.
Must be &lt;= number(columns).</p>
</dd>
<dt><strong>svd_solver</strong><span class="classifier">‘full’</span></dt><dd><p>Only Full algorithm is supported since it’s significantly faster on GPU
then the other solvers including randomized SVD.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">wait</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cuml.dask.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">cuml.dask.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">(</span><span class="n">threads_per_worker</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>

<span class="n">nrows</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">ncols</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_parts</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">X_cudf</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_parts</span><span class="p">,</span>
                <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">cuml</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">level_info</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wait</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Input Matrix"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_cudf</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>

<span class="n">cumlModel</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">XT</span> <span class="o">=</span> <span class="n">cumlModel</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Transformed Input Matrix"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">XT</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
                    <span class="mi">0</span>         <span class="mi">1</span>          <span class="mi">2</span>
          <span class="mi">0</span> <span class="o">-</span><span class="mf">8.519647</span> <span class="o">-</span><span class="mf">8.519222</span>  <span class="o">-</span><span class="mf">8.865648</span>
          <span class="mi">1</span> <span class="o">-</span><span class="mf">6.107700</span> <span class="o">-</span><span class="mf">8.350124</span> <span class="o">-</span><span class="mf">10.351215</span>
          <span class="mi">2</span> <span class="o">-</span><span class="mf">8.026635</span> <span class="o">-</span><span class="mf">9.442240</span>  <span class="o">-</span><span class="mf">7.561770</span>
          <span class="mi">0</span> <span class="o">-</span><span class="mf">8.519647</span> <span class="o">-</span><span class="mf">8.519222</span>  <span class="o">-</span><span class="mf">8.865648</span>
          <span class="mi">1</span> <span class="o">-</span><span class="mf">6.107700</span> <span class="o">-</span><span class="mf">8.350124</span> <span class="o">-</span><span class="mf">10.351215</span>
          <span class="mi">2</span> <span class="o">-</span><span class="mf">8.026635</span> <span class="o">-</span><span class="mf">9.442240</span>  <span class="o">-</span><span class="mf">7.561770</span>

<span class="n">Transformed</span> <span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
                     <span class="mi">0</span>
          <span class="mi">0</span>  <span class="mf">14.928891</span>
          <span class="mi">1</span>  <span class="mf">14.487295</span>
          <span class="mi">2</span>  <span class="mf">14.431235</span>
          <span class="mi">0</span>  <span class="mf">14.928891</span>
          <span class="mi">1</span>  <span class="mf">14.487295</span>
          <span class="mi">2</span>  <span class="mf">14.431235</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Everytime this code is run, the output will be different because
“make_blobs” function generates random matrices.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.decomposition.TruncatedSVD.fit" title="cuml.dask.decomposition.TruncatedSVD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X[, _transform])</p></td>
<td><p>Fit the model with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.decomposition.TruncatedSVD.fit_transform" title="cuml.dask.decomposition.TruncatedSVD.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(X)</p></td>
<td><p>Fit the model with X and apply the dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.decomposition.TruncatedSVD.inverse_transform" title="cuml.dask.decomposition.TruncatedSVD.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(X[, delayed])</p></td>
<td><p>Transform data back to its original space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.decomposition.TruncatedSVD.transform" title="cuml.dask.decomposition.TruncatedSVD.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X[, delayed])</p></td>
<td><p>Apply dimensionality reduction to <cite>X</cite>.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%"/>
<col style="width: 34%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>get_param_names</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.decomposition.TruncatedSVD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">_transform</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/tsvd.py#L128"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.TruncatedSVD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF input</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.decomposition.TruncatedSVD.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/tsvd.py#L146"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.TruncatedSVD.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and apply the dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.decomposition.TruncatedSVD.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/tsvd.py#L180"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.TruncatedSVD.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform data back to its original space.</p>
<p>In other words, return an input X_original whose transform would be X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_original</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.decomposition.TruncatedSVD.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/decomposition/tsvd.py#L160"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.decomposition.TruncatedSVD.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply dimensionality reduction to <cite>X</cite>.</p>
<p><cite>X</cite> is projected on the first principal components previously extracted
from a training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">dask cuDF</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="manifold">
<h3>Manifold<a class="headerlink" href="#manifold" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.manifold.UMAP">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.manifold.</code><code class="sig-name descname">UMAP</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/manifold/umap.py#L20"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.manifold.UMAP" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniform Manifold Approximation and Projection</p>
<p>Finds a low dimensional embedding of the data that approximates
an underlying manifold.</p>
<p>Adapted from <a class="reference external" href="https://github.com/lmcinnes/umap/blob/master/umap/">https://github.com/lmcinnes/umap/blob/master/umap/</a><a class="reference internal" href="#umap">umap</a>.py</p>
<p class="rubric">Notes</p>
<p>This module is heavily based on Leland McInnes’ reference UMAP package
<a class="reference internal" href="#r1e0410cceb30-1" id="id32">[1]</a>.</p>
<p>However, there are a number of differences and features that are
not yet implemented in <cite>cuml.umap</cite>:</p>
<ul class="simple">
<li><p>Using a non-Euclidean distance metric (support for a fixed set
of non-Euclidean metrics is planned for an upcoming release).</p></li>
<li><p>Using a pre-computed pairwise distance matrix (under consideration
for future releases)</p></li>
<li><p>Manual initialization of initial embedding positions</p></li>
</ul>
<p>In addition to these missing features, you should expect to see
the final embeddings differing between <cite>cuml.umap</cite> and the reference
UMAP. In particular, the reference UMAP uses an approximate kNN
algorithm for large data sizes while cuml.umap always uses exact
kNN.</p>
<p><strong>Known issue:</strong> If a UMAP model has not yet been fit, it cannot be pickled</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r1e0410cceb30-1"><span class="brackets"><a class="fn-backref" href="#id32">1</a></span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/abs/1802.03426">Leland McInnes, John Healy, James Melville
UMAP: Uniform Manifold Approximation and Projection for Dimension
Reduction.</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">cuml.dask.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">cuml.manifold</span> <span class="kn">import</span> <span class="n">UMAP</span>
<span class="kn">from</span> <span class="nn">cuml.dask.manifold</span> <span class="kn">import</span> <span class="n">UMAP</span> <span class="k">as</span> <span class="n">MNMG_UMAP</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">(</span><span class="n">threads_per_worker</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>
                <span class="n">centers</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="n">n_parts</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">output</span><span class="o">=</span><span class="s1">'array'</span><span class="p">)</span>

<span class="n">local_model</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">()</span>

<span class="n">selection</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selection</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">selection</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>

<span class="n">local_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">distributed_model</span> <span class="o">=</span> <span class="n">MNMG_UMAP</span><span class="p">(</span><span class="n">local_model</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">distributed_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Everytime this code is run, the output will be different because
“make_blobs” function generates random matrices.</p>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.manifold.UMAP.transform" title="cuml.dask.manifold.UMAP.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X[, convert_dtype])</p></td>
<td><p>Transform X into the existing embedded space and return that transformed output.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.manifold.UMAP.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/manifold/umap.py#L100"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.manifold.UMAP.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X into the existing embedded space and return that
transformed output.</p>
<p>Please refer to the reference UMAP implementation for information
on the differences between fit_transform() and running fit()
transform().</p>
<p>Specifically, the transform() function is stochastic:
<a class="reference external" href="https://github.com/lmcinnes/umap/issues/158">https://github.com/lmcinnes/umap/issues/158</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like (device or host) shape = (n_samples, n_features)</span></dt><dd><p>New data to be transformed.
Acceptable formats: dask cuDF, dask CuPy/NumPy/Numba Array</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">array, shape (n_samples, n_components)</span></dt><dd><p>Embedding of the new data in low-dimensional space.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="linear-models">
<h3>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.linear_model.LinearRegression">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.linear_model.</code><code class="sig-name descname">LinearRegression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/linear_regression.py#L23"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.LinearRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>LinearRegression is a simple machine learning model where the response y is
modelled by a linear combination of the predictors in X.</p>
<p>cuML’s dask Linear Regression (multi-node multi-gpu) expects dask cuDF
DataFrame and provides an algorithms, Eig, to fit a linear model.
And provides an eigendecomposition-based algorithm to fit a linear model.
(SVD, which is more stable than eig, will be added in an upcoming version.)
Eig algorithm is usually preferred when the X is a tall and skinny matrix.
As the number of features in X increases, the accuracy of Eig algorithm
drops.</p>
<p>This is an experimental implementation of dask Linear Regresion. It
supports input X that has more than one column. Single column input
X will be supported after SVD algorithm is added in an upcoming version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algorithm</strong><span class="classifier">‘eig’</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much
faster.
SVD is slower, but guaranteed to be stable.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>LinearRegression adds an additional term c to correct for the global
mean of y, modeling the reponse as “x * beta + c”.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by its
L2 norm.
If False, no scaling will be done.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">cuDF series, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.linear_model.LinearRegression.fit" title="cuml.dask.linear_model.LinearRegression.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.linear_model.LinearRegression.predict" title="cuml.dask.linear_model.LinearRegression.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, delayed])</p></td>
<td><p>Make predictions for X and returns a dask collection.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%"/>
<col style="width: 34%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>get_param_names</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.linear_model.LinearRegression.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/linear_regression.py#L70"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.LinearRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Features for regression</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd><p>Labels (outcome values)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.linear_model.LinearRegression.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/linear_regression.py#L89"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.LinearRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for X and returns a dask collection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.linear_model.Ridge">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.linear_model.</code><code class="sig-name descname">Ridge</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/ridge.py#L23"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.Ridge" title="Permalink to this definition">¶</a></dt>
<dd><p>Ridge extends LinearRegression by providing L2 regularization on the
coefficients when predicting response y with a linear combination of the
predictors in X. It can reduce the variance of the predictors, and improves
the conditioning of the problem.</p>
<p>cuML’s dask Ridge (multi-node multi-gpu) expects dask cuDF
DataFrame and provides an algorithms, Eig, to fit a linear model.
And provides an eigendecomposition-based algorithm to fit a linear model.
(SVD, which is more stable than eig, will be added in an upcoming version)
Eig algorithm is usually preferred when the X is a tall and skinny matrix.
As the number of features in X increases, the accuracy of Eig algorithm
drops.</p>
<p>This is an experimental implementation of dask Ridge Regresion. It
supports input X that has more than one column. Single column input
X will be supported after SVD algorithm is added in an upcoming version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Regularization strength - must be a positive float. Larger values
specify stronger regularization. Array input will be supported later.</p>
</dd>
<dt><strong>solver</strong><span class="classifier">{‘eig’}</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much
faster.
Other solvers will be supported in the future.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Ridge adds an additional term c to correct for the global
mean of y, modeling the reponse as “x * beta + c”.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.linear_model.Ridge.fit" title="cuml.dask.linear_model.Ridge.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.linear_model.Ridge.predict" title="cuml.dask.linear_model.Ridge.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, delayed])</p></td>
<td><p>Make predictions for X and returns a dask collection.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%"/>
<col style="width: 34%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>get_param_names</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.linear_model.Ridge.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/ridge.py#L82"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.Ridge.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Features for regression</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd><p>Labels (outcome values)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.linear_model.Ridge.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/ridge.py#L100"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.Ridge.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for X and returns a dask collection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.linear_model.Lasso">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.linear_model.</code><code class="sig-name descname">Lasso</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/lasso.py#L21"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.Lasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Lasso extends LinearRegression by providing L1 regularization on the
coefficients when predicting response y with a linear combination of the
predictors in X. It can zero some of the coefficients for feature
selection and improves the conditioning of the problem.</p>
<p>cuML’s Lasso an array-like object or cuDF DataFrame and
uses coordinate descent to fit a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Constant that multiplies the L1 term.
alpha = 0 is equivalent to an ordinary least square, solved by the
LinearRegression class.
For numerical reasons, using alpha = 0 with the Lasso class is not
advised.
Given this, you should use the LinearRegression class.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Lasso tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than
tol, the optimization code checks the dual gap for optimality and
continues until it is smaller than tol.</p>
</dd>
<dt><strong>selection</strong><span class="classifier">{‘cyclic’, ‘random’} (default=’cyclic’)</span></dt><dd><p>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default.
This (setting to ‘random’) often leads to significantly faster
convergence especially when tol is higher than 1e-4.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
<dt><strong>For additional docs, see `scikitlearn’s Lasso</strong></dt><dd></dd>
<dt><strong>&lt;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&gt;`_.</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.linear_model.Lasso.fit" title="cuml.dask.linear_model.Lasso.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.linear_model.Lasso.predict" title="cuml.dask.linear_model.Lasso.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, delayed])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.linear_model.Lasso.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/lasso.py#L84"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.Lasso.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.linear_model.Lasso.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/lasso.py#L102"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.Lasso.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.linear_model.ElasticNet">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.linear_model.</code><code class="sig-name descname">ElasticNet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/elastic_net.py#L21"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.ElasticNet" title="Permalink to this definition">¶</a></dt>
<dd><p>ElasticNet extends LinearRegression with combined L1 and L2 regularizations
on the coefficients when predicting response y with a linear combination of
the predictors in X. It can reduce the variance of the predictors, force
some coefficients to be small, and improves the conditioning of the
problem.</p>
<p>cuML’s ElasticNet an array-like object or cuDF DataFrame, uses coordinate
descent to fit a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float (default = 1.0)</span></dt><dd><p>Constant that multiplies the L1 term.
alpha = 0 is equivalent to an ordinary least square, solved by the
LinearRegression object.
For numerical reasons, using alpha = 0 with the Lasso object is not
advised.
Given this, you should use the LinearRegression object.</p>
</dd>
<dt><strong>l1_ratio: float (default = 0.5)</strong></dt><dd><p>The ElasticNet mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is
an L1 penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Lasso tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2
norm.
If False, no scaling will be done.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The tolerance for the optimization: if the updates are smaller than
tol, the optimization code checks the dual gap for optimality and
continues until it is smaller than tol.</p>
</dd>
<dt><strong>selection</strong><span class="classifier">{‘cyclic’, ‘random’} (default=’cyclic’)</span></dt><dd><p>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default.
This (setting to ‘random’) often leads to significantly faster
convergence especially when tol is higher than 1e-4.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <cite>fit_intercept</cite> is False, will be 0.</p>
</dd>
<dt><strong>For additional docs, see `scikitlearn’s ElasticNet</strong></dt><dd></dd>
<dt><strong>&lt;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html&gt;`_.</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.linear_model.ElasticNet.fit" title="cuml.dask.linear_model.ElasticNet.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.linear_model.ElasticNet.predict" title="cuml.dask.linear_model.ElasticNet.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, delayed])</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.linear_model.ElasticNet.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/elastic_net.py#L103"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.ElasticNet.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.linear_model.ElasticNet.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/linear_model/elastic_net.py#L120"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.linear_model.ElasticNet.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF DataFrame or CuPy backed Dask Array</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="naive-bayes">
<h3>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.naive_bayes.MultinomialNB">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.naive_bayes.</code><code class="sig-name descname">MultinomialNB</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/naive_bayes/naive_bayes.py#L40"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.naive_bayes.MultinomialNB" title="Permalink to this definition">¶</a></dt>
<dd><p>Distributed Naive Bayes classifier for multinomial models</p>
<p class="rubric">Examples</p>
<p>Load the 20 newsgroups dataset from Scikit-learn and train a
Naive Bayes classifier.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="kn">from</span> <span class="nn">cuml.dask.common</span> <span class="kn">import</span> <span class="n">to_sparse_dask_array</span>

<span class="kn">from</span> <span class="nn">cuml.dask.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>

<span class="c1"># Create a local CUDA cluster</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>

<span class="c1"># Load corpus</span>

<span class="n">twenty_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'train'</span><span class="p">,</span>
                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">xformed</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">to_sparse_dask_array</span><span class="p">(</span><span class="n">xformed</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">array</span><span class="o">.</span><span class="n">from_array</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">asarray</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">fancy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># Train model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Compute accuracy on training set</span>

<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mf">0.9244298934936523</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.naive_bayes.MultinomialNB.fit" title="cuml.dask.naive_bayes.MultinomialNB.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y[, classes])</p></td>
<td><p>Fit distributed Naive Bayes classifier model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.naive_bayes.MultinomialNB.predict" title="cuml.dask.naive_bayes.MultinomialNB.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X)</p></td>
<td><p>Use distributed Naive Bayes model to predict the classes for a given set of data samples.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.naive_bayes.MultinomialNB.score" title="cuml.dask.naive_bayes.MultinomialNB.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X, y)</p></td>
<td><p>Compute accuracy score</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.naive_bayes.MultinomialNB.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/naive_bayes/naive_bayes.py#L148"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.naive_bayes.MultinomialNB.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit distributed Naive Bayes classifier model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask.Array with blocks containing dense or sparse cupy arrays</span></dt><dd></dd>
<dt><strong>y</strong><span class="classifier">dask.Array with blocks containing cupy.ndarray</span></dt><dd></dd>
<dt><strong>classes</strong><span class="classifier">array-like containing unique class labels</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>cuml.dask.naive_bayes.MultinomialNB current model instance</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.naive_bayes.MultinomialNB.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/naive_bayes/naive_bayes.py#L210"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.naive_bayes.MultinomialNB.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Use distributed Naive Bayes model to predict the classes for a
given set of data samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">dask.Array with blocks containing dense or sparse cupy arrays</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>dask.Array containing predicted classes</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.naive_bayes.MultinomialNB.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/naive_bayes/naive_bayes.py#L237"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.naive_bayes.MultinomialNB.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute accuracy score</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask.Array</span></dt><dd><p>Features to predict. Note- it is assumed that chunk sizes and
shape of X are known. This can be done for a fully delayed
Array by calling X.compute_chunks_sizes()</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask.Array</span></dt><dd><p>Labels to use for computing accuracy. Note- it is assumed that
chunk sizes and shape of X are known. This can be done for a fully
delayed Array by calling X.compute_chunks_sizes()</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float the resulting accuracy score</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="solvers">
<h3>Solvers<a class="headerlink" href="#solvers" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.solvers.CD">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.solvers.</code><code class="sig-name descname">CD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/solvers/cd.py#L23"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.solvers.CD" title="Permalink to this definition">¶</a></dt>
<dd><p>Model-Parallel Multi-GPU Linear Regression Model. Single Process Multi GPU
supported currently</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.solvers.CD.fit" title="cuml.dask.solvers.CD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.dask.solvers.CD.predict" title="cuml.dask.solvers.CD.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(X[, delayed])</p></td>
<td><p>Make predictions for X and returns a dask collection.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.solvers.CD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/solvers/cd.py#L40"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.solvers.CD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Features for regression</p>
</dd>
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd><p>Labels (outcome values)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.dask.solvers.CD.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">delayed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/solvers/cd.py#L58"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.solvers.CD.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions for X and returns a dask collection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, n_features)</span></dt><dd><p>Distributed dense matrix (floats or doubles) of shape
(n_samples, n_features).</p>
</dd>
<dt><strong>delayed</strong><span class="classifier">bool (default = True)</span></dt><dd><p>Whether to do a lazy prediction (and return Delayed objects) or an
eagerly executed one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">Dask cuDF dataframe  or CuPy backed Dask Array (n_rows, 1)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dask-base-classes-and-mixins">
<h3>Dask Base Classes and Mixins<a class="headerlink" href="#dask-base-classes-and-mixins" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.dask.common.base.BaseEstimator">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.common.base.</code><code class="sig-name descname">BaseEstimator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">client</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/common/base.py#L39"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.common.base.BaseEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.dask.common.base.BaseEstimator.get_combined_model" title="cuml.dask.common.base.BaseEstimator.get_combined_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_combined_model</span></code></a>()</p></td>
<td><p>Return single-GPU model for serialization</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.dask.common.base.BaseEstimator.get_combined_model">
<code class="sig-name descname">get_combined_model</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/common/base.py#L51"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.common.base.BaseEstimator.get_combined_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Return single-GPU model for serialization</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model</strong><span class="classifier">Trained single-GPU model or None if the model has not</span></dt><dd><p>yet been trained.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.dask.common.base.DelayedParallelFunc">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.common.base.</code><code class="sig-name descname">DelayedParallelFunc</code><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/common/base.py#L213"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.common.base.DelayedParallelFunc" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="cuml.dask.common.base.DelayedPredictionMixin">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.common.base.</code><code class="sig-name descname">DelayedPredictionMixin</code><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/common/base.py#L324"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.common.base.DelayedPredictionMixin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="cuml.dask.common.base.DelayedTransformMixin">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.common.base.</code><code class="sig-name descname">DelayedTransformMixin</code><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/common/base.py#L332"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.common.base.DelayedTransformMixin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="cuml.dask.common.base.DelayedInverseTransformMixin">
<em class="property">class </em><code class="sig-prename descclassname">cuml.dask.common.base.</code><code class="sig-name descname">DelayedInverseTransformMixin</code><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/dask/common/base.py#L342"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.dask.common.base.DelayedInverseTransformMixin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</div>
</div>
<div class="section" id="experimental">
<h2>Experimental<a class="headerlink" href="#experimental" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <cite>cuml.experimental</cite> module contains features that are still
under development. It is not recommended to depend on features in this
module as they may change in future releases.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to the nature of this module, it is not imported by default by
the root <cite>cuml</cite> package. Each <cite>experimental</cite> submodule must be imported
separately.</p>
</div>
<div class="section" id="decomposition">
<h3>Decomposition<a class="headerlink" href="#decomposition" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.experimental.decomposition.IncrementalPCA">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.decomposition.</code><code class="sig-name descname">IncrementalPCA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">whiten</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/experimental/decomposition/incremental_pca.py#L29"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.decomposition.IncrementalPCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Based on sklearn.decomposition.IncrementalPCA from scikit-learn 0.23.1</p>
<p>Incremental principal components analysis (IPCA).
Linear dimensionality reduction using Singular Value Decomposition of
the data, keeping only the most significant singular vectors to
project the data to a lower dimensional space. The input data is
centered but not scaled for each feature before applying the SVD.
Depending on the size of the input data, this algorithm can be much
more memory efficient than a PCA, and allows sparse input.
This algorithm has constant memory complexity, on the order of
<code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">*</span> <span class="pre">n_features</span></code>, enabling use of np.memmap files without
loading the entire file into memory. For sparse matrices, the input
is converted to dense in batches (in order to be able to subtract the
mean) which avoids storing the entire dense matrix at any one time.
The computational overhead of each SVD is
<code class="docutils literal notranslate"><span class="pre">O(batch_size</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2)</span></code>, but only 2 * batch_size samples
remain in memory at a time. There will be <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">batch_size</span></code>
SVD computations to get the principal components, versus 1 large SVD
of complexity <code class="docutils literal notranslate"><span class="pre">O(n_samples</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2)</span></code> for PCA.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>Specifies the cuml.handle that holds internal CUDA state for
computations in this model. Most importantly, this specifies the CUDA
stream that will be used for the model’s computations, so users can
run different models concurrently in different streams by creating
handles in several streams.
If it is None, a new one is created.</p>
</dd>
<dt><strong>n_components</strong><span class="classifier">int or None, (default=None)</span></dt><dd><p>Number of components to keep. If <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>,
then <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is set to <code class="docutils literal notranslate"><span class="pre">min(n_samples,</span> <span class="pre">n_features)</span></code>.</p>
</dd>
<dt><strong>whiten</strong><span class="classifier">bool, optional</span></dt><dd><p>If True, de-correlates the components. This is done by dividing them by
the corresponding singular values then multiplying by sqrt(n_samples).
Whitening allows each component to have unit variance and removes
multi-collinearity. It might be beneficial for downstream
tasks like LinearRegression where correlated features cause problems.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool, (default=True)</span></dt><dd><p>If False, X will be overwritten. <code class="docutils literal notranslate"><span class="pre">copy=False</span></code> can be used to
save memory but is unsafe for general use.</p>
</dd>
<dt><strong>batch_size</strong><span class="classifier">int or None, (default=None)</span></dt><dd><p>The number of samples to use for each batch. Only used when calling
<code class="docutils literal notranslate"><span class="pre">fit</span></code>. If <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, then <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>
is inferred from the data and set to <code class="docutils literal notranslate"><span class="pre">5</span> <span class="pre">*</span> <span class="pre">n_features</span></code>, to provide a
balance between approximation accuracy and memory consumption.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int or boolean, default=False</span></dt><dd><p>Sets logging level. It must be one of <cite>cuml.common.logger.level_*</cite>.
See <a class="reference internal" href="#verbosity-levels"><span class="std std-ref">Verbosity Levels</span></a> for more info.</p>
</dd>
<dt><strong>output_type</strong><span class="classifier">{‘input’, ‘cudf’, ‘cupy’, ‘numpy’, ‘numba’}, default=None</span></dt><dd><p>Variable to control output type of the results and attributes of
the estimator. If None, it’ll inherit the output type set at the
module level, <cite>cuml.global_output_type</cite>.
See <a class="reference internal" href="#output-data-type-configuration"><span class="std std-ref">Output Data Type Configuration</span></a> for more info.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Implements the incremental PCA model from <a class="reference internal" href="#ra14d707b5b61-1" id="id35">[1]</a>. This model is an extension
of the Sequential Karhunen-Loeve Transform from <a class="reference internal" href="#ra14d707b5b61-2" id="id36">[2]</a>. We have specifically
abstained from an optimization used by authors of both papers, a QR
decomposition used in specific situations to reduce the algorithmic
complexity of the SVD. The source for this technique is <a class="reference internal" href="#ra14d707b5b61-3" id="id37">[3]</a>. This
technique has been omitted because it is advantageous only when decomposing
a matrix with <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">&gt;=</span> <span class="pre">5/3</span> <span class="pre">*</span> <span class="pre">n_features</span></code> where <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> and
<code class="docutils literal notranslate"><span class="pre">n_features</span></code> are the matrix rows and columns, respectively. In addition,
it hurts the readability of the implemented algorithm. This would be a good
opportunity for future optimization, if it is deemed necessary.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="ra14d707b5b61-1"><span class="brackets"><a class="fn-backref" href="#id35">1</a></span></dt>
<dd><p><a class="reference external" href="https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf">D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust
Visual Tracking, International Journal of Computer Vision, Volume 77,
Issue 1-3, pp. 125-141, May 2008.</a></p>
</dd>
<dt class="label" id="ra14d707b5b61-2"><span class="brackets"><a class="fn-backref" href="#id36">2</a></span></dt>
<dd><p><a class="reference external" href="https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf">A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis
Extraction and its Application to Images, IEEE Transactions on Image
Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.</a></p>
</dd>
<dt class="label" id="ra14d707b5b61-3"><span class="brackets"><a class="fn-backref" href="#id37">3</a></span></dt>
<dd><p>G. Golub and C. Van Loan. Matrix Computations, Third Edition,
Chapter 5, Section 5.4.4, pp. 252-253.</p>
</dd>
<dt class="label" id="ra14d707b5b61-4"><span class="brackets"><a class="fn-backref" href="#id42">4</a></span></dt>
<dd><p><a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">C. Bishop, 1999. “Pattern Recognition and Machine Learning”,
Section 12.2.1, pp. 574</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.experimental.decomposition</span> <span class="kn">import</span> <span class="n">IncrementalPCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cupyx</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">cupyx</span><span class="o">.</span><span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'csr'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="mf">0.07</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span> <span class="o">=</span> <span class="n">IncrementalPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Components:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[-0.02362926,  0.87328851, -0.15971988,  0.45967206],</span>
<span class="go">    [-0.14643883,  0.11414225,  0.97589354,  0.11471273]])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Singular Values:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">singular_values_</span>
<span class="go">array([4.90298662, 4.54498226])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Explained Variance:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">explained_variance_</span>
<span class="go">array([0.02406334, 0.02067754])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Explained Variance Ratio:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="go">array([0.28018011, 0.24075775])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Mean:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">mean_</span>
<span class="go">array([0.03249896, 0.03629852, 0.03268694, 0.03216601])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Noise Variance:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ipca</span><span class="o">.</span><span class="n">noise_variance_</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">0.003474966583315544</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array, shape (n_components, n_features)</span></dt><dd><p>Components with maximum variance.</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array, shape (n_components,)</span></dt><dd><p>Variance explained by each of the selected components.</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array, shape (n_components,)</span></dt><dd><p>Percentage of variance explained by each of the selected components.
If all components are stored, the sum of explained variances is equal
to 1.0.</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array, shape (n_components,)</span></dt><dd><p>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code class="docutils literal notranslate"><span class="pre">n_components</span></code>
variables in the lower-dimensional space.</p>
</dd>
<dt><strong>mean_</strong><span class="classifier">array, shape (n_features,)</span></dt><dd><p>Per-feature empirical mean, aggregate over calls to <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>.</p>
</dd>
<dt><strong>var_</strong><span class="classifier">array, shape (n_features,)</span></dt><dd><p>Per-feature empirical variance, aggregate over calls to
<code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>.</p>
</dd>
<dt><strong>noise_variance_</strong><span class="classifier">float</span></dt><dd><p>The estimated noise covariance following the Probabilistic PCA model
from <a class="reference internal" href="#ra14d707b5b61-4" id="id42">[4]</a>.</p>
</dd>
<dt><strong>n_components_</strong><span class="classifier">int</span></dt><dd><p>The estimated number of components. Relevant when
<code class="docutils literal notranslate"><span class="pre">n_components=None</span></code>.</p>
</dd>
<dt><strong>n_samples_seen_</strong><span class="classifier">int</span></dt><dd><p>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> calls.</p>
</dd>
<dt><strong>batch_size_</strong><span class="classifier">int</span></dt><dd><p>Inferred batch size from <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.experimental.decomposition.IncrementalPCA.fit" title="cuml.experimental.decomposition.IncrementalPCA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(X[, y])</p></td>
<td><p>Fit the model with X, using minibatches of size batch_size.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.experimental.decomposition.IncrementalPCA.get_param_names" title="cuml.experimental.decomposition.IncrementalPCA.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.experimental.decomposition.IncrementalPCA.partial_fit" title="cuml.experimental.decomposition.IncrementalPCA.partial_fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">partial_fit</span></code></a>(X[, y, check_input])</p></td>
<td><p>Incremental fit with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.experimental.decomposition.IncrementalPCA.transform" title="cuml.experimental.decomposition.IncrementalPCA.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X[, convert_dtype])</p></td>
<td><p>Apply dimensionality reduction to X.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cuml.experimental.decomposition.IncrementalPCA.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/experimental/decomposition/incremental_pca.py#L205"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.decomposition.IncrementalPCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X, using minibatches of size batch_size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like or sparse matrix, shape (n_samples, n_features)</span></dt><dd><p>Training data, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt><strong>y</strong><span class="classifier">Ignored</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Returns the instance itself.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.decomposition.IncrementalPCA.get_param_names">
<code class="sig-name descname">get_param_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/experimental/decomposition/incremental_pca.py#L428"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.decomposition.IncrementalPCA.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="cuml.experimental.decomposition.IncrementalPCA.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">check_input</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/experimental/decomposition/incremental_pca.py#L262"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.decomposition.IncrementalPCA.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Incremental fit with X. All of X is processed as a single batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like or sparse matrix, shape (n_samples, n_features)</span></dt><dd><p>Training data, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt><strong>check_input</strong><span class="classifier">bool</span></dt><dd><p>Run check_array on X.</p>
</dd>
<dt><strong>y</strong><span class="classifier">Ignored</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Returns the instance itself.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.decomposition.IncrementalPCA.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">convert_dtype</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/experimental/decomposition/incremental_pca.py#L383"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.decomposition.IncrementalPCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply dimensionality reduction to X.</p>
<p>X is projected on the first principal components previously extracted
from a training set, using minibatches of size batch_size if X is
sparse.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like or sparse matrix, shape (n_samples, n_features)</span></dt><dd><p>New data, where n_samples is the number of samples
and n_features is the number of features.</p>
</dd>
<dt><strong>convert_dtype</strong><span class="classifier">bool, optional (default = False)</span></dt><dd><p>When set to True, the transform method will automatically
convert the input to the data type which was used to train the
model. This will increase memory used for the method.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">array-like, shape (n_samples, n_components)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-cuml.experimental.preprocessing">
<span id="preprocessing"></span><h3>Preprocessing<a class="headerlink" href="#module-cuml.experimental.preprocessing" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.Binarizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">Binarizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1851"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.Binarizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Binarize data (set feature values to 0 or 1) according to a threshold</p>
<p>Values greater than the threshold map to 1, while values less than
or equal to the threshold map to 0. With the default threshold of 0,
only positive values map to 1.</p>
<p>Binarization is a common operation on text count data where the
analyst can decide to only consider the presence or absence of a
feature rather than a quantified number of occurrences for instance.</p>
<p>It can also be used as a pre-processing step for estimators that
consider boolean random variables (e.g. modelled using the Bernoulli
distribution in a Bayesian setting).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>threshold</strong><span class="classifier">float, optional (0.0 by default)</span></dt><dd><p>Feature values below or equal to this are replaced by 0, above it by 1.
Threshold may not be less than 0 for operations on sparse matrices.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.binarize" title="cuml.experimental.preprocessing.binarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">binarize</span></code></a></dt><dd><p>Equivalent function without the estimator API.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>If the input is a sparse matrix, only the non-zero values are subject
to update by the Binarizer class.</p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">Binarizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Binarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># fit does nothing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">Binarizer()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[1., 0., 1.],</span>
<span class="go">       [1., 0., 0.],</span>
<span class="go">       [0., 1., 0.]])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.Binarizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1908"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.Binarizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Do nothing and return the estimator unchanged</p>
<p>This method is just there to implement the usual API and hence
work in pipelines.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.Binarizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1921"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.Binarizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Binarize each element of X</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data to binarize, element by element.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False,
a copy might be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.KBinsDiscretizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">KBinsDiscretizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_discretization.py#L47"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.KBinsDiscretizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bin continuous data into intervals.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>n_bins</strong><span class="classifier">int or array-like, shape (n_features,) (default=5)</span></dt><dd><p>The number of bins to produce. Raises ValueError if <code class="docutils literal notranslate"><span class="pre">n_bins</span> <span class="pre">&lt;</span> <span class="pre">2</span></code>.</p>
</dd>
<dt><strong>encode</strong><span class="classifier">{‘onehot’, ‘onehot-dense’, ‘ordinal’}, (default=’onehot’)</span></dt><dd><p>Method used to encode the transformed result.</p>
<dl class="simple">
<dt>onehot</dt><dd><p>Encode the transformed result with one-hot encoding
and return a sparse matrix. Ignored features are always
stacked to the right.</p>
</dd>
<dt>onehot-dense</dt><dd><p>Encode the transformed result with one-hot encoding
and return a dense array. Ignored features are always
stacked to the right.</p>
</dd>
<dt>ordinal</dt><dd><p>Return the bin identifier encoded as an integer value.</p>
</dd>
</dl>
</dd>
<dt><strong>strategy</strong><span class="classifier">{‘uniform’, ‘quantile’, ‘kmeans’}, (default=’quantile’)</span></dt><dd><p>Strategy used to define the widths of the bins.</p>
<dl class="simple">
<dt>uniform</dt><dd><p>All bins in each feature have identical widths.</p>
</dd>
<dt>quantile</dt><dd><p>All bins in each feature have the same number of points.</p>
</dd>
<dt>kmeans</dt><dd><p>Values in each bin have the same nearest center of a 1D k-means
cluster.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuml.preprocessing.Binarizer</span></code></dt><dd><p>Class used to bin values as <code class="docutils literal notranslate"><span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">1</span></code> based on a parameter <code class="docutils literal notranslate"><span class="pre">threshold</span></code>.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>In bin edges for feature <code class="docutils literal notranslate"><span class="pre">i</span></code>, the first and last values are used only for
<code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code>. During transform, bin edges are extended to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">bin_edges_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">])</span>
</pre></div>
</div>
<p>You can combine <code class="docutils literal notranslate"><span class="pre">KBinsDiscretizer</span></code> with
<code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.compose.ColumnTransformer</span></code> if you only want to preprocess
part of the features.</p>
<p><code class="docutils literal notranslate"><span class="pre">KBinsDiscretizer</span></code> might produce constant features (e.g., when
<code class="docutils literal notranslate"><span class="pre">encode</span> <span class="pre">=</span> <span class="pre">'onehot'</span></code> and certain bins do not contain any data).
These features can be removed with feature selection algorithms
(e.g., <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.feature_selection.VarianceThreshold</span></code>).</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span>   <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">KBinsDiscretizer</span><span class="p">(</span><span class="n">n_bins</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">encode</span><span class="o">=</span><span class="s1">'ordinal'</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">'uniform'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">KBinsDiscretizer(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xt</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xt</span>  
<span class="go">array([[ 0., 0., 0., 0.],</span>
<span class="go">       [ 1., 1., 1., 0.],</span>
<span class="go">       [ 2., 2., 2., 1.],</span>
<span class="go">       [ 2., 2., 2., 2.]])</span>
</pre></div>
</div>
<p>Sometimes it may be useful to convert the data back into the original
feature space. The <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">bin_edges_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">array([-2., -1.,  0.,  1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
<span class="go">array([[-1.5,  1.5, -3.5, -0.5],</span>
<span class="go">       [-0.5,  2.5, -2.5, -0.5],</span>
<span class="go">       [ 0.5,  3.5, -1.5,  0.5],</span>
<span class="go">       [ 0.5,  3.5, -1.5,  1.5]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_bins_</strong><span class="classifier">int array, shape (n_features,)</span></dt><dd><p>Number of bins per feature. Bins whose width are too small
(i.e., &lt;= 1e-8) are removed with a warning.</p>
</dd>
<dt><strong>bin_edges_</strong><span class="classifier">array of arrays, shape (n_features, )</span></dt><dd><p>The edges of each bin. Contain arrays of varying shapes <code class="docutils literal notranslate"><span class="pre">(n_bins_,</span> <span class="pre">)</span></code>
Ignored features will have empty arrays.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.KBinsDiscretizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_discretization.py#L150"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.KBinsDiscretizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">numeric array-like, shape (n_samples, n_features)</span></dt><dd><p>Data to be discretized.</p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored. This parameter exists only for compatibility with
<code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.pipeline.Pipeline</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>self</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.KBinsDiscretizer.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">Xt</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_discretization.py#L319"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.KBinsDiscretizer.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform discretized data back to original feature space.</p>
<p>Note that this function does not regenerate the original data
due to discretization rounding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>Xt</strong><span class="classifier">numeric array-like, shape (n_sample, n_features)</span></dt><dd><p>Transformed data in the binned space.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>Xinv</strong><span class="classifier">numeric array-like</span></dt><dd><p>Data in the original feature space.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.KBinsDiscretizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_discretization.py#L273"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.KBinsDiscretizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Discretize the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">numeric array-like, shape (n_samples, n_features)</span></dt><dd><p>Data to be discretized.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>Xt</strong><span class="classifier">numeric array-like or sparse matrix</span></dt><dd><p>Data in the binned space.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.MaxAbsScaler">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">MaxAbsScaler</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L846"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MaxAbsScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale each feature by its maximum absolute value.</p>
<p>This estimator scales and translates each feature individually such
that the maximal absolute value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.</p>
<p>This scaler can also be applied to sparse CSR or CSC matrices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>copy</strong><span class="classifier">boolean, optional, default is True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">maxabs_scale</span></code></dt><dd><p>Equivalent function without the estimator API.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">MaxAbsScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">MaxAbsScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">MaxAbsScaler()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 0.5, -1. ,  1. ],</span>
<span class="go">       [ 1. ,  0. ,  0. ],</span>
<span class="go">       [ 0. ,  1. , -0.5]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>scale_</strong><span class="classifier">ndarray, shape (n_features,)</span></dt><dd><p>Per feature relative scaling of the data.</p>
</dd>
<dt><strong>max_abs_</strong><span class="classifier">ndarray, shape (n_features,)</span></dt><dd><p>Per feature maximum absolute value.</p>
</dd>
<dt><strong>n_samples_seen_</strong><span class="classifier">int</span></dt><dd><p>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> calls.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MaxAbsScaler.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L916"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MaxAbsScaler.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the maximum absolute value to be used for later scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to compute the per-feature minimum and maximum
used for later scaling along the features axis.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MaxAbsScaler.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L997"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MaxAbsScaler.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale back the data to the original representation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd><p>The data that should be transformed back.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MaxAbsScaler.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L930"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MaxAbsScaler.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Online computation of max absolute value of X for later scaling.</p>
<p>All of X is processed as a single batch. This is intended for cases
when <a class="reference internal" href="#cuml.experimental.preprocessing.MaxAbsScaler.fit" title="cuml.experimental.preprocessing.MaxAbsScaler.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> is not feasible due to very large number of
<cite>n_samples</cite> or because X is read from a continuous stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to compute the mean and standard deviation
used for later scaling along the features axis.</p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Transformer instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MaxAbsScaler.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L974"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MaxAbsScaler.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale the data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd><p>The data that should be scaled.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.MinMaxScaler">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">MinMaxScaler</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L204"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MinMaxScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform features by scaling each feature to a given range.</p>
<p>This estimator scales and translates each feature individually such
that it is in the given range on the training set, e.g. between
zero and one.</p>
<p>The transformation is given by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_std</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">+</span> <span class="nb">min</span>
</pre></div>
</div>
<p>where min, max = feature_range.</p>
<p>This transformation is often used as an alternative to zero mean,
unit variance scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>feature_range</strong><span class="classifier">tuple (min, max), default=(0, 1)</span></dt><dd><p>Desired range of transformed data.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool, default=True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.minmax_scale" title="cuml.experimental.preprocessing.minmax_scale"><code class="xref py py-obj docutils literal notranslate"><span class="pre">minmax_scale</span></code></a></dt><dd><p>Equivalent function without the estimator API.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">18</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">MinMaxScaler()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">data_max_</span><span class="p">)</span>
<span class="go">[ 1. 18.]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">[[0.   0.  ]</span>
<span class="go"> [0.25 0.25]</span>
<span class="go"> [0.5  0.5 ]</span>
<span class="go"> [1.   1.  ]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span>
<span class="go">[[1.5 0. ]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>min_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Per feature adjustment for minimum. Equivalent to
<code class="docutils literal notranslate"><span class="pre">min</span> <span class="pre">-</span> <span class="pre">X.min(axis=0)</span> <span class="pre">*</span> <span class="pre">self.scale_</span></code></p>
</dd>
<dt><strong>scale_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Per feature relative scaling of the data. Equivalent to
<code class="docutils literal notranslate"><span class="pre">(max</span> <span class="pre">-</span> <span class="pre">min)</span> <span class="pre">/</span> <span class="pre">(X.max(axis=0)</span> <span class="pre">-</span> <span class="pre">X.min(axis=0))</span></code></p>
</dd>
<dt><strong>data_min_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Per feature minimum seen in the data</p>
</dd>
<dt><strong>data_max_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Per feature maximum seen in the data</p>
</dd>
<dt><strong>data_range_</strong><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Per feature range <code class="docutils literal notranslate"><span class="pre">(data_max_</span> <span class="pre">-</span> <span class="pre">data_min_)</span></code> seen in the data</p>
</dd>
<dt><strong>n_samples_seen_</strong><span class="classifier">int</span></dt><dd><p>The number of samples processed by the estimator.
It will be reset on new calls to fit, but increments across
<code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> calls.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MinMaxScaler.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L302"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MinMaxScaler.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the minimum and maximum to be used for later scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>The data used to compute the per-feature minimum and maximum
used for later scaling along the features axis.</p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Fitted scaler.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MinMaxScaler.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L399"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MinMaxScaler.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Undo the scaling of X according to feature_range.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Input data that will be transformed. It cannot be sparse.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>Xt</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Transformed data.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MinMaxScaler.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L324"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MinMaxScaler.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Online computation of min and max on X for later scaling.</p>
<p>All of X is processed as a single batch. This is intended for cases
when <a class="reference internal" href="#cuml.experimental.preprocessing.MinMaxScaler.fit" title="cuml.experimental.preprocessing.MinMaxScaler.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> is not feasible due to very large number of
<cite>n_samples</cite> or because X is read from a continuous stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>The data used to compute the mean and standard deviation
used for later scaling along the features axis.</p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Transformer instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.MinMaxScaler.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L374"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.MinMaxScaler.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale features of X according to feature_range.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Input data that will be transformed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>Xt</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Transformed data.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.Normalizer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">Normalizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1719"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.Normalizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalize samples individually to unit norm.</p>
<p>Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1, l2 or inf) equals one.</p>
<p>This transformer is able to work both with dense numpy arrays and
sparse matrix</p>
<p>Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>norm</strong><span class="classifier">‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)</span></dt><dd><p>The norm to use to normalize each non zero sample. If norm=’max’
is used, values will be rescaled by the maximum of the absolute
values.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.normalize" title="cuml.experimental.preprocessing.normalize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normalize</span></code></a></dt><dd><p>Equivalent function without the estimator API.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># fit does nothing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">Normalizer()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[0.8, 0.2, 0.4, 0.4],</span>
<span class="go">       [0.1, 0.3, 0.9, 0.3],</span>
<span class="go">       [0.5, 0.7, 0.5, 0.1]])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.Normalizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1777"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.Normalizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Do nothing and return the estimator unchanged</p>
<p>This method is just there to implement the usual API and hence
work in pipelines.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, CSR matrix}</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.Normalizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1790"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.Normalizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale each non zero row of X to unit norm</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, CSR matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data to normalize, row by row.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool, optional (default: None)</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False,
a copy might be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.PolynomialFeatures">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">PolynomialFeatures</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1348"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.PolynomialFeatures" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate polynomial and interaction features.</p>
<p>Generate a new feature matrix consisting of all polynomial combinations
of the features with degree less than or equal to the specified degree.
For example, if an input sample is two dimensional and of the form
[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>degree</strong><span class="classifier">integer</span></dt><dd><p>The degree of the polynomial features. Default = 2.</p>
</dd>
<dt><strong>interaction_only</strong><span class="classifier">boolean, default = False</span></dt><dd><p>If true, only interaction features are produced: features that are
products of at most <code class="docutils literal notranslate"><span class="pre">degree</span></code> <em>distinct</em> input features (so not
<code class="docutils literal notranslate"><span class="pre">x[1]</span> <span class="pre">**</span> <span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">x[0]</span> <span class="pre">*</span> <span class="pre">x[2]</span> <span class="pre">**</span> <span class="pre">3</span></code>, etc.).</p>
</dd>
<dt><strong>include_bias</strong><span class="classifier">boolean</span></dt><dd><p>If True (default), then include a bias column, the feature in which
all polynomial powers are zero (i.e. a column of ones - acts as an
intercept term in a linear model).</p>
</dd>
<dt><strong>order</strong><span class="classifier">str in {‘C’, ‘F’}, default ‘C’</span></dt><dd><p>Order of output array in the dense case. ‘F’ order is faster to
compute, but may slow down subsequent estimators.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Be aware that the number of features in the output array scales
polynomially in the number of features of the input array, and
exponentially in the degree. High degrees can cause overfitting.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [2, 3],</span>
<span class="go">       [4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 1.,  0.,  1.,  0.,  0.,  1.],</span>
<span class="go">       [ 1.,  2.,  3.,  4.,  6.,  9.],</span>
<span class="go">       [ 1.,  4.,  5., 16., 20., 25.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 1.,  0.,  1.,  0.],</span>
<span class="go">       [ 1.,  2.,  3.,  6.],</span>
<span class="go">       [ 1.,  4.,  5., 20.]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>powers_</strong><span class="classifier">array, shape (n_output_features, n_input_features)</span></dt><dd><p>powers_[i, j] is the exponent of the jth input in the ith output.</p>
</dd>
<dt><strong>n_input_features_</strong><span class="classifier">int</span></dt><dd><p>The total number of input features.</p>
</dd>
<dt><strong>n_output_features_</strong><span class="classifier">int</span></dt><dd><p>The total number of polynomial output features. The number of output
features is computed by iterating over all suitably sized combinations
of input features.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.PolynomialFeatures.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1471"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.PolynomialFeatures.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute number of output features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like, shape (n_samples, n_features)</span></dt><dd><p>The data.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">instance</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.PolynomialFeatures.get_feature_names">
<code class="sig-name descname">get_feature_names</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_features</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1441"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.PolynomialFeatures.get_feature_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Return feature names for output features</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>input_features</strong><span class="classifier">list of string, length n_features, optional</span></dt><dd><p>String names for input features if available. By default,
“x0”, “x1”, … “xn_features” is used.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output_feature_names</strong><span class="classifier">list of string, length n_output_features</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.PolynomialFeatures.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1494"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.PolynomialFeatures.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform data to polynomial features</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data to transform, row by row.</p>
<p>Prefer CSR over CSC for sparse input (for speed), but CSC is
required if the degree is 4 or higher. If the degree is less than
4 and the input format is CSC, it will be converted to CSR, have
its polynomial features generated, then converted back to CSC.</p>
<p>If the degree is 2 or 3, the method described in “Leveraging
Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices
Using K-Simplex Numbers” by Andrew Nystrom and John Hughes is
used, which is much faster than the method used on CSC input. For
this reason, a CSC input will be converted to CSR, and the output
will be converted back to CSC prior to being returned, hence the
preference of CSR.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>XP</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, NP]</span></dt><dd><p>The matrix of features, where NP is the number of polynomial
features generated from the combination of inputs.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.RobustScaler">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">RobustScaler</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1080"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.RobustScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale features using statistics that are robust to outliers.</p>
<p>This Scaler removes the median and scales the data according to the
quantile range (defaults to IQR: Interquartile Range). The IQR is the range
between the 1st quartile (25th quantile) and the 3rd quartile (75th
quantile).</p>
<p>Centering and scaling happen independently on each feature by computing the
relevant statistics on the samples in the training set. Median and
interquartile range are then stored to be used on later data using the
<code class="docutils literal notranslate"><span class="pre">transform</span></code> method.</p>
<p>Standardization of a dataset is a common requirement for many machine
learning estimators. Typically this is done by removing the mean and
scaling to unit variance. However, outliers can often influence the sample
mean / variance in a negative way. In such cases, the median and the
interquartile range often give better results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>with_centering</strong><span class="classifier">boolean, default=True</span></dt><dd><p>If True, center the data before scaling.
This will cause <code class="docutils literal notranslate"><span class="pre">transform</span></code> to raise an exception when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</p>
</dd>
<dt><strong>with_scaling</strong><span class="classifier">boolean, default=True</span></dt><dd><p>If True, scale the data to interquartile range.</p>
</dd>
<dt><strong>quantile_range</strong><span class="classifier">tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0</span></dt><dd><p>Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR
Quantile range used to calculate <code class="docutils literal notranslate"><span class="pre">scale_</span></code>.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default=True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.robust_scale" title="cuml.experimental.preprocessing.robust_scale"><code class="xref py py-obj docutils literal notranslate"><span class="pre">robust_scale</span></code></a></dt><dd><p>Equivalent function without the estimator API.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuml.decomposition.PCA</span></code></dt><dd><p>Further removes the linear correlation across features with <code class="docutils literal notranslate"><span class="pre">whiten=True</span></code>.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">RobustScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>
<span class="go">RobustScaler()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 0. , -2. ,  0. ],</span>
<span class="go">       [-1. ,  0. ,  0.4],</span>
<span class="go">       [ 1. ,  0. , -1.6]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>center_</strong><span class="classifier">array of floats</span></dt><dd><p>The median value for each feature in the training set.</p>
</dd>
<dt><strong>scale_</strong><span class="classifier">array of floats</span></dt><dd><p>The (scaled) interquartile range for each feature in the training set.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.RobustScaler.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1159"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.RobustScaler.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the median and quantiles to be used for scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, CSC matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to compute the median and quantiles
used for later scaling along the features axis.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.RobustScaler.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1245"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.RobustScaler.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale back the data to the original representation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd><p>The data used to scale along the specified axis.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.RobustScaler.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1220"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.RobustScaler.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Center and scale the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd><p>The data used to scale along the specified axis.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.SimpleImputer">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">SimpleImputer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_imputation.py#L140"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.SimpleImputer" title="Permalink to this definition">¶</a></dt>
<dd><p>Imputation transformer for completing missing values.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>missing_values</strong><span class="classifier">number, string, np.nan (default) or None</span></dt><dd><p>The placeholder for the missing values. All occurrences of
<cite>missing_values</cite> will be imputed. For pandas’ dataframes with
nullable integer dtypes with missing values, <cite>missing_values</cite>
should be set to <cite>np.nan</cite>, since <cite>pd.NA</cite> will be converted to <cite>np.nan</cite>.</p>
</dd>
<dt><strong>strategy</strong><span class="classifier">string, default=’mean’</span></dt><dd><p>The imputation strategy.</p>
<ul class="simple">
<li><p>If “mean”, then replace missing values using the mean along
each column. Can only be used with numeric data.</p></li>
<li><p>If “median”, then replace missing values using the median along
each column. Can only be used with numeric data.</p></li>
<li><p>If “most_frequent”, then replace missing using the most frequent
value along each column. Can be used with strings or numeric data.</p></li>
<li><p>If “constant”, then replace missing values with fill_value. Can be
used with strings or numeric data.</p></li>
</ul>
<p>strategy=”constant” for fixed value imputation.</p>
</dd>
<dt><strong>fill_value</strong><span class="classifier">string or numerical value, default=None</span></dt><dd><p>When strategy == “constant”, fill_value is used to replace all
occurrences of missing_values.
If left to the default, fill_value will be 0 when imputing numerical
data and “missing_value” for strings or object data types.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">integer, default=0</span></dt><dd><p>Controls the verbosity of the imputer.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, default=True</span></dt><dd><p>If True, a copy of X will be created. If False, imputation will
be done in-place whenever possible. Note that, in the following cases,
a new copy will always be made, even if <cite>copy=False</cite>:</p>
<ul class="simple">
<li><p>If X is not an array of floating values;</p></li>
<li><p>If X is encoded as a CSR matrix;</p></li>
<li><p>If add_indicator=True.</p></li>
</ul>
</dd>
<dt><strong>add_indicator</strong><span class="classifier">boolean, default=False</span></dt><dd><p>If True, a <code class="xref py py-class docutils literal notranslate"><span class="pre">MissingIndicator</span></code> transform will stack onto output
of the imputer’s transform. This allows a predictive estimator
to account for missingness despite imputation. If a feature has no
missing values at fit/train time, the feature won’t appear on
the missing indicator even if there are missing values at
transform/test time.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">IterativeImputer</span></code></dt><dd><p>Multivariate imputation of missing values.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>Columns which only contained missing values at <a class="reference internal" href="#cuml.experimental.preprocessing.SimpleImputer.fit" title="cuml.experimental.preprocessing.SimpleImputer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> are discarded
upon <a class="reference internal" href="#cuml.experimental.preprocessing.SimpleImputer.transform" title="cuml.experimental.preprocessing.SimpleImputer.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform()</span></code></a> if strategy is not “constant”.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">imp_mean</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">'mean'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">imp_mean</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="go">SimpleImputer()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">imp_mean</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[[ 7.   2.   3. ]</span>
<span class="go"> [ 4.   3.5  6. ]</span>
<span class="go"> [10.   3.5  9. ]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>statistics_</strong><span class="classifier">array of shape (n_features,)</span></dt><dd><p>The imputation fill value for each feature.
Computing statistics can result in <cite>np.nan</cite> values.
During <a class="reference internal" href="#cuml.experimental.preprocessing.SimpleImputer.transform" title="cuml.experimental.preprocessing.SimpleImputer.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform()</span></code></a>, features corresponding to <cite>np.nan</cite>
statistics will be discarded.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.SimpleImputer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_imputation.py#L276"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.SimpleImputer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the imputer on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape (n_samples, n_features)</span></dt><dd><p>Input data, where <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> is the number of samples and
<code class="docutils literal notranslate"><span class="pre">n_features</span></code> is the number of features.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">SimpleImputer</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.SimpleImputer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_imputation.py#L384"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.SimpleImputer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Impute all missing values in X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape (n_samples, n_features)</span></dt><dd><p>The input data to complete.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="cuml.experimental.preprocessing.StandardScaler">
<em class="property">class </em><code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">StandardScaler</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L495"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.StandardScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Standardize features by removing the mean and scaling to unit variance</p>
<p>The standard score of a sample <cite>x</cite> is calculated as:</p>
<blockquote>
<div><p>z = (x - u) / s</p>
</div></blockquote>
<p>where <cite>u</cite> is the mean of the training samples or zero if <cite>with_mean=False</cite>,
and <cite>s</cite> is the standard deviation of the training samples or one if
<cite>with_std=False</cite>.</p>
<p>Centering and scaling happen independently on each feature by computing
the relevant statistics on the samples in the training set. Mean and
standard deviation are then stored to be used on later data using
<a class="reference internal" href="#cuml.experimental.preprocessing.StandardScaler.transform" title="cuml.experimental.preprocessing.StandardScaler.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform()</span></code></a>.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the
individual features do not more or less look like standard normally
distributed data (e.g. Gaussian with 0 mean and unit variance).</p>
<p>For instance many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the L1 and L2 regularizers of linear models) assume that
all features are centered around 0 and have variance in the same
order. If a feature has a variance that is orders of magnitude larger
that others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.</p>
<p>This scaler can also be applied to sparse CSR or CSC matrices by passing
<cite>with_mean=False</cite> to avoid breaking the sparsity structure of the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>copy</strong><span class="classifier">boolean, optional, default True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
<dt><strong>with_mean</strong><span class="classifier">boolean, True by default</span></dt><dd><p>If True, center the data before scaling.
This does not work (and will raise an exception) when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</p>
</dd>
<dt><strong>with_std</strong><span class="classifier">boolean, True by default</span></dt><dd><p>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.scale" title="cuml.experimental.preprocessing.scale"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale</span></code></a></dt><dd><p>Equivalent function without the estimator API.</p>
</dd>
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">cuml.decomposition.PCA</span></code></dt><dd><p>Further removes the linear correlation across features with ‘whiten=True’.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>NaNs are treated as missing values: disregarded in fit, and maintained in
transform.</p>
<p>We use a biased estimator for the standard deviation, equivalent to
<cite>numpy.std(x, ddof=0)</cite>. Note that the choice of <cite>ddof</cite> is unlikely to
affect model performance.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">StandardScaler()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span>
<span class="go">[0.5 0.5]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="go">[[-1. -1.]</span>
<span class="go"> [-1. -1.]</span>
<span class="go"> [ 1.  1.]</span>
<span class="go"> [ 1.  1.]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span>
<span class="go">[[3. 3.]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>scale_</strong><span class="classifier">ndarray or None, shape (n_features,)</span></dt><dd><p>Per feature relative scaling of the data. This is calculated using
<cite>sqrt(var_)</cite>. Equal to <code class="docutils literal notranslate"><span class="pre">None</span></code> when <code class="docutils literal notranslate"><span class="pre">with_std=False</span></code>.</p>
</dd>
<dt><strong>mean_</strong><span class="classifier">ndarray or None, shape (n_features,)</span></dt><dd><p>The mean value for each feature in the training set.
Equal to <code class="docutils literal notranslate"><span class="pre">None</span></code> when <code class="docutils literal notranslate"><span class="pre">with_mean=False</span></code>.</p>
</dd>
<dt><strong>var_</strong><span class="classifier">ndarray or None, shape (n_features,)</span></dt><dd><p>The variance for each feature in the training set. Used to compute
<cite>scale_</cite>. Equal to <code class="docutils literal notranslate"><span class="pre">None</span></code> when <code class="docutils literal notranslate"><span class="pre">with_std=False</span></code>.</p>
</dd>
<dt><strong>n_samples_seen_</strong><span class="classifier">int or array, shape (n_features,)</span></dt><dd><p>The number of samples processed by the estimator for each feature.
If there are not missing samples, the <code class="docutils literal notranslate"><span class="pre">n_samples_seen</span></code> will be an
integer, otherwise it will be an array.
Will be reset on new calls to fit, but increments across
<code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> calls.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.StandardScaler.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L619"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.StandardScaler.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the mean and std to be used for later scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to compute the mean and standard deviation
used for later scaling along the features axis.</p>
</dd>
<dt><strong>y</strong></dt><dd><p>Ignored</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.StandardScaler.inverse_transform">
<code class="sig-name descname">inverse_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L793"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.StandardScaler.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale back the data to the original representation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to scale along the features axis.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool, optional (default: None)</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False,
a copy might be triggered by a conversion.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_tr</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>Transformed array.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.StandardScaler.partial_fit">
<code class="sig-name descname">partial_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L636"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.StandardScaler.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Online computation of mean and std on X for later scaling.</p>
<p>All of X is processed as a single batch. This is intended for cases
when <a class="reference internal" href="#cuml.experimental.preprocessing.StandardScaler.fit" title="cuml.experimental.preprocessing.StandardScaler.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> is not feasible due to very large number of
<cite>n_samples</cite> or because X is read from a continuous stream.</p>
<p>The algorithm for incremental mean and std is given in Equation 1.5a,b
in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. “Algorithms
for computing the sample variance: Analysis and recommendations.”
The American Statistician 37.3 (1983): 242-247:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to compute the mean and standard deviation
used for later scaling along the features axis.</p>
</dd>
<dt><strong>y</strong><span class="classifier">None</span></dt><dd><p>Ignored.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Transformer instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="cuml.experimental.preprocessing.StandardScaler.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L756"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.StandardScaler.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform standardization by centering and scaling</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data used to scale along the features axis.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool, optional (default: None)</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False,
a copy might be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py function">
<dt id="cuml.experimental.preprocessing.add_dummy_feature">
<code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">add_dummy_feature</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">value</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L2043"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.add_dummy_feature" title="Permalink to this definition">¶</a></dt>
<dd><p>Augment dataset with an additional dummy feature.</p>
<p>This is useful for fitting an intercept term with implementations which
cannot otherwise fit it directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>Data.</p>
</dd>
<dt><strong>value</strong><span class="classifier">float</span></dt><dd><p>Value to use for the dummy feature.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array, sparse matrix}, shape [n_samples, n_features + 1]</span></dt><dd><p>Same data with dummy feature added as first column.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">cuml.preprocessing</span> <span class="kn">import</span> <span class="n">add_dummy_feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add_dummy_feature</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([[1., 0., 1.],</span>
<span class="go">       [1., 1., 0.]])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="cuml.experimental.preprocessing.binarize">
<code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">binarize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1811"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.binarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean thresholding of array-like or sparse matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data to binarize, element by element.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float, optional (0.0 by default)</span></dt><dd><p>Feature values below or equal to this are replaced by 0, above it by 1.
Threshold may not be less than 0 for operations on sparse matrices.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.Binarizer" title="cuml.experimental.preprocessing.Binarizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Binarizer</span></code></a></dt><dd><p>Performs binarization using the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> API</p>
</dd>
</dl>
</div>
</dd></dl>
<dl class="py function">
<dt id="cuml.experimental.preprocessing.minmax_scale">
<code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">minmax_scale</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">feature_range</span><span class="o">=</span><span class="default_value">0, 1</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L428"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.minmax_scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform features by scaling each feature to a given range.</p>
<p>This estimator scales and translates each feature individually such
that it is in the given range on the training set, i.e. between
zero and one.</p>
<p>The transformation is given by (when <code class="docutils literal notranslate"><span class="pre">axis=0</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_std</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">+</span> <span class="nb">min</span>
</pre></div>
</div>
<p>where min, max = feature_range.</p>
<p>The transformation is calculated as (when <code class="docutils literal notranslate"><span class="pre">axis=0</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="nb">min</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
<span class="n">where</span> <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>This transformation is often used as an alternative to zero mean,
unit variance scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>The data.</p>
</dd>
<dt><strong>feature_range</strong><span class="classifier">tuple (min, max), default=(0, 1)</span></dt><dd><p>Desired range of transformed data.</p>
</dd>
<dt><strong>axis</strong><span class="classifier">int, default=0</span></dt><dd><p>Axis used to scale along. If 0, independently scale each feature,
otherwise (if 1) scale each sample.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">bool, default=True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.MinMaxScaler" title="cuml.experimental.preprocessing.MinMaxScaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinMaxScaler</span></code></a></dt><dd><p>Performs scaling to a given range using the``Transformer`` API</p>
</dd>
</dl>
</div>
</dd></dl>
<dl class="py function">
<dt id="cuml.experimental.preprocessing.normalize">
<code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">normalize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">norm</span><span class="o">=</span><span class="default_value">'l2'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">return_norm</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1625"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Scale input vectors individually to unit norm (vector length).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>The data to normalize, element by element.
Please provide CSC matrix to normalize on axis 0,
conversely provide CSR matrix to normalize on axis 1</p>
</dd>
<dt><strong>norm</strong><span class="classifier">‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)</span></dt><dd><p>The norm to use to normalize each non zero sample (or each non-zero
feature if axis is 0).</p>
</dd>
<dt><strong>axis</strong><span class="classifier">0 or 1, optional (1 by default)</span></dt><dd><p>axis used to normalize the data along. If 1, independently normalize
each sample, otherwise (if 0) normalize each feature.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
<dt><strong>return_norm</strong><span class="classifier">boolean, default False</span></dt><dd><p>whether to return the computed norms</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, shape [n_samples, n_features]</span></dt><dd><p>Normalized input X.</p>
</dd>
<dt><strong>norms</strong><span class="classifier">array, shape [n_samples] if axis=1 else [n_features]</span></dt><dd><p>An array of norms along given axis for X.
When X is sparse, a NotImplementedError will be raised
for norm ‘l1’ or ‘l2’.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.Normalizer" title="cuml.experimental.preprocessing.Normalizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Normalizer</span></code></a></dt><dd><p>Performs normalization using the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> API</p>
</dd>
</dl>
</div>
</dd></dl>
<dl class="py function">
<dt id="cuml.experimental.preprocessing.robust_scale">
<code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">robust_scale</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">with_centering</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">with_scaling</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">quantile_range</span><span class="o">=</span><span class="default_value">25.0, 75.0</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L1274"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.robust_scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Standardize a dataset along any axis</p>
<p>Center to the median and component wise scale
according to the interquartile range.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd><p>The data to center and scale.</p>
</dd>
<dt><strong>axis</strong><span class="classifier">int (0 by default)</span></dt><dd><p>axis used to compute the medians and IQR along. If 0,
independently scale each feature, otherwise (if 1) scale
each sample.</p>
</dd>
<dt><strong>with_centering</strong><span class="classifier">boolean, True by default</span></dt><dd><p>If True, center the data before scaling.</p>
</dd>
<dt><strong>with_scaling</strong><span class="classifier">boolean, True by default</span></dt><dd><p>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</p>
</dd>
<dt><strong>quantile_range</strong><span class="classifier">tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0</span></dt><dd><p>Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR
Quantile range used to calculate <code class="docutils literal notranslate"><span class="pre">scale_</span></code>.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default is True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.RobustScaler" title="cuml.experimental.preprocessing.RobustScaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RobustScaler</span></code></a></dt><dd><p>Performs centering and scaling using the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> API</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>This implementation will refuse to center sparse matrices
since it would make them non-sparse and would potentially crash the
program with memory exhaustion problems.</p>
<p>Instead the caller is expected to either set explicitly
<cite>with_centering=False</cite> (in that case, only variance scaling will be
performed on the features of the CSR matrix) or to densify the matrix
if he/she expects the materialized dense array to fit in memory.</p>
<p>To avoid memory copy the caller should pass a CSR matrix.</p>
</dd></dl>
<dl class="py function">
<dt id="cuml.experimental.preprocessing.scale">
<code class="sig-prename descclassname">cuml.experimental.preprocessing.</code><code class="sig-name descname">scale</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">with_mean</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">with_std</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/rapidsai/cuml/blob/544ec2f/python/cuml/_thirdparty/sklearn/preprocessing/_data.py#L89"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cuml.experimental.preprocessing.scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Standardize a dataset along any axis</p>
<p>Center to the mean and component wise scale to unit variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}</span></dt><dd><p>The data to center and scale.</p>
</dd>
<dt><strong>axis</strong><span class="classifier">int (0 by default)</span></dt><dd><p>axis used to compute the means and standard deviations along. If 0,
independently standardize each feature, otherwise (if 1) standardize
each sample.</p>
</dd>
<dt><strong>with_mean</strong><span class="classifier">boolean, True by default</span></dt><dd><p>If True, center the data before scaling.</p>
</dd>
<dt><strong>with_std</strong><span class="classifier">boolean, True by default</span></dt><dd><p>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean, optional, default True</span></dt><dd><p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#cuml.experimental.preprocessing.StandardScaler" title="cuml.experimental.preprocessing.StandardScaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StandardScaler</span></code></a></dt><dd><p>Performs scaling to unit variance using the``Transformer`` API</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>This implementation will refuse to center sparse matrices
since it would make them non-sparse and would potentially crash the
program with memory exhaustion problems.</p>
<p>Instead the caller is expected to either set explicitly
<cite>with_mean=False</cite> (in that case, only variance scaling will be
performed on the features of the sparse matrix) or to densify the matrix
if he/she expects the materialized dense array to fit in memory.</p>
<p>For optimal processing the caller should pass a CSC matrix.</p>
<p>NaNs are treated as missing values: disregarded to compute the statistics,
and maintained during the data transformation.</p>
<p>We use a biased estimator for the standard deviation, equivalent to
<cite>numpy.std(x, ddof=0)</cite>. Note that the choice of <cite>ddof</cite> is unlikely to
affect model performance.</p>
</dd></dl>
</div>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="cuml_intro.html" rel="next" title="Intro and key concepts for cuML">Next <span class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="index.html" rel="prev" title="Welcome to cuML’s documentation!"><span class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        
        © Copyright 2020, nvidia

    </p>
</div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script defer id="rapids-selector-js" src="/assets/js/custom.js"></script></body>
</html>