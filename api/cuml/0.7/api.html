

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>cuML API Reference &mdash; cuml 0.7.0.dev0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to cuML’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> cuml
          

          
          </a>

          
            
            
              <div class="version">
                0.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">cuML API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ridge-regression">Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nearest-neighbors">Nearest Neighbors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#k-means-clustering">K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dbscan">DBSCAN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kalman-filter">Kalman Filter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#principal-component-analysis">Principal Component Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#truncated-svd">Truncated SVD</a></li>
<li class="toctree-l2"><a class="reference internal" href="#umap">UMAP</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">cuml</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>cuML API Reference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="cuml-api-reference">
<h1>cuML API Reference<a class="headerlink" href="#cuml-api-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.LinearRegression">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">LinearRegression</code><a class="headerlink" href="#cuml.LinearRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>LinearRegression is a simple machine learning model where the response y is modelled by a
linear combination of the predictors in X.</p>
<p>cuML’s LinearRegression expects either a cuDF DataFrame or a NumPy matrix and provides 2
algorithms SVD and Eig to fit a linear model. SVD is more stable, but Eig (default)
is much more faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>algorithm</strong><span class="classifier">‘eig’ or ‘svd’ (default = ‘eig’)</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much faster.
SVD is slower, but is guaranteed to be stable.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, LinearRegression tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2 norm.
If False, no scaling will be done.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>LinearRegression suffers from multicollinearity (when columns are correlated with each other),
and variance explosions from outliers. Consider using Ridge Regression to fix the multicollinearity 
problem,and consider maybe first DBSCAN to remove the outliers, or using leverage statistics to 
filter possible outliers.</p>
<p><strong>Applications of LinearRegression</strong></p>
<blockquote>
<div><p>LinearRegression is used in regression tasks where one wants to predict say sales or house prices.
It is also used in extrapolation or time series tasks, dynamic systems modelling and many other
machine learning tasks. This model should be first tried if the machine learning problem is a
regression task (predicting a continuous variable).</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">scikitlearn’s OLS</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;eig&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">1.0000001</span>
            <span class="mi">1</span> <span class="mf">1.9999998</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">15.999999</span>
            <span class="mi">1</span> <span class="mf">14.999999</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <a href="#id7"><span class="problematic" id="id8">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LinearRegression.fit" title="cuml.LinearRegression.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LinearRegression.get_params" title="cuml.LinearRegression.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.LinearRegression.predict" title="cuml.LinearRegression.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.LinearRegression.set_params" title="cuml.LinearRegression.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.LinearRegression.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
<dt><strong>y: cuDF DataFrame</strong></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LinearRegression.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>self</em>, <em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LinearRegression.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.LinearRegression.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>self</em>, <em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.LinearRegression.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="ridge-regression">
<h2>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.Ridge">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">Ridge</code><a class="headerlink" href="#cuml.Ridge" title="Permalink to this definition">¶</a></dt>
<dd><p>Ridge extends LinearRegression by providing L2 regularization on the coefficients when
predicting response y with a linear combination of the predictors in X. It can reduce
the variance of the predictors, and improves the conditioning of the problem.</p>
<p>cuML’s Ridge expects a cuDF DataFrame, and provides 3 algorithms SVD, Eig and CD to
fit a linear model. SVD is more stable, but Eig (default) is much more faster. CD uses
Coordinate Descent and can be faster if the data is large.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha</strong><span class="classifier">float or double</span></dt><dd><p>Regularization strength - must be a positive float. Larger values specify
stronger regularization. Array input will be supported later.</p>
</dd>
<dt><strong>solver</strong><span class="classifier">‘eig’ or ‘svd’ or ‘cd’ (default = ‘eig’)</span></dt><dd><p>Eig uses a eigendecomposition of the covariance matrix, and is much faster.
SVD is slower, but is guaranteed to be stable.
CD or Coordinate Descent is very fast and is suitable for large problems.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, Ridge tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, the predictors in X will be normalized by dividing by it’s L2 norm.
If False, no scaling will be done.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Ridge provides L2 regularization. This means that the coefficients can shrink to become
very very small, but not zero. This can cause issues of interpretabiliy on the coefficients.
Consider using Lasso, or thresholding small coefficients to zero.</p>
<p><strong>Applications of Ridge</strong></p>
<blockquote>
<div><p>Ridge Regression is used in the same way as LinearRegression, but is used more frequently
as it does not suffer from multicollinearity issues. Ridge is used in insurance premium
prediction, stock market analysis and much more.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">scikitlearn’s Ridge</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">cuml.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">solver</span> <span class="o">=</span> <span class="s2">&quot;eig&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="p">)</span>

<span class="n">result_ridge</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cudf</span><span class="p">,</span> <span class="n">y_cudf</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;intercept:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_ridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">X_new</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_new</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">result_ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">1.0000001</span>
            <span class="mi">1</span> <span class="mf">1.9999998</span>

<span class="n">Intercept</span><span class="p">:</span>
            <span class="mf">3.0</span>

<span class="n">Preds</span><span class="p">:</span>

            <span class="mi">0</span> <span class="mf">15.999999</span>
            <span class="mi">1</span> <span class="mf">14.999999</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array, shape (n_features)</span></dt><dd><p>The estimated coefficients for the linear regression model.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">array</span></dt><dd><p>The independent term. If <a href="#id9"><span class="problematic" id="id10">fit_intercept_</span></a> is False, will be 0.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Ridge.fit" title="cuml.Ridge.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Ridge.get_params" title="cuml.Ridge.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.Ridge.predict" title="cuml.Ridge.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.Ridge.set_params" title="cuml.Ridge.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.Ridge.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
<dt><strong>y: cuDF DataFrame</strong></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Ridge.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>self</em>, <em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Ridge.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.Ridge.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>self</em>, <em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.Ridge.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stochastic-gradient-descent">
<h2>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.SGD">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">SGD</code><a class="headerlink" href="#cuml.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Stochastic Gradient Descent is a very common machine learning algorithm where one optimizes
some cost function via gradient steps. This makes SGD very attractive for large problems
when the exact solution is hard or even impossible to find.</p>
<p>cuML’s SGD algorithm accepts a numpy matrix or a cuDF DataFrame as the input dataset.
The SGD algorithm currently works with linear regression, ridge regression and SVM models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">‘hinge’, ‘log’, ‘squared_loss’ (default = ‘squared_loss’)</span></dt><dd><p>‘hinge’ uses linear SVM   
‘log’ uses logistic regression
‘squared_loss’ uses linear regression</p>
</dd>
<dt><strong>penalty: ‘none’, ‘l1’, ‘l2’, ‘elasticnet’ (default = ‘none’)</strong></dt><dd><p>‘none’ does not perform any regularization
‘l1’ performs L1 norm (Lasso) which minimizes the sum of the abs value of coefficients
‘l2’ performs L2 norm (Ridge) which minimizes the sum of the square of the coefficients
‘elasticnet’ performs Elastic Net regularization which is a weighted average of L1 and L2 norms</p>
</dd>
<dt><strong>alpha: float (default = 0.0001)</strong></dt><dd><p>The constant value which decides the degree of regularization</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, the model tries to correct for the global mean of y.
If False, the model expects that you have centered the data.</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int (default = 1000)</span></dt><dd><p>The number of times the model should iterate through the entire dataset during training (default = 1000)</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-3)</span></dt><dd><p>The training process will stop if current_loss &gt; previous_loss - tol</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>True, shuffles the training data after each epoch
False, does not shuffle the training data after each epoch</p>
</dd>
<dt><strong>eta0</strong><span class="classifier">float (default = 0.0)</span></dt><dd><p>Initial learning rate</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The exponent used for calculating the invscaling learning rate</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">‘optimal’, ‘constant’, ‘invscaling’, ‘adaptive’ (default = ‘constant’)</span></dt><dd><p>optimal option supported in the next version
constant keeps the learning rate constant
adaptive changes the learning rate if the training loss or the validation accuracy does not improve for n_iter_no_change epochs.
The old learning rate is generally divide by 5</p>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int (default = 5)</span></dt><dd><p>the number of epochs to train without any imporvement in the model</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For additional docs, see <a href="#id1"><span class="problematic" id="id2">`</span></a>scikitlearn’s OLS &lt;<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html</a>&gt;</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.solvers</span> <span class="kn">import</span> <span class="n">SGD</span> <span class="k">as</span> <span class="n">cumlSGD</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">pred_data</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">datatype</span><span class="p">)</span>
<span class="n">pred_data</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">datatype</span><span class="p">)</span>

<span class="n">cu_sgd</span> <span class="o">=</span> <span class="n">cumlSGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lrate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

<span class="n">cu_sgd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">cu_pred</span> <span class="o">=</span> <span class="n">cu_sgd</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_data</span><span class="p">)</span><span class="o">.</span><span class="n">to_array</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot; cuML intercept : &quot;</span><span class="p">,</span> <span class="n">cu_sgd</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot; cuML coef : &quot;</span><span class="p">,</span> <span class="n">cu_sgd</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;cuML predictions : &quot;</span><span class="p">,</span> <span class="n">cu_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cuML</span> <span class="n">intercept</span> <span class="p">:</span>  <span class="mf">0.004561662673950195</span>
<span class="n">cuML</span> <span class="n">coef</span> <span class="p">:</span>  <span class="mi">0</span>      <span class="mf">0.9834546</span>
            <span class="mi">1</span>    <span class="mf">0.010128272</span>
           <span class="n">dtype</span><span class="p">:</span> <span class="n">float32</span>
<span class="n">cuML</span> <span class="n">predictions</span> <span class="p">:</span>  <span class="p">[</span><span class="mf">3.0055666</span> <span class="mf">2.0221121</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.SGD.fit" title="cuml.SGD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X, y)</p></td>
<td><p>Fit the model with X and y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.SGD.predict" title="cuml.SGD.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.SGD.predictClass" title="cuml.SGD.predictClass"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predictClass</span></code></a>(self, X)</p></td>
<td><p>Predicts the y for X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.SGD.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
<dt><strong>y: cuDF DataFrame</strong></dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.SGD.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.SGD.predictClass">
<code class="descname">predictClass</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.SGD.predictClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the y for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y: cuDF DataFrame</dt><dd><p>Dense vector (floats or doubles) of shape (n_samples, 1)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="nearest-neighbors">
<h2>Nearest Neighbors<a class="headerlink" href="#nearest-neighbors" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.NearestNeighbors">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">NearestNeighbors</code><a class="headerlink" href="#cuml.NearestNeighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>NearestNeighbors is a unsupervised algorithm where if one wants to find the “closest”
datapoint(s) to new unseen data, one can calculate a suitable “distance” between 
each and every point, and return the top K datapoints which have the smallest distance to it.</p>
<p>cuML’s KNN expects a cuDF DataFrame or a Numpy Array (where automatic chunking will be done
in to a Numpy Array in a future release), and fits a special data structure first to
approximate the distance calculations, allowing our querying times to be O(plogn)
and not the brute force O(np) [where p = no(features)]:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors: int (default = 5)</strong></dt><dd><p>The top K closest datapoints you want the algorithm to return. If this number is large,
then expect the algorithm to run slower.</p>
</dd>
<dt><strong>should_downcast</strong><span class="classifier">bool (default = False)</span></dt><dd><p>Currently only single precision is supported in the underlying undex. Setting this to
true will allow single-precision input arrays to be automatically downcasted to single
precision. Default = False.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>NearestNeighbors is a generative model. This means the data X has to be stored in order
for inference to occur.</p>
<p><strong>Applications of NearestNeighbors</strong></p>
<blockquote>
<div><p>Applications of NearestNeighbors include recommendation systems where content or colloborative
filtering is used. Since NearestNeighbors is a relatively simple generative model, it is also
used in data visualization and regression / classification tasks.</p>
</div></blockquote>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/notebook/blob/master/python/notebooks/knn_demo.ipynb">the NearestNeighbors notebook</a>.</p>
<p>For additional docs, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors">scikitlearn’s NearestNeighbors</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">from</span> <span class="nn">cuml.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">np_float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
  <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="c1"># Point 1</span>
  <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="c1"># Point 2</span>
  <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>  <span class="c1"># Point 3</span>
<span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;dim_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">np_float</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;dim_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">np_float</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;dim_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">np_float</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;n_samples = 3, n_dims = 3&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="n">nn_float</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">()</span>
<span class="n">nn_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="n">distances</span><span class="p">,</span><span class="n">indices</span> <span class="o">=</span> <span class="n">nn_float</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#get 3 nearest neighbors</span>

<span class="k">print</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cudf</span>

<span class="c1"># Both import methods supported</span>
<span class="c1"># from cuml.neighbors import NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_dims</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">dim_0</span> <span class="n">dim_1</span> <span class="n">dim_2</span>

<span class="mi">0</span>   <span class="mf">1.0</span>   <span class="mf">2.0</span>   <span class="mf">3.0</span>
<span class="mi">1</span>   <span class="mf">1.0</span>   <span class="mf">2.0</span>   <span class="mf">4.0</span>
<span class="mi">2</span>   <span class="mf">2.0</span>   <span class="mf">2.0</span>   <span class="mf">4.0</span>

<span class="c1"># indices:</span>

         <span class="n">index_neighbor_0</span> <span class="n">index_neighbor_1</span> <span class="n">index_neighbor_2</span>
<span class="mi">0</span>                <span class="mi">0</span>                <span class="mi">1</span>                <span class="mi">2</span>
<span class="mi">1</span>                <span class="mi">1</span>                <span class="mi">0</span>                <span class="mi">2</span>
<span class="mi">2</span>                <span class="mi">2</span>                <span class="mi">1</span>                <span class="mi">0</span>
<span class="c1"># distances:</span>

         <span class="n">distance_neighbor_0</span> <span class="n">distance_neighbor_1</span> <span class="n">distance_neighbor_2</span>
<span class="mi">0</span>                 <span class="mf">0.0</span>                 <span class="mf">1.0</span>                 <span class="mf">2.0</span>
<span class="mi">1</span>                 <span class="mf">0.0</span>                 <span class="mf">1.0</span>                 <span class="mf">1.0</span>
<span class="mi">2</span>                 <span class="mf">0.0</span>                 <span class="mf">1.0</span>                 <span class="mf">2.0</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.NearestNeighbors.fit" title="cuml.NearestNeighbors.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Fit GPU index for performing nearest neighbor queries.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.NearestNeighbors.kneighbors" title="cuml.NearestNeighbors.kneighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kneighbors</span></code></a>(self, X[, k])</p></td>
<td><p>Query the GPU index for the k nearest neighbors of row vectors in X.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cuml.NearestNeighbors.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.NearestNeighbors.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit GPU index for performing nearest neighbor queries.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame or numpy ndarray</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.NearestNeighbors.kneighbors">
<code class="descname">kneighbors</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>k=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.NearestNeighbors.kneighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Query the GPU index for the k nearest neighbors of row vectors in X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame or numpy ndarray</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
<dt><strong>k: Integer</strong></dt><dd><p>The number of neighbors</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>distances: cuDF DataFrame or numpy ndarray</dt><dd><p>The distances of the k-nearest neighbors for each column vector in X</p>
</dd>
<dt>indices: cuDF DataFrame of numpy ndarray</dt><dd><p>The indices of the k-nearest neighbors for each column vector in X</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="k-means-clustering">
<h2>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.KMeans">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">KMeans</code><a class="headerlink" href="#cuml.KMeans" title="Permalink to this definition">¶</a></dt>
<dd><p>KMeans is a basic but powerful clustering method which is optimized via Expectation Maximization.
It randomnly selects K data points in X, and computes which samples are close to these points.
For every cluster of points, a mean is computed (hence the name), and this becomes the new
centroid.</p>
<p>cuML’s KMeans expects a cuDF DataFrame, and supports the fast KMeans++ intialization method. This
method is more stable than randomnly selecting K points.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class.</p>
</dd>
<dt><strong>n_clusters</strong><span class="classifier">int (default = 8)</span></dt><dd><p>The number of centroids or clusters you want.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int (default = 300)</span></dt><dd><p>The more iterations of EM, the more accurate, but slower.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-4)</span></dt><dd><p>Stopping criterion when centroid means do not change much.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">boolean (default = 0)</span></dt><dd><p>If True, prints diagnositc information.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default = 1)</span></dt><dd><p>If you want results to be the same when you restart Python, select a state.</p>
</dd>
<dt><strong>precompute_distances</strong><span class="classifier">boolean (default = ‘auto’)</span></dt><dd><p>Not supported yet.</p>
</dd>
<dt><strong>init</strong><span class="classifier">{‘scalable-kmeans++’, ‘k-means||’ , ‘random’ or an ndarray} (default = ‘scalable-k-means++’)</span></dt><dd><p>‘scalable-k-means++’ or ‘k-means||’: Uses fast and stable scalable kmeans++ intialization.
‘random’: Choose ‘n_cluster’ observations (rows) at random from data for the initial centroids.
If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</p>
</dd>
<dt><strong>n_init</strong><span class="classifier">int (default = 1)</span></dt><dd><p>Number of times intialization is run. More is slower, but can be better.</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">“auto”</span></dt><dd><p>Currently uses full EM, but will support others later.</p>
</dd>
<dt><strong>n_gpu</strong><span class="classifier">int (default = 1)</span></dt><dd><p>Number of GPUs to use. Currently uses single GPU, but will support multiple GPUs later.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>KMeans requires n_clusters to be specified. This means one needs to approximately guess or know
how many clusters a dataset has. If one is not sure, one can start with a small number of clusters, and
visualize the resulting clusters with PCA, UMAP or T-SNE, and verify that they look appropriate.</p>
<p><strong>Applications of KMeans</strong></p>
<blockquote>
<div><p>The biggest advantage of KMeans is its speed and simplicity. That is why KMeans is many practitioner’s
first choice of a clustering algorithm. KMeans has been extensively used when the number of clusters is
approximately known, such as in big data clustering tasks, image segmentation and medical clustering.</p>
</div></blockquote>
<p>For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">scikitlearn’s Kmeans</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">cuml.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">np2cudf</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># convert numpy array to cuDF dataframe</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;fea</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">i</span><span class="p">:</span><span class="n">df</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])})</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">column</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
      <span class="n">pdf</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pdf</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np2cudf</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;input:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Calling fit&quot;</span><span class="p">)</span>
<span class="n">kmeans_float</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_gpu</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">kmeans_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;labels:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kmeans_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;cluster_centers:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kmeans_float</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span><span class="p">:</span>

     <span class="mi">0</span>    <span class="mi">1</span>
 <span class="mi">0</span>  <span class="mf">1.0</span>  <span class="mf">1.0</span>
 <span class="mi">1</span>  <span class="mf">1.0</span>  <span class="mf">2.0</span>
 <span class="mi">2</span>  <span class="mf">3.0</span>  <span class="mf">2.0</span>
 <span class="mi">3</span>  <span class="mf">4.0</span>  <span class="mf">3.0</span>

<span class="n">Calling</span> <span class="n">fit</span>

<span class="n">labels</span><span class="p">:</span>

   <span class="mi">0</span>    <span class="mi">0</span>
   <span class="mi">1</span>    <span class="mi">0</span>
   <span class="mi">2</span>    <span class="mi">1</span>
   <span class="mi">3</span>    <span class="mi">1</span>

<span class="n">cluster_centers</span><span class="p">:</span>

   <span class="mi">0</span>    <span class="mi">1</span>
<span class="mi">0</span>  <span class="mf">1.0</span>  <span class="mf">1.5</span>
<span class="mi">1</span>  <span class="mf">3.5</span>  <span class="mf">2.5</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>cluster_centers_</strong><span class="classifier">array</span></dt><dd><p>The coordinates of the final clusters. This represents of “mean” of each data cluster.</p>
</dd>
<dt><strong>labels_</strong><span class="classifier">array</span></dt><dd><p>Which cluster each datapoint belongs to.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.fit" title="cuml.KMeans.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Compute k-means clustering with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.fit_predict" title="cuml.KMeans.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(self, X)</p></td>
<td><p>Compute cluster centers and predict cluster index for each sample.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.fit_transform" title="cuml.KMeans.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, input_gdf)</p></td>
<td><p>Compute clustering and transform input_gdf to cluster-distance space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.get_params" title="cuml.KMeans.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.predict" title="cuml.KMeans.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self, X)</p></td>
<td><p>Predict the closest cluster each sample in X belongs to.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KMeans.set_params" title="cuml.KMeans.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KMeans.transform" title="cuml.KMeans.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X)</p></td>
<td><p>Transform X to a cluster-distance space.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.KMeans.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute k-means clustering with X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.fit_predict">
<code class="descname">fit_predict</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cluster centers and predict cluster index for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>self</em>, <em>input_gdf</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute clustering and transform input_gdf to cluster-distance space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>input_gdf</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>self</em>, <em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the closest cluster each sample in X belongs to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>self</em>, <em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KMeans.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KMeans.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X to a cluster-distance space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dbscan">
<h2>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.DBSCAN">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">DBSCAN</code><a class="headerlink" href="#cuml.DBSCAN" title="Permalink to this definition">¶</a></dt>
<dd><p>DBSCAN is a very powerful yet fast clustering technique that finds clusters where
data is concentrated. This allows DBSCAN to generalize to many problems if the
datapoints tend to congregate in larger groups.</p>
<p>cuML’s DBSCAN expects a cuDF DataFrame, and constructs an adjacency graph to compute
the distances between close neighbours.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>eps</strong><span class="classifier">float (default = 0.5)</span></dt><dd><p>The maximum distance between 2 points such they reside in the same neighborhood.</p>
</dd>
<dt><strong>handle</strong><span class="classifier">cuml.Handle</span></dt><dd><p>If it is None, a new one is created just for this class</p>
</dd>
<dt><strong>min_samples</strong><span class="classifier">int (default = 5)</span></dt><dd><p>The number of samples in a neighborhood such that this group can be considered as
an important core point (including the point itself).</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool</span></dt><dd><p>Whether to print debug spews</p>
</dd>
<dt><strong>max_bytes_per_batch</strong><span class="classifier">(optional) int64</span></dt><dd><p>Calculate batch size using no more than this number of bytes for the pairwise
distance computation. This enables the trade-off between runtime and memory usage for
making the N^2 pairwise distance computations more tractable for large numbers of samples.
If you are experiencing out of memory errors when running DBSCAN, you can set this
value based on the memory size of your device. Note: this option does not set
the maximum total memory used in the DBSCAN computation and so this value will not
be able to be set to the total memory available on the device.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>DBSCAN is very sensitive to the distance metric it is used with, and a large assumption
is that datapoints need to be concentrated in groups for clusters to be constructed.</p>
<p><strong>Applications of DBSCAN</strong></p>
<blockquote>
<div><p>DBSCAN’s main benefit is that the number of clusters is not a hyperparameter, and that
it can find non-linearly shaped clusters. This also allows DBSCAN to be robust to noise.
DBSCAN has been applied to analyzing particle collisons in the Large Hadron Collider,
customer segmentation in marketing analyses, and much more.</p>
</div></blockquote>
<p>For an additional example, see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/dbscan_demo.ipynb">the DBSCAN notebook</a>.
For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">scikitlearn’s DBSCAN</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">DBSCAN</span>
<span class="kn">from</span> <span class="nn">cuml.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">dbscan_float</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">min_samples</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dbscan_float</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>    <span class="mi">0</span>
<span class="mi">1</span>    <span class="mi">1</span>
<span class="mi">2</span>    <span class="mi">2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>labels_</strong><span class="classifier">array</span></dt><dd><p>Which cluster each datapoint belongs to. Noisy samples are labeled as -1.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.DBSCAN.fit" title="cuml.DBSCAN.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X)</p></td>
<td><p>Perform DBSCAN clustering from features.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.DBSCAN.fit_predict" title="cuml.DBSCAN.fit_predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_predict</span></code></a>(self, X)</p></td>
<td><p>Performs clustering on input_gdf and returns cluster labels.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.DBSCAN.get_param_names" title="cuml.DBSCAN.get_param_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_param_names</span></code></a>(self)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.DBSCAN.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform DBSCAN clustering from features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.DBSCAN.fit_predict">
<code class="descname">fit_predict</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs clustering on input_gdf and returns cluster labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features),</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">cuDF Series, shape (n_samples)</span></dt><dd><p>cluster labels</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.DBSCAN.get_param_names">
<code class="descname">get_param_names</code><span class="sig-paren">(</span><em>self</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.DBSCAN.get_param_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="kalman-filter">
<h2>Kalman Filter<a class="headerlink" href="#kalman-filter" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.KalmanFilter">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">KalmanFilter</code><a class="headerlink" href="#cuml.KalmanFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements a Kalman filter. You are responsible for setting the
various state variables to reasonable values; defaults  will
not give you a functional filter.
After construction the filter will have default matrices created for you,
but you must specify the values for each.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dim_x</strong><span class="classifier">int</span></dt><dd><p>Number of state variables for the Kalman filter.
This is used to set the default size of P, Q, and u</p>
</dd>
<dt><strong>dim_z</strong><span class="classifier">int</span></dt><dd><p>Number of of measurement inputs.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cuml</span> <span class="k">import</span> <span class="n">KalmanFilter</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">KalmanFilter</span><span class="p">(</span><span class="n">dim_x</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim_z</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">],</span>    <span class="c1"># position</span>
                <span class="p">[</span><span class="mf">0.</span><span class="p">]])</span>   <span class="c1"># velocity</span>
<span class="n">f</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]])</span>
<span class="n">f</span><span class="o">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">]])</span>
<span class="n">f</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1000.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span>   <span class="mf">0.</span><span class="p">,</span> <span class="mf">1000.</span><span class="p">]</span> <span class="p">])</span>
<span class="n">f</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Now just perform the standard predict/update loop:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">while</span> <span class="n">some_condition_is_true</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">numba</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">])</span>
    <span class="n">f</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
    <span class="n">f</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">numba device array, numpy array or cuDF series (dim_x, 1),</span></dt><dd><p>Current state estimate. Any call to update() or predict() updates
this variable.</p>
</dd>
<dt><strong>P</strong><span class="classifier">numba device array, numpy array or cuDF dataframe(dim_x, dim_x)</span></dt><dd><p>Current state covariance matrix. Any call to update() or predict()
updates this variable.</p>
</dd>
<dt><strong>x_prior</strong><span class="classifier">numba device array, numpy array or cuDF series(dim_x, 1)</span></dt><dd><p>Prior (predicted) state estimate. The <a href="#id3"><span class="problematic" id="id4">*</span></a>_prior and <a href="#id5"><span class="problematic" id="id6">*</span></a>_post attributes
are for convienence; they store the  prior and posterior of the
current epoch. Read Only.</p>
</dd>
<dt><strong>P_prior</strong><span class="classifier">numba device array, numpy array or cuDF dataframe(dim_x, dim_x)</span></dt><dd><p>Prior (predicted) state covariance matrix. Read Only.</p>
</dd>
<dt><strong>x_post</strong><span class="classifier">numba device array, numpy array or cuDF series(dim_x, 1)</span></dt><dd><p>Posterior (updated) state estimate. Read Only.</p>
</dd>
<dt><strong>P_post</strong><span class="classifier">numba device array, numpy array or cuDF dataframe(dim_x, dim_x)</span></dt><dd><p>Posterior (updated) state covariance matrix. Read Only.</p>
</dd>
<dt><strong>z</strong><span class="classifier">numba device array or cuDF series (dim_x, 1)</span></dt><dd><p>Last measurement used in update(). Read only.</p>
</dd>
<dt><strong>R</strong><span class="classifier">numba device array(dim_z, dim_z)</span></dt><dd><p>Measurement noise matrix</p>
</dd>
<dt><strong>Q</strong><span class="classifier">numba device array(dim_x, dim_x)</span></dt><dd><p>Process noise matrix</p>
</dd>
<dt><strong>F</strong><span class="classifier">numba device array()</span></dt><dd><p>State Transition matrix</p>
</dd>
<dt><strong>H</strong><span class="classifier">numba device array(dim_z, dim_x)</span></dt><dd><p>Measurement function</p>
</dd>
<dt><strong>y</strong><span class="classifier">numba device array</span></dt><dd><p>Residual of the update step. Read only.</p>
</dd>
<dt><strong>K</strong><span class="classifier">numba device array(dim_x, dim_z)</span></dt><dd><p>Kalman gain of the update step. Read only.</p>
</dd>
<dt><strong>precision: ‘single’ or ‘double’</strong></dt><dd><p>Whether the Kalman Filter uses single or double precision</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.KalmanFilter.predict" title="cuml.KalmanFilter.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(self[, B, F, Q])</p></td>
<td><p>Predict next state (prior) using the Kalman filter state propagation equations.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.KalmanFilter.update" title="cuml.KalmanFilter.update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update</span></code></a>(self, z[, R, H])</p></td>
<td><p>Add a new measurement (z) to the Kalman filter.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.KalmanFilter.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>self</em>, <em>B=None</em>, <em>F=None</em>, <em>Q=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KalmanFilter.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict next state (prior) using the Kalman filter state propagation
equations.
Parameters
———-
u : np.array</p>
<blockquote>
<div><p>Optional control vector. If not <cite>None</cite>, it is multiplied by B
to create the control input into the system.</p>
</div></blockquote>
<dl class="simple">
<dt>B<span class="classifier">np.array(dim_x, dim_z), or None</span></dt><dd><p>Optional control transition matrix; a value of None
will cause the filter to use <cite>self.B</cite>.</p>
</dd>
<dt>F<span class="classifier">np.array(dim_x, dim_x), or None</span></dt><dd><p>Optional state transition matrix; a value of None
will cause the filter to use <cite>self.F</cite>.</p>
</dd>
<dt>Q<span class="classifier">np.array(dim_x, dim_x), scalar, or None</span></dt><dd><p>Optional process noise matrix; a value of None will cause the
filter to use <cite>self.Q</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.KalmanFilter.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>self</em>, <em>z</em>, <em>R=None</em>, <em>H=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.KalmanFilter.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a new measurement (z) to the Kalman filter.
If z is None, nothing is computed. However, x_post and P_post are
updated with the prior (x_prior, P_prior), and self.z is set to None.
Parameters
———-
z : (dim_z, 1): array_like</p>
<blockquote>
<div><p>measurement for this update. z can be a scalar if dim_z is 1,
otherwise it must be convertible to a column vector.</p>
</div></blockquote>
<dl class="simple">
<dt>R<span class="classifier">np.array, scalar, or None</span></dt><dd><p>Optionally provide R to override the measurement noise for this
one call, otherwise  self.R will be used.</p>
</dd>
<dt>H<span class="classifier">np.array, or None</span></dt><dd><p>Optionally provide H to override the measurement function for this
one call, otherwise self.H will be used.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="principal-component-analysis">
<h2>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.PCA">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">PCA</code><a class="headerlink" href="#cuml.PCA" title="Permalink to this definition">¶</a></dt>
<dd><p>PCA (Principal Component Analysis) is a fundamental dimensionality reduction technique used to
combine features in X in linear combinations such that each new component captures the most
information or variance of the data. N_components is usually small, say at 3, where it can be
used for data visualization, data compression and exploratory analysis.</p>
<p>cuML’s PCA expects a cuDF DataFrame, and provides 2 algorithms Full and Jacobi.
Full (default) uses a full eigendecomposition then selects the top K eigenvectors. 
The Jacobi algorithm is much faster as it iteratively tries to correct the top K eigenvectors,
but might be less accurate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want. Must be &lt;= number(columns).</p>
</dd>
<dt><strong>svd_solver</strong><span class="classifier">‘full’ or ‘jacobi’ or ‘auto’ (default = ‘full’)</span></dt><dd><p>Full uses a eigendecomposition of the covariance matrix then discards components.
Jacobi is much faster as it iteratively corrects, but is less accurate.</p>
</dd>
<dt><strong>iterated_power</strong><span class="classifier">int (default = 15)</span></dt><dd><p>Used in Jacobi solver. The more iterations, the more accurate, but the slower.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-7)</span></dt><dd><p>Used if algorithm = “jacobi”. The smaller the tolerance, the more accurate,
but the more slower the algorithm will get to converge.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int / None (default = None)</span></dt><dd><p>If you want results to be the same when you restart Python, select a state.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean (default = True)</span></dt><dd><p>If True, then copies data then removes mean from data. False might cause data to be
overwritten with its mean centered version.</p>
</dd>
<dt><strong>whiten</strong><span class="classifier">boolean (default = False)</span></dt><dd><p>If True, de-correlates the components. This is done by dividing them by the corresponding
singular values then multiplying by sqrt(n_samples). Whitening allows each component
to have unit variance and removes multi-collinearity. It might be beneficial for downstream
tasks like LinearRegression where correlated features cause problems.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>PCA considers linear combinations of features, specifically those that maximise global
variance structure. This means PCA is fantastic for global structure analyses, but weak
for local relationships. Consider UMAP or T-SNE for a locally important embedding.</p>
<p><strong>Applications of PCA</strong></p>
<blockquote>
<div><p>PCA is used extensively in practice for data visualization and data compression. It has been used
to visualize extremely large word embeddings like Word2Vec and GloVe in 2 or 3 dimensions, large
datasets of everyday objects and images, and used to distinguish between cancerous cells from 
healthy cells.</p>
</div></blockquote>
<p>For an additional example see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/pca_demo.ipynb">the PCA notebook</a>. 
For additional docs, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">scikitlearn’s PCA</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">cuml.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">pca_float</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pca_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;components: {pca_float.components_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance: {pca_float.explained_variance_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance ratio: {pca_float.explained_variance_ratio_}&#39;</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;singular values: {pca_float.singular_values_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;mean: {pca_float.mean_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;noise variance: {pca_float.noise_variance_}&#39;</span><span class="p">)</span>

<span class="n">trans_gdf_float</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Inverse: {trans_gdf_float}&#39;</span><span class="p">)</span>

<span class="n">input_gdf_float</span> <span class="o">=</span> <span class="n">pca_float</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">trans_gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Input: {input_gdf_float}&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">components</span><span class="p">:</span>
            <span class="mi">0</span>           <span class="mi">1</span>           <span class="mi">2</span>
            <span class="mi">0</span>  <span class="mf">0.69225764</span>  <span class="o">-</span><span class="mf">0.5102837</span> <span class="o">-</span><span class="mf">0.51028395</span>
            <span class="mi">1</span> <span class="o">-</span><span class="mf">0.72165036</span> <span class="o">-</span><span class="mf">0.48949987</span>  <span class="o">-</span><span class="mf">0.4895003</span>

<span class="n">explained</span> <span class="n">variance</span><span class="p">:</span>

            <span class="mi">0</span>   <span class="mf">8.510402</span>
            <span class="mi">1</span> <span class="mf">0.48959687</span>

<span class="n">explained</span> <span class="n">variance</span> <span class="n">ratio</span><span class="p">:</span>

             <span class="mi">0</span>   <span class="mf">0.9456003</span>
             <span class="mi">1</span> <span class="mf">0.054399658</span>

<span class="n">singular</span> <span class="n">values</span><span class="p">:</span>

           <span class="mi">0</span> <span class="mf">4.1256275</span>
           <span class="mi">1</span> <span class="mf">0.9895422</span>

<span class="n">mean</span><span class="p">:</span>

          <span class="mi">0</span> <span class="mf">2.6666667</span>
          <span class="mi">1</span> <span class="mf">2.3333333</span>
          <span class="mi">2</span> <span class="mf">2.3333333</span>

<span class="n">noise</span> <span class="n">variance</span><span class="p">:</span>

      <span class="mi">0</span>  <span class="mf">0.0</span>

<span class="n">transformed</span> <span class="n">matrix</span><span class="p">:</span>
             <span class="mi">0</span>           <span class="mi">1</span>
             <span class="mi">0</span>   <span class="o">-</span><span class="mf">2.8547091</span> <span class="o">-</span><span class="mf">0.42891636</span>
             <span class="mi">1</span> <span class="o">-</span><span class="mf">0.121316016</span>  <span class="mf">0.80743366</span>
             <span class="mi">2</span>    <span class="mf">2.9760244</span> <span class="o">-</span><span class="mf">0.37851727</span>

<span class="n">Input</span> <span class="n">Matrix</span><span class="p">:</span>
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
          <span class="mi">0</span> <span class="mf">1.0000001</span> <span class="mf">3.9999993</span>       <span class="mf">4.0</span>
          <span class="mi">1</span>       <span class="mf">2.0</span> <span class="mf">2.0000002</span> <span class="mf">1.9999999</span>
          <span class="mi">2</span> <span class="mf">4.9999995</span> <span class="mf">1.0000006</span>       <span class="mf">1.0</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
<dt><strong>mean_</strong><span class="classifier">array</span></dt><dd><p>The column wise mean of X. Used to mean - center the data first.</p>
</dd>
<dt><strong>noise_variance_</strong><span class="classifier">float</span></dt><dd><p>From Bishop 1999’s Textbook. Used in later tasks like calculating the
estimated covariance of X.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.fit" title="cuml.PCA.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, _transform])</p></td>
<td><p>Fit the model with X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.fit_transform" title="cuml.PCA.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X[, y])</p></td>
<td><p>Fit the model with X and apply the dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.get_params" title="cuml.PCA.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.inverse_transform" title="cuml.PCA.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(self, X)</p></td>
<td><p>Transform data back to its original space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.PCA.set_params" title="cuml.PCA.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **parameter)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.PCA.transform" title="cuml.PCA.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X)</p></td>
<td><p>Apply dimensionality reduction to X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.PCA.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>_transform=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame</span></dt><dd><p>Dense matrix (floats or doubles) of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>cluster labels</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model with X and apply the dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_features)</span></dt><dd><p>training data (floats or doubles), where n_samples is the number of samples, and n_features is the number of features.</p>
</dd>
<dt><strong>y</strong><span class="classifier">ignored</span></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>self</em>, <em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.inverse_transform">
<code class="descname">inverse_transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform data back to its original space.</p>
<p>In other words, return an input X_original whose transform would be X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd><p>New data (floats or doubles), where n_samples is the number of samples and n_components is the number of components.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_original</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_features)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>self</em>, <em>**parameter</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.set_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="cuml.PCA.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.PCA.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply dimensionality reduction to X.</p>
<p>X is projected on the first principal components previously extracted from a training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_features)</span></dt><dd><p>New data (floats or doubles), where n_samples is the number of samples and n_features is the number of features.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="truncated-svd">
<h2>Truncated SVD<a class="headerlink" href="#truncated-svd" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.TruncatedSVD">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">TruncatedSVD</code><a class="headerlink" href="#cuml.TruncatedSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>TruncatedSVD is used to compute the top K singular values and vectors of a large matrix X. 
It is much faster when n_components is small, such as in the use of PCA when 3 components is 
used for 3D visualization.</p>
<p>cuML’s TruncatedSVD expects a cuDF DataFrame, and provides 2 algorithms Full and Jacobi.
Full (default) uses a full eigendecomposition then selects the top K singular vectors. 
The Jacobi algorithm is much faster as it iteratively tries to correct the top K singular
vectors, but might be less accurate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_components</strong><span class="classifier">int (default = 1)</span></dt><dd><p>The number of top K singular vectors / values you want. Must be &lt;= number(columns).</p>
</dd>
<dt><strong>algorithm</strong><span class="classifier">‘full’ or ‘jacobi’ or ‘auto’ (default = ‘full’)</span></dt><dd><p>Full uses a eigendecomposition of the covariance matrix then discards components.
Jacobi is much faster as it iteratively corrects, but is less accurate.</p>
</dd>
<dt><strong>n_iter</strong><span class="classifier">int (default = 15)</span></dt><dd><p>Used in Jacobi solver. The more iterations, the more accurate, but the slower.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (default = 1e-7)</span></dt><dd><p>Used if algorithm = “jacobi”. The smaller the tolerance, the more accurate,
but the more slower the algorithm will get to converge.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int / None (default = None)</span></dt><dd><p>If you want results to be the same when you restart Python, select a state.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>TruncatedSVD (the randomized version [Jacobi]) is fantastic when the number of components
you want is much smaller than the number of features. The approximation to the largest
singular values and vectors is very robust, however, this method loses a lot of accuracy
when you want many many components.</p>
<p><strong>Applications of TruncatedSVD</strong></p>
<blockquote>
<div><p>TruncatedSVD is also known as Latent Semantic Indexing (LSI) which tries to find topics of a
word count matrix. If X previously was centered with mean removal, TruncatedSVD is the 
same as TruncatedPCA. TruncatedSVD is also used in information retrieval tasks, recommendation
systems and data compression.</p>
</div></blockquote>
<p>For additional examples, see <a class="reference external" href="https://github.com/rapidsai/notebooks/blob/master/cuml/tsvd_demo.ipynb">the Truncated SVD  notebook</a>. 
For additional documentation, see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">scikitlearn’s TruncatedSVD docs</a>.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Both import methods supported</span>
<span class="kn">from</span> <span class="nn">cuml</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="kn">from</span> <span class="nn">cuml.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>

<span class="kn">import</span> <span class="nn">cudf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">gdf_float</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gdf_float</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">tsvd_float</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;jacobi&quot;</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">)</span>
<span class="n">tsvd_float</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;components: {tsvd_float.components_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance: {tsvd_float.explained_variance_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;explained variance ratio: {tsvd_float.explained_variance_ratio_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;singular values: {tsvd_float.singular_values_}&#39;</span><span class="p">)</span>

<span class="n">trans_gdf_float</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Transformed matrix: {trans_gdf_float}&#39;</span><span class="p">)</span>

<span class="n">input_gdf_float</span> <span class="o">=</span> <span class="n">tsvd_float</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">trans_gdf_float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Input matrix: {input_gdf_float}&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">components</span><span class="p">:</span>            <span class="mi">0</span>           <span class="mi">1</span>          <span class="mi">2</span>
<span class="mi">0</span> <span class="mf">0.58725953</span>  <span class="mf">0.57233137</span>  <span class="mf">0.5723314</span>
<span class="mi">1</span> <span class="mf">0.80939883</span> <span class="o">-</span><span class="mf">0.41525528</span> <span class="o">-</span><span class="mf">0.4152552</span>
<span class="n">explained</span> <span class="n">variance</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">55.33908</span>
<span class="mi">1</span> <span class="mf">16.660923</span>

<span class="n">explained</span> <span class="n">variance</span> <span class="n">ratio</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">0.7685983</span>
<span class="mi">1</span> <span class="mf">0.23140171</span>

<span class="n">singular</span> <span class="n">values</span><span class="p">:</span>
<span class="mi">0</span>  <span class="mf">7.439024</span>
<span class="mi">1</span> <span class="mf">4.0817795</span>

<span class="n">Transformed</span> <span class="n">matrix</span><span class="p">:</span>           <span class="mi">0</span>            <span class="mi">1</span>
<span class="mi">0</span> <span class="mf">5.1659107</span>    <span class="o">-</span><span class="mf">2.512643</span>
<span class="mi">1</span> <span class="mf">3.4638448</span> <span class="o">-</span><span class="mf">0.042223275</span>                                                                                                                     <span class="mi">2</span> <span class="mf">4.0809603</span>    <span class="mf">3.2164836</span>

<span class="n">Input</span> <span class="n">matrix</span><span class="p">:</span>           <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>
<span class="mi">0</span>       <span class="mf">1.0</span>  <span class="mf">4.000001</span>  <span class="mf">4.000001</span>
<span class="mi">1</span> <span class="mf">2.0000005</span> <span class="mf">2.0000005</span> <span class="mf">2.0000007</span>
<span class="mi">2</span>  <span class="mf">5.000001</span> <span class="mf">0.9999999</span> <span class="mf">1.0000004</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>components_</strong><span class="classifier">array</span></dt><dd><p>The top K components (VT.T[:,:n_components]) in U, S, VT = svd(X)</p>
</dd>
<dt><strong>explained_variance_</strong><span class="classifier">array</span></dt><dd><p>How much each component explains the variance in the data given by S**2</p>
</dd>
<dt><strong>explained_variance_ratio_</strong><span class="classifier">array</span></dt><dd><p>How much in % the variance is explained given by S**2/sum(S**2)</p>
</dd>
<dt><strong>singular_values_</strong><span class="classifier">array</span></dt><dd><p>The top K singular values. Remember all singular values &gt;= 0</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.fit" title="cuml.TruncatedSVD.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self, X[, _transform])</p></td>
<td><p>Fit LSI model on training cudf DataFrame X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.fit_transform" title="cuml.TruncatedSVD.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self, X)</p></td>
<td><p>Fit LSI model to X and perform dimensionality reduction on X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.get_params" title="cuml.TruncatedSVD.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>(self[, deep])</p></td>
<td><p>Sklearn style return parameter state</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.inverse_transform" title="cuml.TruncatedSVD.inverse_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse_transform</span></code></a>(self, X)</p></td>
<td><p>Transform X back to its original space.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.set_params" title="cuml.TruncatedSVD.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(self, **params)</p></td>
<td><p>Sklearn style set parameter state to dictionary of params.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.TruncatedSVD.transform" title="cuml.TruncatedSVD.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self, X)</p></td>
<td><p>Perform dimensionality reduction on X.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="cuml.TruncatedSVD.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>_transform=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit LSI model on training cudf DataFrame X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame, dense matrix, shape (n_samples, n_features)</span></dt><dd><p>Training data (floats or doubles)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit LSI model to X and perform dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X GDF</strong><span class="classifier">cuDF DataFrame, dense matrix, shape (n_samples, n_features)</span></dt><dd><p>Training data (floats or doubles)</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd><p>Reduced version of X. This will always be a dense cuDF DataFrame</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>self</em>, <em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style return parameter state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">boolean (default = True)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.inverse_transform">
<code class="descname">inverse_transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X back to its original space.</p>
<p>Returns a cuDF DataFrame X_original whose transform would be X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd><p>New data.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_original</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_features)</span></dt><dd><p>Note that this is always a dense cuDF DataFrame.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>self</em>, <em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Sklearn style set parameter state to dictionary of params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict of new params</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="cuml.TruncatedSVD.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#cuml.TruncatedSVD.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform dimensionality reduction on X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">cuDF DataFrame, dense matrix, shape (n_samples, n_features)</span></dt><dd><p>New data.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_new</strong><span class="classifier">cuDF DataFrame, shape (n_samples, n_components)</span></dt><dd><p>Reduced version of X. This will always be a dense DataFrame.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="umap">
<h2>UMAP<a class="headerlink" href="#umap" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuml.UMAP">
<em class="property">class </em><code class="descclassname">cuml.</code><code class="descname">UMAP</code><a class="headerlink" href="#cuml.UMAP" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniform Manifold Approximation and Projection
Finds a low dimensional embedding of the data that approximates
an underlying manifold.</p>
<p>Adapted from <a class="reference external" href="https://github.com/lmcinnes/umap/blob/master/umap/">https://github.com/lmcinnes/umap/blob/master/umap/</a><a class="reference internal" href="#umap">umap</a>.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_neighbors: float (optional, default 15)</strong></dt><dd><p>The size of local neighborhood (in terms of number of neighboring
sample points) used for manifold approximation. Larger values
result in more global views of the manifold, while smaller
values result in more local data being preserved. In general
values should be in the range 2 to 100.</p>
</dd>
<dt><strong>n_components: int (optional, default 2)</strong></dt><dd><p>The dimension of the space to embed into. This defaults to 2 to
provide easy visualization, but can reasonably be set to any</p>
</dd>
<dt><strong>n_epochs: int (optional, default None)</strong></dt><dd><p>The number of training epochs to be used in optimizing the
low dimensional embedding. Larger values result in more accurate
embeddings. If None is specified a value will be selected based on
the size of the input dataset (200 for large datasets, 500 for small).</p>
</dd>
<dt><strong>learning_rate: float (optional, default 1.0)</strong></dt><dd><p>The initial learning rate for the embedding optimization.</p>
</dd>
<dt><strong>init: string (optional, default ‘spectral’)</strong></dt><dd><dl class="simple">
<dt>How to initialize the low dimensional embedding. Options are:</dt><dd><ul class="simple">
<li><p>‘spectral’: use a spectral embedding of the fuzzy 1-skeleton</p></li>
<li><p>‘random’: assign initial embedding positions at random.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt><strong>min_dist: float (optional, default 0.1)</strong></dt><dd><p>The effective minimum distance between embedded points. Smaller values
will result in a more clustered/clumped embedding where nearby points
on the manifold are drawn closer together, while larger values will
result on a more even dispersal of points. The value should be set
relative to the <code class="docutils literal notranslate"><span class="pre">spread</span></code> value, which determines the scale at which
embedded points will be spread out.</p>
</dd>
<dt><strong>spread: float (optional, default 1.0)</strong></dt><dd><p>The effective scale of embedded points. In combination with <code class="docutils literal notranslate"><span class="pre">min_dist</span></code>
this determines how clustered/clumped the embedded points are.</p>
</dd>
<dt><strong>set_op_mix_ratio: float (optional, default 1.0)</strong></dt><dd><p>Interpolate between (fuzzy) union and intersection as the set operation
used to combine local fuzzy simplicial sets to obtain a global fuzzy
simplicial sets. Both fuzzy set operations use the product t-norm.
The value of this parameter should be between 0.0 and 1.0; a value of
1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
intersection.</p>
</dd>
<dt><strong>local_connectivity: int (optional, default 1)</strong></dt><dd><p>The local connectivity required – i.e. the number of nearest
neighbors that should be assumed to be connected at a local level.
The higher this value the more connected the manifold becomes
locally. In practice this should be not more than the local intrinsic
dimension of the manifold.</p>
</dd>
<dt><strong>repulsion_strength: float (optional, default 1.0)</strong></dt><dd><p>Weighting applied to negative samples in low dimensional embedding
optimization. Values higher than one will result in greater weight
being given to negative samples.</p>
</dd>
<dt><strong>negative_sample_rate: int (optional, default 5)</strong></dt><dd><p>The number of negative samples to select per positive sample
in the optimization process. Increasing this value will result
in greater repulsive force being applied, greater optimization
cost, but slightly more accuracy.</p>
</dd>
<dt><strong>transform_queue_size: float (optional, default 4.0)</strong></dt><dd><p>For transform operations (embedding new points using a trained <a href="#id11"><span class="problematic" id="id12">model_</span></a>
this will control how aggressively to search for nearest neighbors.
Larger values will result in slower performance but more accurate
nearest neighbor evaluation.</p>
</dd>
<dt><strong>a: float (optional, default None)</strong></dt><dd><p>More specific parameters controlling the embedding. If None these
values are set automatically as determined by <code class="docutils literal notranslate"><span class="pre">min_dist</span></code> and
<code class="docutils literal notranslate"><span class="pre">spread</span></code>.</p>
</dd>
<dt><strong>b: float (optional, default None)</strong></dt><dd><p>More specific parameters controlling the embedding. If None these
values are set automatically as determined by <code class="docutils literal notranslate"><span class="pre">min_dist</span></code> and
<code class="docutils literal notranslate"><span class="pre">spread</span></code>.</p>
</dd>
<dt><strong>verbose: bool (optional, default False)</strong></dt><dd><p>Controls verbosity of logging.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This module is heavily based on Leland McInnes’ reference UMAP package.
However, there are a number of differences and features that are not yet
implemented in cuml.umap:</p>
<blockquote>
<div><ul class="simple">
<li><p>Specifying the random seed</p></li>
<li><p>Using a non-euclidean distance metric (support for a fixed set
of non-euclidean metrics is planned for an upcoming release).</p></li>
<li><p>Using a pre-computed pairwise distance matrix (under consideration
for future releases)</p></li>
<li><p>Manual initialization of initial embedding positions</p></li>
</ul>
</div></blockquote>
<p>In addition to these missing features, you should expect to see
the final embeddings differing between cuml.umap and the reference
UMAP. In particular, the reference UMAP uses an approximate kNN
algorithm for large data sizes while cuml.umap always uses exact
kNN.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Leland McInnes, John Healy, James Melville
UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction
<a class="reference external" href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</a></p></li>
</ul>
<p class="rubric">Methods</p>
<table class="longtable docutils align-center">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.fit" title="cuml.UMAP.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>()</p></td>
<td><p>Fit X into an embedded space.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cuml.UMAP.fit_transform" title="cuml.UMAP.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>()</p></td>
<td><p>Fit X into an embedded space and return that transformed output.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cuml.UMAP.transform" title="cuml.UMAP.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>()</p></td>
<td><p>Transform X into the existing embedded space and return that transformed output.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cuml.UMAP.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit X into an embedded space.
Parameters
———-
X : array, shape (n_samples, n_features)</p>
<blockquote>
<div><p>X contains a sample per row.</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">array, shape (n_samples)</span></dt><dd><p>y contains a label per row.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.UMAP.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit X into an embedded space and return that transformed
output.
Parameters
———-
X : array, shape (n_samples, n_features) or (n_samples, n_samples)</p>
<blockquote>
<div><p>X contains a sample per row.</p>
</div></blockquote>
<dl class="simple">
<dt>X_new<span class="classifier">array, shape (n_samples, n_components)</span></dt><dd><p>Embedding of the training data in low-dimensional space.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cuml.UMAP.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cuml.UMAP.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X into the existing embedded space and return that
transformed output.</p>
<p>Please refer to the reference UMAP implementation for information
on the differences between fit_transform() and running fit() transform().</p>
<p>Specifically, the transform() function is stochastic:
<a class="reference external" href="https://github.com/lmcinnes/umap/issues/158">https://github.com/lmcinnes/umap/issues/158</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array, shape (n_samples, n_features)</span></dt><dd><p>New data to be transformed.</p>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>——-</strong></dt><dd></dd>
<dt><strong>X_new</strong><span class="classifier">array, shape (n_samples, n_components)</span></dt><dd><p>Embedding of the new data in low-dimensional space.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to cuML’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, nvidia

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>